{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_reber_grammar = [\n",
    "    [(\"B\", 1)],\n",
    "    [(\"T\", 2), (\"P\", 3)],\n",
    "    [(\"S\", 2), (\"X\", 4)],\n",
    "    [(\"T\", 3), (\"V\", 5)],\n",
    "    [(\"X\", 3), (\"S\", 6)],\n",
    "    [(\"P\", 4), (\"V\", 6)],\n",
    "    [(\"E\", None)]\n",
    "]\n",
    "\n",
    "embedded_reber_grammar = [\n",
    "    [(\"B\", 1)],\n",
    "    [(\"T\", 2), (\"P\", 3)],\n",
    "    [(default_reber_grammar, 4)],\n",
    "    [(default_reber_grammar, 5)],\n",
    "    [(\"T\", 6)],\n",
    "    [(\"P\", 6)],\n",
    "    [(\"E\", None)]\n",
    "]\n",
    "\n",
    "def generate_string(grammar):\n",
    "    state = 0\n",
    "    output = []\n",
    "    while state is not None:\n",
    "        index = np.random.randint(len(grammar[state]))\n",
    "        production, state = grammar[state][index]\n",
    "        if isinstance(production, list):\n",
    "            production = generate_string(grammar=production)\n",
    "        output.append(production)\n",
    "    return \"\".join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTXXTVVE BTXSE BPVVE BPVPSE BPTTVVE "
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    print(generate_string(default_reber_grammar), end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPBTSXXVVEPE BPBPTTTVPSEPE BPBPVPXTTTVPSEPE BPBTSXSEPE BTBPVVETE "
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    print(generate_string(embedded_reber_grammar), end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "POSSIBLE_CHARS = \"BEPSTVX\"\n",
    "\n",
    "def generate_corrupted_string(grammar, chars=POSSIBLE_CHARS):\n",
    "    good_string = generate_string(grammar)\n",
    "    index = np.random.randint(len(good_string))\n",
    "    good_char = good_string[index]\n",
    "    bad_char = np.random.choice(sorted(set(chars) - set(good_char)))\n",
    "    return good_string[:index] + bad_char + good_string[index + 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTBPXXVVETE BTBTTXTVVETE BTBXSXSETE BPBPTTTPTVVEPE VPBPTVVEPE "
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    print(generate_corrupted_string(embedded_reber_grammar), end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_ids(s, chars=POSSIBLE_CHARS):\n",
    "    return [chars.index(c) for c in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 4, 4, 4, 6, 6, 5, 5, 1, 4, 1]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_to_ids(\"BTTTXXVVETE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(size):\n",
    "    good_strings = [string_to_ids(generate_string(embedded_reber_grammar)) for _ in range(size // 2)]\n",
    "    bad_strings = [string_to_ids(generate_corrupted_string(embedded_reber_grammar)) for _ in range(size - size // 2)]\n",
    "    all_strings = good_strings + bad_strings\n",
    "    X = tf.ragged.constant(all_strings, ragged_rank=1)\n",
    "    y = np.array([[1.] for _ in range(len(good_strings))] + [[0.] for _ in range(len(bad_strings))])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = generate_dataset(10000)\n",
    "X_valid, y_valid = generate_dataset(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(9,), dtype=int32, numpy=array([0, 4, 0, 4, 6, 3, 1, 4, 1])>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ozing\\miniconda3\\envs\\handson-ml2\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:446: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/sequential_1/gru_1/RaggedToTensor/boolean_mask_1/GatherV2:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/sequential_1/gru_1/RaggedToTensor/boolean_mask/GatherV2:0\", shape=(None, 5), dtype=float32), dense_shape=Tensor(\"gradient_tape/sequential_1/gru_1/RaggedToTensor/Shape:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 5s 11ms/step - loss: 0.6935 - accuracy: 0.5083 - val_loss: 0.6864 - val_accuracy: 0.5500\n",
      "Epoch 2/20\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.6730 - accuracy: 0.5544 - val_loss: 0.6734 - val_accuracy: 0.6090\n",
      "Epoch 3/20\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.6509 - accuracy: 0.5769 - val_loss: 0.6534 - val_accuracy: 0.6230\n",
      "Epoch 4/20\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.6356 - accuracy: 0.6007 - val_loss: 0.6254 - val_accuracy: 0.6580\n",
      "Epoch 5/20\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.6114 - accuracy: 0.6297 - val_loss: 0.6125 - val_accuracy: 0.6345\n",
      "Epoch 6/20\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.5398 - accuracy: 0.7143 - val_loss: 0.6343 - val_accuracy: 0.4920\n",
      "Epoch 7/20\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.3464 - accuracy: 0.8536 - val_loss: 0.2393 - val_accuracy: 0.9360\n",
      "Epoch 8/20\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.2951 - accuracy: 0.8834 - val_loss: 0.1502 - val_accuracy: 0.9520\n",
      "Epoch 9/20\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.1126 - accuracy: 0.9678 - val_loss: 0.1122 - val_accuracy: 0.9810\n",
      "Epoch 10/20\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 0.0776 - accuracy: 0.9813 - val_loss: 0.0934 - val_accuracy: 0.9790\n",
      "Epoch 11/20\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 0.0672 - accuracy: 0.9847 - val_loss: 0.0566 - val_accuracy: 0.9885\n",
      "Epoch 12/20\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 0.0846 - accuracy: 0.9780 - val_loss: 0.0793 - val_accuracy: 0.9840\n",
      "Epoch 13/20\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 0.0468 - accuracy: 0.9885 - val_loss: 0.0126 - val_accuracy: 0.9930\n",
      "Epoch 14/20\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.0042 - accuracy: 0.9989 - val_loss: 0.1043 - val_accuracy: 0.9910\n",
      "Epoch 15/20\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.0128 - accuracy: 0.9968 - val_loss: 0.0503 - val_accuracy: 0.9885\n",
      "Epoch 16/20\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.0170 - accuracy: 0.9955 - val_loss: 0.0668 - val_accuracy: 0.9930\n",
      "Epoch 17/20\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 6.5546e-04 - accuracy: 1.0000 - val_loss: 0.0564 - val_accuracy: 0.9930\n",
      "Epoch 18/20\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.3595e-04 - accuracy: 1.0000 - val_loss: 0.1453 - val_accuracy: 0.9860\n",
      "Epoch 19/20\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 3.3760e-04 - accuracy: 1.0000 - val_loss: 3.8687e-04 - val_accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 2.7621e-04 - accuracy: 1.0000 - val_loss: 0.0023 - val_accuracy: 0.9995\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2af1d991288>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_size = 5\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=[None], dtype=tf.int32, ragged=True),\n",
    "    keras.layers.Embedding(input_dim=len(POSSIBLE_CHARS), output_dim=embedding_size),\n",
    "    keras.layers.GRU(30),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.02, momentum = 0.95, nesterov=True)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "레버 문자열일 추정 확률:\n",
      "BPBTSSSSSSSXXTTVPXVPXTTTTTVVETE: 4.67%\n",
      "BPBTSSSSSSSXXTTVPXVPXTTTTTVVEPE: 99.87%\n",
      "BB: 0.44%\n"
     ]
    }
   ],
   "source": [
    "test_strings = [\"BPBTSSSSSSSXXTTVPXVPXTTTTTVVETE\", \"BPBTSSSSSSSXXTTVPXVPXTTTTTVVEPE\", \"BB\"]\n",
    "X_test = tf.ragged.constant([string_to_ids(s) for s in test_strings], ragged_rank=1)\n",
    "\n",
    "y_proba = model.predict(X_test)\n",
    "print()\n",
    "print(\"레버 문자열일 추정 확률:\")\n",
    "for index, string in enumerate(test_strings):\n",
    "    print(\"{}: {:.2f}%\".format(string, 100 * y_proba[index][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "# strftime()의 %B 포맷은 로케일에 의존하기 때문에 사용할 수 있습니다.\n",
    "MONTHS = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\",\n",
    "          \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n",
    "\n",
    "def random_dates(n_dates):\n",
    "    min_date = date(1000, 1, 1).toordinal()\n",
    "    max_date = date(9999, 12, 31).toordinal()\n",
    "\n",
    "    ordinals = np.random.randint(max_date - min_date, size=n_dates) + min_date\n",
    "    dates = [date.fromordinal(ordinal) for ordinal in ordinals]\n",
    "\n",
    "    x = [MONTHS[dt.month - 1] + \" \" + dt.strftime(\"%d, %Y\") for dt in dates]\n",
    "    y = [dt.isoformat() for dt in dates]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input                    Target                   \n",
      "--------------------------------------------------\n",
      "July 21, 7829            7829-07-21               \n",
      "November 09, 7880        7880-11-09               \n",
      "May 29, 2675             2675-05-29               \n"
     ]
    }
   ],
   "source": [
    "n_dates = 3\n",
    "x_example, y_example = random_dates(n_dates)\n",
    "print(\"{:25s}{:25s}\".format(\"Input\", \"Target\"))\n",
    "print(\"-\" * 50)\n",
    "for idx in range(n_dates):\n",
    "    print(\"{:25s}{:25s}\".format(x_example[idx], y_example[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_CHARS = \"\".join(sorted(set(\"\".join(MONTHS) + \"0123456789, \")))\n",
    "OUTPUT_CHARS = \"0123456789-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_str_to_ids(date_str, chars=INPUT_CHARS):\n",
    "    return [chars.index(c) for c in date_str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_date_strs(date_strs, chars=INPUT_CHARS):\n",
    "    X_ids = [date_str_to_ids(dt, chars) for dt in date_strs]\n",
    "    X = tf.ragged.constant(X_ids, ragged_rank=1)\n",
    "    return (X + 1).to_tensor()\n",
    "\n",
    "def create_dataset(n_dates):\n",
    "    x, y = random_dates(n_dates)\n",
    "    return prepare_date_strs(x, INPUT_CHARS), prepare_date_strs(y, OUTPUT_CHARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = create_dataset(10000)\n",
    "X_valid, y_valid = create_dataset(2000)\n",
    "X_test, y_test = create_dataset(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(18,), dtype=int32, numpy=\n",
       " array([13, 36, 25, 36, 34, 35,  1,  5, 10,  2,  1,  7,  5,  8,  4,  0,  0,\n",
       "         0])>,\n",
       " <tf.Tensor: shape=(10,), dtype=int32, numpy=array([ 5,  3,  6,  2, 11,  1,  9, 11,  3,  8])>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0], y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "313/313 [==============================] - 10s 21ms/step - loss: 1.8167 - accuracy: 0.3464 - val_loss: 1.4181 - val_accuracy: 0.4602\n",
      "Epoch 2/10\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.3399 - accuracy: 0.5132 - val_loss: 1.1718 - val_accuracy: 0.5745\n",
      "Epoch 3/10\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.0408 - accuracy: 0.6261 - val_loss: 0.8484 - val_accuracy: 0.6903\n",
      "Epoch 4/10\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.7343 - accuracy: 0.7248 - val_loss: 0.6391 - val_accuracy: 0.7517\n",
      "Epoch 5/10\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.5243 - accuracy: 0.7934 - val_loss: 0.4278 - val_accuracy: 0.8307\n",
      "Epoch 6/10\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.7263 - accuracy: 0.7457 - val_loss: 0.3936 - val_accuracy: 0.8446\n",
      "Epoch 7/10\n",
      "313/313 [==============================] - 5s 17ms/step - loss: 0.3072 - accuracy: 0.8870 - val_loss: 0.2466 - val_accuracy: 0.9114\n",
      "Epoch 8/10\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.1965 - accuracy: 0.9416 - val_loss: 0.1648 - val_accuracy: 0.9532\n",
      "Epoch 9/10\n",
      "313/313 [==============================] - 5s 17ms/step - loss: 0.7316 - accuracy: 0.7638 - val_loss: 0.3522 - val_accuracy: 0.8903\n",
      "Epoch 10/10\n",
      "313/313 [==============================] - 5s 17ms/step - loss: 0.2215 - accuracy: 0.9425 - val_loss: 0.1437 - val_accuracy: 0.9697\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 32\n",
    "max_output_length = y_train.shape[1]\n",
    "\n",
    "encoder = keras.models.Sequential([\n",
    "    keras.layers.Embedding(input_dim=len(INPUT_CHARS) + 1, output_dim=embedding_size, input_shape=[None]),\n",
    "    keras.layers.LSTM(128)\n",
    "])\n",
    "\n",
    "decoder = keras.models.Sequential([\n",
    "    keras.layers.LSTM(128, return_sequences=True),\n",
    "    keras.layers.Dense(len(OUTPUT_CHARS) + 1, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    encoder,\n",
    "    keras.layers.RepeatVector(max_output_length),\n",
    "    decoder\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"Nadam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ids_to_date_strs(ids, chars=OUTPUT_CHARS):\n",
    "    return [\"\".join([(\"?\" + chars)[index] for index in sequence]) for sequence in ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9999-09-27\n",
      "1789-07-14\n",
      "1111-05-01\n",
      "2000-05-02\n",
      "1789-07-14\n"
     ]
    }
   ],
   "source": [
    "X_new = prepare_date_strs([\"September 27, 9999\", \"July 14, 1789\", \"May 01, 1111\", \"May 02, 2020\", \"July 14, 1789\"])\n",
    "ids = np.argmax(model.predict(X_new), axis=-1)\n",
    "for date_str in ids_to_date_strs(ids):\n",
    "    print(date_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = X_train.shape[1]\n",
    "\n",
    "def prepare_date_strs_padded(date_strs):\n",
    "    X = prepare_date_strs(date_strs)\n",
    "    if X.shape[1] < max_input_length:\n",
    "        X = tf.pad(X, [[0, 0], [0, max_input_length - X.shape[1]]])\n",
    "    return X\n",
    "\n",
    "def convert_date_strs(date_strs):\n",
    "    X = prepare_date_strs_padded(date_strs)\n",
    "    ids = np.argmax(model.predict(X), axis=-1)\n",
    "    return ids_to_date_strs(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['9999-09-27', '1789-07-14', '1111-05-01', '2000-05-02', '1789-07-14']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_date_strs([\"September 27, 9999\", \"July 14, 1789\", \"May 01, 1111\", \"May 02, 2020\", \"July 14, 1789\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_id = len(OUTPUT_CHARS) + 1\n",
    "\n",
    "def shifted_output_sequences(Y):\n",
    "    sos_tokens = tf.fill(dims=(len(Y), 1), value=sos_id)\n",
    "    return tf.concat([sos_tokens, Y[:, :-1]], axis=1)\n",
    "\n",
    "X_train_decoder = shifted_output_sequences(y_train)\n",
    "X_valid_decoder = shifted_output_sequences(y_valid)\n",
    "X_test_decoder = shifted_output_sequences(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "313/313 [==============================] - 10s 22ms/step - loss: 1.6644 - accuracy: 0.3742 - val_loss: 1.3989 - val_accuracy: 0.4685\n",
      "Epoch 2/10\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.1791 - accuracy: 0.5550 - val_loss: 0.9202 - val_accuracy: 0.6569\n",
      "Epoch 3/10\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.6615 - accuracy: 0.7618 - val_loss: 0.4463 - val_accuracy: 0.8422\n",
      "Epoch 4/10\n",
      "313/313 [==============================] - 5s 17ms/step - loss: 0.2669 - accuracy: 0.9256 - val_loss: 0.1328 - val_accuracy: 0.9748\n",
      "Epoch 5/10\n",
      "313/313 [==============================] - 5s 16ms/step - loss: 0.1199 - accuracy: 0.9793 - val_loss: 0.1812 - val_accuracy: 0.9517\n",
      "Epoch 6/10\n",
      "313/313 [==============================] - 5s 17ms/step - loss: 0.0459 - accuracy: 0.9967 - val_loss: 0.0307 - val_accuracy: 0.9981\n",
      "Epoch 7/10\n",
      "313/313 [==============================] - 5s 17ms/step - loss: 0.0552 - accuracy: 0.9912 - val_loss: 0.0283 - val_accuracy: 0.9985\n",
      "Epoch 8/10\n",
      "313/313 [==============================] - 5s 17ms/step - loss: 0.0161 - accuracy: 0.9998 - val_loss: 0.0123 - val_accuracy: 0.9998\n",
      "Epoch 9/10\n",
      "313/313 [==============================] - 5s 17ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.0082 - val_accuracy: 0.9999\n",
      "Epoch 10/10\n",
      "313/313 [==============================] - 5s 17ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.0059 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b1224ee848>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_embedding_size = 32\n",
    "decoder_embedding_size = 32\n",
    "\n",
    "encoder_input = keras.layers.Input(shape=[None], dtype=tf.int32)\n",
    "encoder_embedding = keras.layers.Embedding(input_dim=len(INPUT_CHARS) + 1, output_dim=encoder_embedding_size)(encoder_input)\n",
    "_, encoder_state_h, encoder_state_c = keras.layers.LSTM(128, return_state=True)(encoder_embedding)\n",
    "encoder_state = [encoder_state_h, encoder_state_c]\n",
    "\n",
    "decoder_input = keras.layers.Input(shape=[None], dtype=tf.int32)\n",
    "decoder_embedding = keras.layers.Embedding(input_dim=len(OUTPUT_CHARS) + 2, output_dim=decoder_embedding_size)(decoder_input)\n",
    "decoder_lstm_output = keras.layers.LSTM(128, return_sequences=True)(decoder_embedding, initial_state=encoder_state)\n",
    "decoder_output = keras.layers.Dense(len(OUTPUT_CHARS) + 1, activation=\"softmax\")(decoder_lstm_output)\n",
    "\n",
    "model = keras.models.Model(inputs=[encoder_input, decoder_input], outputs=[decoder_output])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"Nadam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit([X_train, X_train_decoder], y_train, epochs=10, validation_data=([X_valid, X_valid_decoder], y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_id = len(OUTPUT_CHARS) + 1\n",
    "\n",
    "def predict_date_strs(date_strs):\n",
    "    X = prepare_date_strs_padded(date_strs)\n",
    "    Y_pred = tf.fill(dims=(len(X), 1), value=sos_id)\n",
    "    for index in range(max_output_length):\n",
    "        pad_size = max_output_length - Y_pred.shape[1]\n",
    "        X_decoder = tf.pad(Y_pred, [[0, 0], [0, pad_size]])\n",
    "        Y_probas_next = model.predict([X, X_decoder])[:, index:index+1]\n",
    "        Y_pred_next = tf.argmax(Y_probas_next, axis=-1, output_type=tf.int32)\n",
    "        Y_pred = tf.concat([Y_pred, Y_pred_next], axis=1)\n",
    "    return ids_to_date_strs(Y_pred[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1789-07-14', '2020-05-01']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_date_strs([\"July 14, 1789\", \"May 01, 2020\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "313/313 [==============================] - 25s 69ms/step - loss: 1.6936 - accuracy: 0.3574 - val_loss: 1.5185 - val_accuracy: 0.3802\n",
      "Epoch 2/10\n",
      "313/313 [==============================] - 22s 71ms/step - loss: 1.3799 - accuracy: 0.4676 - val_loss: 1.1978 - val_accuracy: 0.5454\n",
      "Epoch 3/10\n",
      "313/313 [==============================] - 23s 73ms/step - loss: 0.9244 - accuracy: 0.6618 - val_loss: 0.6172 - val_accuracy: 0.7831\n",
      "Epoch 4/10\n",
      "313/313 [==============================] - 23s 73ms/step - loss: 0.3462 - accuracy: 0.9014 - val_loss: 0.2196 - val_accuracy: 0.9411\n",
      "Epoch 5/10\n",
      "313/313 [==============================] - 22s 69ms/step - loss: 0.1009 - accuracy: 0.9885 - val_loss: 0.0622 - val_accuracy: 0.9953\n",
      "Epoch 6/10\n",
      "313/313 [==============================] - 21s 66ms/step - loss: 0.0639 - accuracy: 0.9930 - val_loss: 0.0294 - val_accuracy: 0.9992\n",
      "Epoch 7/10\n",
      "313/313 [==============================] - 21s 67ms/step - loss: 0.0216 - accuracy: 0.9999 - val_loss: 0.0185 - val_accuracy: 0.9997\n",
      "Epoch 8/10\n",
      "313/313 [==============================] - 21s 66ms/step - loss: 0.0131 - accuracy: 1.0000 - val_loss: 0.0145 - val_accuracy: 0.9994\n",
      "Epoch 9/10\n",
      "313/313 [==============================] - 22s 69ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.0078 - val_accuracy: 0.9999\n",
      "Epoch 10/10\n",
      "313/313 [==============================] - 22s 71ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.0063 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b11743ba48>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow_addons as tfa\n",
    "\n",
    "encoder_embedding_size = 32\n",
    "decoder_embedding_size = 32\n",
    "units = 128\n",
    "\n",
    "encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "sequence_lengths = keras.layers.Input(shape=[], dtype=np.int32)\n",
    "\n",
    "encoder_embeddings = keras.layers.Embedding(len(INPUT_CHARS) + 1, encoder_embedding_size)(encoder_inputs)\n",
    "\n",
    "decoder_embedding_layer = keras.layers.Embedding(len(OUTPUT_CHARS) + 2, decoder_embedding_size)\n",
    "decoder_embeddings = decoder_embedding_layer(decoder_inputs)\n",
    "\n",
    "encoder = keras.layers.LSTM(units, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_embeddings)\n",
    "encoder_state = [state_h, state_c]\n",
    "\n",
    "sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "\n",
    "decoder_cell = keras.layers.LSTMCell(units)\n",
    "output_layer = keras.layers.Dense(len(OUTPUT_CHARS) + 1)\n",
    "\n",
    "decoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell, sampler, output_layer=output_layer)\n",
    "final_outputs, final_state, final_sequence_lengths = decoder(decoder_embeddings, initial_state=encoder_state)\n",
    "Y_proba = keras.layers.Activation(\"softmax\")(final_outputs.rnn_output)\n",
    "\n",
    "model = keras.models.Model(inputs=[encoder_inputs, decoder_inputs], outputs=[Y_proba])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"Nadam\",metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit([X_train, X_train_decoder], y_train, epochs=10, validation_data=([X_valid, X_valid_decoder], y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1789-07-14', '2020-05-01']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_date_strs([\"July 14, 1789\", \"May 01, 2020\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_sampler = tfa.seq2seq.sampler.GreedyEmbeddingSampler(embedding_fn=decoder_embedding_layer)\n",
    "inference_decoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell, inference_sampler, output_layer=output_layer, maximum_iterations=max_output_length)\n",
    "batch_size = tf.shape(encoder_inputs)[:1]\n",
    "start_tokens = tf.fill(dims=batch_size, value=sos_id)\n",
    "final_outputs, final_state, final_sequence_lengths = inference_decoder(start_tokens, initial_state=encoder_state, start_tokens=start_tokens, end_token=0)\n",
    "\n",
    "inference_model = keras.models.Model(inputs=[encoder_inputs], outputs=[final_outputs.sample_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c4e1c02ba9a4315c9c9b9f3ccdc568bf0028a114bbf7c4447cf8df78c88a2f71"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 ('handson-ml2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
