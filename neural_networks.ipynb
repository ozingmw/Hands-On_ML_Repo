{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data[:, (2, 3)]\n",
    "y = (iris.target == 0).astype(np.int64)\n",
    "\n",
    "per_clf = Perceptron()\n",
    "per_clf.fit(X, y)\n",
    "\n",
    "y_pred = per_clf.predict([[2, 0.5]])\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_full.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255.\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "X_test = X_test / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[0], cmap=\"binary\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 0, 7, ..., 3, 0, 5], dtype=uint8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Coat'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names[y_train[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqkAAAEjCAYAAAAR5ZjkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAADXdklEQVR4nOydd5hkRbn/PzV5Z2dnZvOysCwLLHHJSFAQkIwgmAiCil4DoldFrmIAFRX1chXjT0QFFBEQEZUgmMhJcg4CyyY2h9mduJPq90edb3X16Z7Z2dkJp5f6Ps88093n9OlTdareeuv7JmOtJSIiIiIiIiIiIiJLKBvtG4iIiIiIiIiIiIhIIyqpERERERERERERmUNUUiMiIiIiIiIiIjKHqKRGRERERERERERkDlFJjYiIiIiIiIiIyByikhoREREREREREZE5RCV1FGCMmWeMOaKPYwcbY14a6XvaXNBf32YdxhhrjNl+Y49t4JpnGmPu2/S7G3nE/shH7I+IiIg3GkZUSTXGvM8Y86gxpsUYs8QYc5sx5qBNvOZdxpiPDNU9buC3WoK/XmNMe/D+9KH4DWvtvdbaHTdwH0UVMWPMacaYa4wx2ySLVsVQ3NNgYYw5yBjzgDFmrTFmtTHmfmPMm0bznkYCyZhcY4ypHu17GS4YYw41xiwa4LmxP/LPjf0xPL9Z0uvLUOON3h/JOtlujGk2xjQla9FZxphIzlE642PEHpYx5nPAD4FvA1OBrYGfASeO1D1sKqy1dfoDFgAnBJ/9brh/fwBK59uBvw73fQwExph64BbgJ8AEYEvgQmD9aN7XQLApyr0xZhvgYMAC7xiqeypVxP7IR+yP4cHmsL4MJWJ/eJxgrR0HzAS+C5wHXF7sRGNM+Uje2GiipMaHtXbY/4AGoAV4bx/Hq3Edtjj5+yFQnRwbj1N2VgBrktdbJccuAnqAjuT6Px2J9iS/PQ84op/jk5J7bQJWA/cCZcF3/wd4GlgL/B6oSY4dCixK/c55ybnrgWuBXqA9afMXkvPKgGXJ7y7ALYItyd+ByfHzgfnAcuAqoCH57jbJ+R9L+n8J8D+b2D/7Ak19HDsTuA/4XvJMXwOOTY2Xy5P7eB34FlCeHNsOuANYBawEfgc0FnsuwM7JtU9L3h8PPJk8kweA3fvp54pBtvurwP3AJcAtqWO/Bv4fcCvQDPwb2C44boHtk9cHAQuBQ4scq076bkHyzH8OjOmnr+8HfpqMtReBw4Pj04GbcGP0FeCjG5qXwNhk/PUGY2x67I/YHxvbH0Pxx2a4vsT+GJJ+mEdqjQb2S8blnGS+XYojdlqBI5Lx/sek/a8Bn05991FgXTKvLkk+rwGuxq1JTcAjwNTRbv/mMj5GqlOOAbrpY+EHvgE8BEwBJuMUiG8mxyYC7wZqgXHAH4A/B9+9C/jIKDzoggmQOv4d3OJQmfwdDJjguw8nE2IC8AJwVnLsUAqV1CeBGSQLTR+T7wDgweT1NrhFqyI4/mHcIrMtUAfcCPw2df61uAVmt2QQ9tm+AfRPfTJpfwMcC4wPjp0JdAEfBcqBTySTQf3zJ+Cy5F6mJH318eTY9sCRyUSaDNwD/DD9XIC9cYv08cnne+GU8/2T3/xgcm51X/08yHa/ApwN7JO0cWpw7NdJn+wHVOAU7OuC4zZp3zE4BWS/9LHk9Q9wisME3Jy4GfhOH/dzJm7unYMbh6fglJEJyfF7cDvoGmDP5Lm/bQDz8lCCcRr7I/bHYPpjKP7YDNeX2B9D0g/zKLKG4daFTyTzbS3wFhyJUws8httIVuHWyrnA0cn3HgTen7yuAw5IXn88mWO1uLVlH6B+tNu/uYyPkeqU04Gl/Rx/FTgueH80MK+Pc/cE1gxnpwywTUUnQOpB/4Vk4Sjy3TOC9xcDP09eH0qhkvrhDf028E3gguT1NhQqqf8Czg7e74hbJCuC83dK3dPlm9hHOyeCYFEyKW7CmRbOBF4JzqtNfn9acnw9gaIInAbc2cdvnAQ8keqbC5PfPDT4/FJNtOCzl4BD+urnQbT3oKRPJyXvXwTOCY7/GvhV8P444MXgvQW+hGO756SuLQXF4Hb9IcN2IPBaH/d0JsEGIPnsYeD9OIW8BxgXHPsO8OvkdZ/zMj1OY3/E/tjY/hiqPzbD9SX2x5D0wzyKK6kPAV9J5ttVwef7AwtS534JuDJ5fQ9ubZmUOufDpCxzWf4rtfExUj6pq4BJ/fj6TccJXmF+8hnGmFpjzGXGmPnGmHW4gdKYJf8RY8zWYVBV8vH/4ViTvxtj5hpjvpj62tLgdRtuZ9YXFg7gNo6jf3/UYn1cgVMKi/2OfwaDhbX2BWvtmdbarXDmlek40wEE7bfWtiUv63C+Q5XAksTZvQnHqk4BMMZMNcZcZ4x5PRkPV+NcHEKcBTxgrb0r+GwmcK6umVx3RqqNA+nn/vBB4O/W2pXJ+2uSz0Js6Ll/FrjeWvtsH78xmWTHH7Tj9uTzvvC6TSRIAj3b6cBqa21z6tiWyes+5+UAEfsjH7E/hgeb9foyCMT+6B9b4txXIF/mzwSmp9aIL5NbI/8L2AF40RjziDHm+OTz3wJ/A64zxiw2xlxsjKkc9lYMHiU1PkZKSX0Qx46d1MfxxbgBImydfAZwLo71299aWw+8NfncJP9D4ToqsNYusPlBVVhrm62151prt8UFSHzOGHP4YH+iv/fGmGnAFsDjfZwPxfu4G+dbI8xIHV/MEMFa+yJu5zpnA6cuxI2VSdbaxuSv3lq7a3L827j27ZaMhzPIjQXhLGBrY8wPUte9KLhmo7W21lp7bXibg2sdGGPGACcDhxhjlhpjluJMqHsYY/bYiEu9FzjJGPOZPo6vxPn77Rq0o0Hjrg9saYwJ+0jPdjEwwRgzLnXs9eR1f/Oy376K/ZGP2B/Dis16fRkEYn/0AeOyy2yJi4mA/PYsxFkcwjVinLX2OABr7cvW2tNwhMn/AjcYY8Zaa7ustRdaa3cB3oyLffjAiDVq41FS42NElFRr7Vqcn8f/M8aclGjjlcaYY40xF+N8Ic83xkw2xkxKzr06+fo4nNBtMsZMAL6WuvwynO9IpmCMOd4Ys30i/NfizGa9Q3T5dJuPBW4P2JAVyW+F51wLnGOMmWWMqcMpe7+31nYH51yQPJtdgQ/hAroGBWPMTsaYc40xWyXvZ+DM9g/19z1r7RLg78D3jTH1xpgyY8x2xphDklPG4Zyy1xpjtgQ+X+QyzTi/m7caY76bfPZL4CxjzP7GYawx5u2pBXhTcBLuGe+CM4HsiXN3uJeNE1iLgcOBzxhjPpE+aK3txbXlB8YYsctbGmOO7ueaU4BPJ3Puvcl9/dVauxBnpvqOMabGGLM7ji3Q3OtvXi4DJhpjGvr4zZOI/RHiJGJ/DAveiOtLf4j9UYhkLTkeuA642lr7TJHTHgaajTHnGWPGGGPKjTFzEsUWY8wZxpjJyRxrSr7Ta4w5zBizW8ImrsO59AzVWj/kKLnxMZS+Axv6w/lCPIrzmVqKi2J9M84p/8e4aO4lyWtFu0/H+Tm0AP/BOSlbEn9LnL/Vf3CRZj8ewbbMo3+f1HOSc1px/pEX9PVd4Ou4iQPFfVLT/qcn4py/m3BZAm4A3pM65xs4ZbUJF1RVhhtsC5PPryYJZqIwun8pSdaATeifLYHrcaxLa/L/MlxA1ZnAfanzLbnAjwacD+kinIL/BHBqcmxXnHN7Cy7Q6dy++gsXOPIUOafvY3CRl03JOPsDib/dhp7nANp7O/D9Ip+fnPRnBY5J/lZwLP2swz6YhTOzfKTIsRrcJmMuTii+QBCFmvr9M8mP3v4PcFRwfCtchOZqnC/SWcGxPudlcvwKchGt02N/xP4YaH8Mxx+b0foS+2NI2j8Pp1A1J2P7QeCT5DLF5M23oP3XJv21BkeqaD25Ghd82wI8B5yUfH4aLr6hFaek/ZhBZoeJ46PwT9HUESUK4/xKlgLbWmvXDfIa2+DSbVTafGY1IiIiIiIiImJUECsvlD4m4FjaQSmoERERERERERFZRGRSIyKTGhEREREREZE5RCU1IiIiIiIiIiIic4jm/oiIiIiIiIiIiMwhKqkRERERERERERGZQ18VBwaCUfcTsNaSn4N6ozDoL/aBjeqPZ591BWNaW1t54YUXALj00ksBuOaaawDYbrvt+r3Gffe5fMTf+ta3APjmN79Jebkr/DBr1iwAxo8fP9BbGtX+yCBif+RjqPsDYp+kEfsjH4Pqj9CFLb0+HHfccdTVuboG3d3O/f7oo4/m4x//eN55vb0uzWVZ2SbxOKPaH/31w5133gnAJz/5SaqrqwHo6Ojw37v55psBmD17dt73ent7/bUGsfZmYnyE+Ne//gXg1+Cdd96Z7bffPu+cpqYmmpqaALjhhhsAOPTQQwE45phjGDt27GB/PhPjo9hz1Hzo7e3lxBNPBGD1alek6/bbb2fFihUA/OMf/9io624ARb+wKT6pQyZQpbD98Y9/5N///jcAPT09AEybNo2dd94ZgMMOOwyA/ffffyh+dlQGyNVXu5y4LS2ueurkyZPZcccdAfjSl74EwF133QXAVlttxZvf/GYAxowZ44+98sorAKxfvx5wQhbghz/8IU8//TQAy5a5QlIzZ87kHe94x0BuLXMCZJQR+yMfUUktRBwj+cjsovv5z7uaH5dddplXQrToVlVV8etf/xrAy9shQubGxx//+EcA3vOe9wCwxx57sGbNGgCvbFVXV/P8888DcNNNNwG5NSbvZjZeGRnV/mhtbQXgi1/8Ii+++CKQW4e32WYbwK25Gh9SxF599VW/oRHmzZvnX2vTc9ttt23k7WdnfKxc6So1n3baaQDcf//9gJsb2rDpOff29noyTJ/9/Oc/B+CUU04puHZPT48/fwPIjpKqCfBf//VfADz66KOA29lWVDhyVzvYsrIyv8PTZzvssAMA5557Lh/5yEcGexsjPkBuueUW7rjjDgDOOOMMABYvXkxjYyOAV1a1i73kkkv8xNLEeeaZZ5g0yZWq/8IXvgDA+973PgAeeeQR31e1tbUAXHfddRxzzDFAcUETIDMTJiOI/ZGPqKQWIo6RfGSmPz7zmc8A8PDDDwO5RXjChAksXOjKtUvujhs3jvb2dsApKQCf/vSnAceUbQKrOuL9Ucy6eOmll/KHP/wBgP/85z+AazPACSec4BVz6QJ/+MMfeOKJJ4Ac2zxjhquY/c53vpP//u//zrt+b2/vQPtmVMeH7rupqcmvoYKU1ZqaGq90anxUVFR4YkiQntLS0uK/K8W/mKLWB0asP4ptKB544AHA6RFPPvkkAPX19QBMmTIFgOXLl/vzxbgDnlmeNm0agJ9T48eP52tf+xrAYHSzov0RfVIjIiIiIiIiIiIyh2FnUovtQqdOnQrkdrcNDQ3ugtZSWVkJ5HZw5eXl3vQvyDyx1VZbeQ2+6A32b44Y8V3dT3/6U15//XUAdtllFwC23nprf7ympgbI7eZ7e3u9z8e6dS5X/3777cfkyZMBxwoAzJ07F4Curi7f34sWLfLHxKp+9rOf7e/2MsOCZASxP/IRmdRCxDGSj0z0x6WXXsrFF18MwJw5c4DcmrF69WrPFrW1tQHOzL3FFlsAsHTp0rxjYpgGiRHvj5DV/OUvfwk4VwcxoVpXZaFbuHChXxe0jtx0001sueWWQI5N1Br8+uuv88lPfhKA73znOxt7/6MyPhS7ceGFFwKOBZRPadqML4YUcmb8jo4Or6uoPzROKioq/HfEtv7qV7/aYDxJglHpjyuvvBLI9Udvb6/Xu6Q/SBdZunQpM2fOBHJj4Nlnn/UMquZSV1cX4HQt6SqKi5E1Awank0UmNSIiIiIiIiIiInPYlOj+DaKYr0pTU5NnUqWti+nbaaedvL+qNO2pU6d6DX7BggVAvi/R448/DsDee++d97uwyZGZQ46nnnrK+502NzcDbpemoKiqqiogtyOrr6/3Oz45Hnd3d7N27VrA+bNCrh8ht6OR03dNTY33Q4rYvBD6n2lMWGvp7OwEcn5Cet/V1eX9ijSHpkyZ4udX2k9rxYoVnvnfc889h68hERFDiLvvvtvLRMnDiRMnAk7Gag4o80lVVZWfA5LFkq2PPfYY++yzz8jd/CYiXPOuv/56wPkNav1QsK3ez5w50zOuWjd32GEHLzPUL1qbtthiC+6+++7hbsaQ4qCDDgJycR133XVXATMq1jSEfE07Ojr8eBLzKp/MSZMm+Zgasatf//rX+e1vfzsMLRkaXHDBBUBO7+rp6fHjRkynYlsmT57s+0iBhjNnzvRWXI0PjSdrrbf0av155JFHeNOb3jTo+82WFhcRERERERERERHBMDGpxZjMAw88EID58+cXpDQQ61dbW+uPvfrqq4BjT8U+Kk2ENPTly5dz5JFH5v3WihUr/Ou0lj/aqKmp8dFyatOSJUu874bYVe1w6urq/GfqlylTphS0R7vj9evXe0ZN5yxevNh/dxPyl2UO/bUlPCYmRX1QVVW1WbQf8tv+oQ99CIDXXnvNfyYmQH2wdOlSvyvWdydPnuxZAfke7bvvvgAcf/zx/O53vwPgiiuuGKZWDA7p578p1pPNaV6MFObPnw/An//8Zz71qU8B2ZGz69at80yQ5KGY1Lq6urz1BpxFTnNA//X9hx9+uKSYVIBVq1YBOYtcTU2Nnx9ikUPmS5HcynBQVlbmmUWtofpfVlbmrSvy892IXNyjAt27fGm/+MUvel/ls846C8hZkcSwQr5/quSmxoV8MufNm+etTBo73/ve94ahFUODzs5On2pM8q6np8f7KKfncE9Pj+8T6WTTpk3zPtsaV0JXV5e3Ruj6f/rTnzyTOhgZOyxKangj5513HpCbMFtvvbWnzEWh68EvXLjQDx4Jl8bGRn88zE0GsO222/qgKzl9f+xjH+MXv/gFkB2hKXOAtdYr2qLOt912W99WtV1YvHix7yOZX5577jmfPkRuE5ocbW1tXvBqEM2YMcMrK8qhusceewxtA0cB4RiTO4NSk33/+98HnLvExz72sZG/uRFCV1eXd3hX7uDnn3/eCwn91zzYbbfdvMDW5qe1tdULY7nTaBFva2vz+SWzhrSwC5XUe+65B8ilaJs9e7Zvt+afUsDtsssuBddqaWnxbkeSOVqU3vrWtw5xS0YO2sxWV1fz97//HYDTTz8dyOXP3FD7rrrqKgCfouizn/0s9957L5BLcD7akKkechs1PeOtttrKzxnNi6qqKh/sIbkp3H///XziE58Y9nseSijYS8+7srLSr6FStmS+X79+fUFqx/nz5/vj6g/NH2utv5bWk0MOOWQ4m7PJ0HMO11cpp6HZHvLTLElPqaio8J9rPOn8lpYWzjzzTCDnTqB1OYt44IEH/FjXWGhra/NyUWNGG5Gamhqvq2ijV1NTk+dCBuTpNWkXkdtvv51vf/vbg77naO6PiIiIiIiIiIjIHIadSX3wwQcBxxjqmHYoMrNJoy8vL/fHZGJ59dVX/W5HlaeULqS9vd3T1HLkfeaZZ4ajSZsEJeefNm2a38WL/Vu7dq1PQxUGQIFLzaVdrpyRJ0+ezJIlSwB8dS4xo6tXr/aMssxx2267re8vVcTYHJjUEKqKIrOE+uyll17iZz/7GZDrv9mzZ3PccccBORcU7RBLDWH6OKU8qampKbBQhGYdMQHaMVdUVPhdscamqpVNnDjRz7msoT8Tve5f7OeYMWM8u6Zqbkrttu2223LdddcB8OMf/xhw1VM0XsQUiCU58MADfT+VGkLTnNyOxKJ/9KMfBdw4kkzVuIBcfyuVkb73j3/8g5NOOml4b3yAEOsnKwHkmC+1qb29vSCl4aJFi/xcEfSMX3755WG73+GCWG49s9AFRv0hZrCsrMz3h6wGXV1dnn1Uv6g/jDF+zj300ENA9pnUYhATKtZZsqKuri4vYApcn0mmSheRztLS0lJS7b/55pvz5jW4eS4dIbRqgxtDGj+y0kJuPIiVlf4FORZW3wtd0AaDyKRGRERERERERERkDsOagqqnp8f7M8g/rr6+3mvk0uj1v7q62jM8YXCVAjnkzK3dzNy5cz0Lpp39ypUrvW9dmCh/NCGn4UceecT7uqlM3VFHHeV3IXJ432uvvQDHLIe7OXA7GyX7V59qR1tbW+sZ2htvvBGAD3/4w95Rer/99huuJo4aVq1a5ftUSYpV0rC6utr7+4oZW7lypWde5bssZvld73qX7/tSg5g+Y0yf/mS1tbX+mPxOu7u7/Y43XRpyU/yIhhtpBjX0P1chC50TBtAp2EPsx1/+8hefvk5yZcaMGX5Oqm/EHJQqiwo5eQG5BOdihCRvH3zwQd9vkqldXV0+OGbXXXcFcv7L06ZNK0hdNloQ67l+/fqC8aG1o6KiwsvUcMykmbJSft5Ky5gOKoScn6WOhX2gz6y1Xmao/Zo/VVVVflxo3SkVhIHUGsdiUvXcq6urvUVO7Crk5oL+63z5YpYKJP9CVFZWcv/99wM5RlRrQEdHh19DZaEYN26c19nU/qeeegpwlmGttWLy6+vrvR4YMq4DxbAqqfPnz/cNk5Do6uryD1omBw2e7u5u/5kiDjs7O72pRiYqLbTjx4/335VyG1aHyIqSevzxx/v/GiR//etfAWeyf9vb3gbklApVaNhtt91826XYr1mzxl9DAkeLz9SpU70LgBaf888/P/PRl2kMJNpaz72urs73nz5T5oTvfve7viKGgs0mTZrkTeM6T1GLX//61/nLX/4ypG0ZThSrFldTU+MVq2L9J+Ea5sXTeRI8OqeUEI4ZLS5aYNeuXevdXxQwdcIJJwDOXK15pMCRqqqqAuVECnwpoliWE7lhqZ1q35QpU/xnGg8dHR1enmgDIJOeXGeyAN1TT0+PHwOSn1pXxo4d602Ukps9PT1+IVbQi84ROVBK0AZCsNb6HJ5qXygbNP6lzFZWVvr5pLGjNSfMuar+LhWEVSzlxqTPpDN0d3f7sRPOl3RlKo2L0NUla9mEiqG1tbWgqmdra6vXEdQu6W0TJ07066XcqLq6uvIUUMjpX0uXLvV9KWW1ra2N5557DoCDDz54o+85mvsjIiIiIiIiIiIyh2FlUhXEAzmWsLW11bOq2t1Ko29vb/e7W2n0bW1tnnkVg6qdSktLi9/xyqTd09PjtfawClVWoB2Lgpg+9alP+R2sdrkvvPAC4PJV6nx9Nn36dE+Z/+tf/wJyLOHLL7/sd4jf+ta38n6vVGCt9f0R5vKD/N2/mOjf//73PPLII4ALeIGcKXPs2LF+XGh3d8ghh3hWSGNH41HMaqkgzOMXsqra1YotVR3ujo4OvxtWH3R3d/t5pevpWCkhHBuyOshhf5tttvFj6qWXXgJygZzNzc1+/iggsbe311tyZAbOagDZQJBm1F9++WXvWqSxIeakrKysILdwe3u7Z1XDNFY6PyvQHF+6dKnPGyw5K6Z47dq1vg2SDTU1NTz77LMAvPOd7wRcqp7wmqUEyc1wfX33u98N4NOz6RlXV1f78SH599JLL3lZoOd+wAEHAM7qpGcepmsqBYTyUuNBn8m83d3d7ftPLHxFRYXXR3S+xksxtjXLTOprr71WIA/a2tr8c1aubLHCK1as8LqbdIn169d78736SPOkrq7Ou9NIfnR3d/sqZZFJjYiIiIiIiIiI2CwwrEzqc88953dd8ml5/fXX2W233YDcjkO7ms7OTq99i93o7u72x6XdawcXMkNy3i8vL/f+Vu9///uHsXUbj9D/T22vqKjwjJ5YGzFbDz30EO973/uAHBM9d+5cvwuW87eYgblz5/o+CtNZlYKvTMiWpu8z3PlpHCkI7B//+IdnRL7+9a8DOYakoaHB+xkKc+fO9WNLDKrOX716deaC7vpD2C8aAy0tLcyePRvI7fp1bMWKFd5Sod1uRUWFv07ailFKCPtCwZQaR6Ef/O233w7ArbfeCrj2azyo3d3d3f56mndZCQ4aDNJs55///Gfvl6ZxoPkXyqgwgEoyWGNJvqlhAZHRRhgUsssuuwC5VGNaV8Lk9WKP6urq/HExyyoSM2/ePM8WSU5kHWK5tAa8+OKLXH/99QDeyigf1bBmvSwPWn8gx7iqWtPJJ5/sWcdS810PmU6lUhMUSLls2bI8XUKQniE5or4NA6dCpjarWLRokb9PBc9++tOf5je/+U3eZ5KJ1dXVfgzoGOTaLRmhsXDSSSd5q4yKGFVWVvrg5sEgMqkRERERERERERGZw7Cq/osWLSrqXygmVDtUaephMv/Qry4daatzOjo6/DWk+dfW1vLiiy8OW5uGCvI/bWho8H0kvw4VMnjyySf5yU9+AuT8hZ599lm/U9ZuUMxAT0+P3wWLEQiPZwX9Re93dXV59kpshpjmqqoqn1rrb3/7G+CitMU2q768/MkaGxs9yyPWeeXKlZ5Z1nXV/1dddZUvEznUTOpg68N3d3cX7NA1X8J5cdlllwEuxYeiU8V2hf5Daruuod+AwjQrYdnVkcaG+itMUdfXeWFmDLXp1FNPBXJs0b333uvHmyJVy8vLfXJr+Z6F6WiyiL76q7e3t2D+X3PNNZ4tUh/114+QmysqfSoGdsWKFd6PbbQhn2LIWQXkexvWng/ZQ8j1AeQyyIiJfeKJJ3whCFkosoyOjg6/dortq6ys9Ouj+kPH1q9f7+d4mGVH67U+K1Z7PV0AoZQgmady7SoLLJkZIpS/8tcXE11qbPK6deu8v72sIz/4wQ98kRMxnpKF4VjQmFm+fLm/htZoWa8PPPBAP/60RodZhwaDYVVSX3jhhaLCMz0B0qanEL29vX5B1WDR9yoqKgqCsKqqqvzCkmXogTc2NvrXotNFl8stAnJmmmOPPdYLYNXMVh9PnDjRD5osmx5CIZoeH11dXX4caBLJbPDss8/yqU99Ku8aTz/9NP/85z+BXFCAnLONMV5J1f99993XKzhS3iRojjzyyMyZ+cPnWEw5veaaawBnwgU48cQT/SZN7ZOSUlZW5seaTJft7e151WTC7y1YsMCnH8kSenp6/LgpNs6VRkxm/4ULF3rTrtKiSF6Ul5f7hVvjoqenxwcJhKbPLKMv5TJUUJVH+NVXX/XV1jSmNOfCCkRCmK/5yCOPBHKb7CeffDIzSqoUzPC1xnIoF7X+qG/C4DnlzVS+5LKyMp+eqhSwYMGCgvVSCgXklLCddtoJcGNe4z7c+Gnca3Oi5z158uSCdEwrVqzw8yrLCGWFZIP6QXIu3MCEwVL6PCTIoHSCTLURC4msMH+u3OK0Lii9WG9vr5ctanNVVZU/ruB4KbV77rmnlwef//zn/fna6A0G2aLYIiIiIiIiIiIiIhhmJvWZZ57JC14QZF4LE4qD28Fpt1OMgU3v+GpqajxDEu4KxEiq+lI6eGa0UIztmDRpUkF1E5kS1q9f782O6qvnnnuuIB2M+qyysrLojnZjTczDjTCoK6wrD26nKid1MeJKn/LTn/7UO+0rJdDixYt57LHHALeLg1yqjMmTJ/uxoACK6upqzyzMmjULyPXftGnTCuq+DxXCZ5BOwh+O9TCARf/TVY+EK6+8kq997WuAY9jBpY9JF7gQi9zd3V1gJof8JN6QexbPPffcqDGp4f2lA//CgAbJFaUku/XWW73ZV+2ZOnWq3/lfe+21QM49aPHixX7OiEXo7Oz0DJL6XoEnSm2UdUhGVFVV+dcaK3vuuafvX1ltwnmYTuReUVHhrQ2qLKPzr7nmGk488cThbs6AIEY8rGojBknPOLTMaRyFwZpiTSVDli9fXlIm3aVLl/rnp3ZutdVW3tqkY2LTQtY8TFml/tDYue666wDHOKr4icbAggULSoJJDaH0jWIHZZ086KCD/Dk6FjLHaZeo3//+98yZMwfIdmCyUlhOmTLFy/vQyiKmXNZZ9UdZWZkfF6G1U59pvmitee211wrSTFVXV3vGWevxxoyXyKRGRERERERERERkDsPKpC5ZssTvakMfDmny2s1pt1ZTU+N3f9LMgYKdvY6FPof6XrjrkR9nVpjUYhg7dqzvG7VPLI+11vtyhOyzdjRpf7n169cPqjbucEOslPyhxFYuW7bMs13y/znwwAM9K/bd734XyO1uv/SlL3lGQIn7ly1b5v3Hdt99dyDXV1VVVd5XRp+FLIFqfeuc3t5ez7jtscceQ9b+ED09Pf2m2OqP9ZZl4PLLLwfgpptu8qlT5s2bB+Snj0qz1CtWrCjw5QxZJI07vX/iiSd4xzvesbFNHBKEu/10fzU3N/OnP/0JKEzWXl9f7xly7dpfeeUVz7Jr568AgYkTJ/ogIvVddXW1Z/vFKCi9UbHnlyVI/oWyQWWDZYVoaGjw/ZBmz0MmVeMnTNqu+SFfvhUrVmQmxZ2ed319vWfF00GBocVN472qqsr7sGotCv04S4lJXbZsWQFTNm7cOC9TxRCnLTZp6Boa/wpG3X777T2TKoS+wKWCG264AciNC839Z5991s8TWTRDyDc1TO9VCtB9hhbtkM1UARxZHrQm9vT0+LVTc6q1tdVbGjU3JEcefvhhPvCBD+T9dm9vr5cR8vmWb/tAMKxKali9JKxUogccmlvATRgJlTCYIy1owu+F9WchXzjLWT7L6Onp8YtAOjCst7fXC1L1Y2gql/IXVn9JBz2MNl577TUf2adJIReGPffc0y8Kd955J+DM1cpuoNxtP/3pT/21NCkkLMINiCaifmfixIl+sdH4mzBhQkE2Bb1vbm4ethrtG7uQ63k//fTT3tSsMa42H3LIIQU1lydMmFAQECZUVlb6+aE5FwZTpe8trBg33EgrRqEpSsqHMjvcfffdBcpYmKtTi4uuOWPGDG/ukkw47LDDAOeSpPPD3MzpCGmNlbvvvtsrfaOBsCJbqGiEeZdDnHDCCX4OKMPFo48+mpcxA3Lzo1iO2OXLl/sxp6h3jbeWlhZf8U1ViUYLmrtTp05l/vz5QG4chRs2KV5aY9ra2vxzTs+dqVOnehlVCmhtbWXhwoVAbiOxbt26gqpB4RqTnnvl5eV58hLchhVcpTb1qcah3CyyjlC+SVnafvvtgdx4rqur82NBMqaurq7f3NFygRGxkcWNbEiEpd0LIbf5TOc8Li8vz6s8B/l6V9pV7PHHH/ffTecjhsFVcIvm/oiIiIiIiIiIiMxhWJnUMJWFdqaTJ08uoNi1s21vb/e7OVHLYZUDHZO2v2bNGr8TEotWVlbmd4syj44m87EhVFZW5jHJIXp6evwuRDvftra2AjP/hiqiDDZH56ZAz7a2ttabjBWkoeff1NTkzQw6NmbMGM+86hrKZbdmzRrfRu3SVq5c6Xe+2i1qrG2xxRYFFclWrlzpq0qlqy+tW7du2JhU7ayXLVvGd77zHSB/1wkwffp03y7dx9ixY9l3330BOOKII4Bcf9x77715QVHgWESZcnUt9ffUqVM9Q6sx0dzc7F+nd9FhBZ/hRrH68uDYU5nmNR4aGxsLdulqf3Nzs2+vnuv69et9hRSxw0prts8++/h2qt+6u7v9b+m+xM7fddddwyZPQmY0bY4t5p5RDAo2/OhHPwo4a4IY6N///veAS1cmRvmZZ54BctaY8ePH+/kjObrNNtt4y4VYM7FHr7zyimepR5tJFVu4dOlS3w8KCAkDM0PLE7gxlK72d//99wNu7KiPSgEh46cxs3jxYt+udMBUXwGKYsgkb/XcFy1a5PtKZn/J7qwitOKCS1Enc7X6K7SYaKyHVjudp/EkNDY2cvPNNwM5JjVrLCrk1pPm5mbfD7JoQq6tYQUtcPInbeGFvlNxPf30015myfrS1NTkZassOBuDyKRGRERERERERERkDsPCpGpHW15eXrADD2tgpyuhhO/DmtJp537tBKqrq30lDNUdbmho8Jq/2JisIKyLrfZ1dXX5nVfa4T3cuehYd3d3QSqiMMl72n9kzJgxo5KCKvT30v2l29fd3e3Td8hX5eWXX/Y7MZ33lre8BXDMnnxnxB6XlZUV1FrW//b29rzUM+BYljTDqF1gfX19XqWu4cDFF1/s58SnP/1pIMeoLlmyxDusi9WcMmWKb59YZ7GBlZWVPpBMbEFnZ6d/3tr1i+no6OjwO2AxaWEBjfTzGcmCEPIL/dWvfgXkrCAVFRWe/dG8b29vL0gjpPctLS1+7KlPWltbfZ+IKVCaqrvuuos3v/nNQH4xAzFI+m1df1MqpwwU/VWIs9b656mAlQcffNAXc1A6tjPOOAOAb33rW/zwhz8E4Ec/+hHgxruqtKlgyKWXXgq4+SdW6TOf+Qzgno2YUwU/yoI1derUPoNvRhrqtwMOOCAv8APyCzWk/Z5D+aj5IVb4jjvu8L7KpYAVK1b48R8GJvdnZVP7ZVnq7OwsWIN0rTVr1vjXkg8jaXEZDMLAP4Crr77aj19ZpUKLbDoYatKkSf48WeY0R7bffnsvu7ISQFgMoS+onpfk3h/+8IeCyqCSnWGcUJgeNExXBuRZOGWFUFL/lStX+vE0mLESmdSIiIiIiIiIiIjMYVioErFcra2tBZr2lClTfFolRQ6GZefSrF8YSSZtXLuYRYsW+V28ds7z58/3u4KwlnPWoOjTrq6ugght7chqamr8ziZkK9J9pP4oKyvLS/oPeH/GkYai9evr631qKEXO6vk3NjYyffp0AF+O9OCDD/Z+hiEbLKgfwjGh5x2mLRN0DfncHHvssXl1iUNUV1f3y2JtCuRXuWTJEs/6v/TSS0DON2jcuHH+eWtMVFRUeCuEdvjazZeXl/u+0ZwLx4z6Uexz6G8bzg0xu+o/sYYjVQ5y1apVfOMb3wByc1xpYLq7u337Q1/29O4+RJguCvKj32XlEWM+adIk/2xklenp6fG+8GIMxE4tWbLEj6WhLokYzuu7774byD1zpRh7/fXXPZOqvpo6dSrvfOc7gVzSdd3vV7/6VX7yk58AsN9++wGOWVcKN7H0it5ubW31Y0oMbF1dnX8eskxovv7973/PTFlU+cYCnHvuuQAFxStCORpmktFYkc+cijaotGOpoKmpyVtQNN/Ly8v9PEkX+Fi/fn3BetLV1VVgzQqjuHVMv5PldTaE2v7KK6/4Z6/5JYtU6H+t/6G80XzR+3nz5nkLlwqFyIqRJYRxCnp+sqbdfPPNXhcLC3hAfllUfW/y5Ml+7ZLuoXPGjx/vrWEaY+E1BpPObViUVN3ImDFjCswt2267bUE1l/REgHyKXtdQo9Ux48aN8wJVx1pbW70SEtbhzRpCB+K0kAj7ITTHgpscmiDpHI7d3d2+b2SeGC0lVRWhLrzwQp+2RkJNVYxqa2vzqliAU8CkcGniSEmprq72fSNlo7q62i/I6WNjxozx40KLa6jUChp/HR0dXpANdQWVO+64A3AmFvWHlHWljFm5cmVBKrXq6mq/2KhdYc5X9VHYpnSOSClTO+20k98U6Hvjx4/35+u/lKDKysqiG4Whgsb21772Nd8HguZ8a2trgTm5tbXVtzcdJNXT01OQT7mnp8f3RdoU19vb69urBWvatGl5gTaQ2zS0tbVx8cUXA/Dtb397kC0vDgX7fO5zn/NjUvNCi+Nuu+3G3nvvnXdsxowZftH48pe/DOTSt40dO9bf+9NPP+1/S3JCfSsFdvLkyf6Za3Pz8ssvexeT/fffP+/769evZ/bs2UPS/qGEFKf+ZGsYHBSaNwEfXBm6p5UCwtzTmhvV1dUFKYOEsMpduInta7NeUVFRkKdb4yTrkFl+6dKl3vwtHeGhhx4CHPGl8xRc1dHR4eWL+lRzddGiRd6FSC4xWVRS5RpUWVnpZbnGwuOPP+6Pa5yErgvpQENjjFdw1Wads2LFCk+iSA+rqqryY6sYkbQhRHN/RERERERERERE5jAsTGpoKtMuTcxhR0eH3+mF1Q+ENONRXV2dV20JckxSRUVFgYkccqxP1hyYw129dhu1tbUFzulCRUVFv32l9hUzfYqRGy3IBHjFFVd41wbVBRYDVVdX54MS9D8MehAjH5oUtCPTTm7hwoW+39TmkEHQLl/92NjYWLCbU/+3t7dz0kknbXrji+Dss88GXOUWBQWJ4VTwSnt7e16SdHBjXW4S6VQvlZWVvm90rerqas8OTJw4EcixgOXl5X4cqc3Nzc0Flal0rblz53oz0XAwqd/61rcA95z17PTMtctfvXp1QbGBysrKAjN8aF3ReWHatjQzqfYUC8xqaGjwrLIYZ423yspK/7yGGmElH/WDGBwxe08++aQv7iB0dXXlJeOH/HR06g8xQ3V1db4/NLYefvhhwPWHmFFdc8stt/TnS27JevPcc88Nm4vMpqBYUJSgdoVjR+cVOz/LATFpNDU1+bGjORVarNLFG6qqqgr6I6zKlw7S1W9ATs6WStUl3fecOXP8PJHc0LGWlhbPpIpt3Xfffbn99tuBnNuN1ommpiY/R7/4xS8OdxMGDbWlqqqqQEY8++yzXHPNNUCOFZe8WbVqlU/Bpms0NDR4dylV8pPFdt999/WWwrPOOgtw80fyczBBltmTLhEREREREREREW94DGsKqqqqKr+DC9kdBSqIuQgTV6fZRGNM3g4PCtPOQK4E3C233OL9CdOBMVmCWLGqqqoCJkI7987OTt8fanMx1kK7wa6urn6DiEYLYlX1P/TZ0S5caSuampr8Lk67upAZFfNzzjnnAPk7fTGGYr8aGxt9AJmY2nXr1vlrhLWIdc5wldLVczvooIM46KCDgNwzkm/qkiVLvC+xLA/r16/3/aZnqzERJn0XMzh+/Hhf/lKM369//WsALrnkEs+u6nuVlZUF5Ym1S16+fHlB+pahhNK+zJ8/3/sp65mEzvoa32F5Pd1XmHoL8oM+NG5Cxll9GKbbSvvdrl+/3l9Xvo3hM5J/pp7jUOHEE0/0/xXI8a9//QvI+X5VVFR4FlNtNsYU+K4LNTU13t8stK6o7+VjquDGV155hQsvvDDvnJUrV3qmSSVWNcdWr17t+0PBVVmAnqnGgmRq+Ly1JoUWKJ0f9tVopPAbLC6//PK8oFyAT37yk35N1jwIY0XUvrS/arHP1q5d69OdiTErhfLj4FJPQX5pcUHWo0WLFnl2UPrDypUrOfTQQ4HcWAnHjhjGBx98EIDjjz9+WO5/UyBZUVtb6610oS6hlHRDCcnTrq4uv2YNRicbFiVVClhoRpFzsTHGB8Jsu+22QM6k1dHR4RcfKRIrV6705l8tpmH9cS0i73//+wGnpKZNgFmEBEhtba0f+BpIYUCZHmpYNSpdYUn/y8vLvcBQ/2Ud2lzo/1BDyuxoIxT2mhMa67NmzfL/lbsuhISKxkCYd3YggR1vf/vbAafASgGVEtbb25tXzQjyXWek+A8HTj75ZMBtXKV4afwqd2xtba2f4xJ6EydO9Pk6pcCrPQ0NDV6hCquoaBOkqHSd//jjj/tAJAUWTZs2zcsYmbe1KIUV1IYTqi6m/yE0HqR8NjU1eSWkGGTml9K5ISj7hORMGPGswDuNu+nTp2cycCqNUH6Gm50NnQ+5hTVNoGQRW265ZUGu556eHt+edLaDcK4L5eXlBS4OoUvUW9/61uG5+WHGk08+Cbh5ECqgkBvrc+bM8QqpzP4vvviiX5+kzOr7TU1NXrf5f//v/wHZVFKlU1hr/bOUXgW5NUXjIk1c9IX0JrC3t9fPkzDrR9q9aKPufaO/ERERERERERERETHMGNY8qQ0NDT6ISvWup02b5k2vadN0aLoNA13SDFLIQkrTP/zww/13w/Q1WUdZWZlvf7rSTzHzS3d3d0F1iDAdjFIulcKu/42ETQku2dRUajLRDldQ2GChsXrCCScUHJPpe6jwiU98YkivN5pI53Ecaih9VSmjLwtDWVlZQU5UY0y/+VRLCT09PQUBXkuWLClw85E8ClnTkG0W0kzarFmzCs4L3dKyCK2vWicrKio8+ykXFVlmli5dWlDNTuZ/yDGputZWW23lx5pY2fnz52cmd3AaZWVlXr8IrWR6lsXyThebC+lAQ30vdMGU5aaqqqro8QHf80Z/IyIiIiIiIiIiImKYMSxMaphMWFr4XnvtBbja16puIj8PObIbYzzLGrKm6RRU8ilqa2vz/llKFD958mS/K84ykxoGl6WrSmnX0dPTU8DAdXV1FfgJybekra3N91tYLEAotlOOiIiI2Nwgn0HJurDgR3odqaioKKhVPxjGJwsoJttnzZqVt8ZCjiUMWdfQcqc1KB3oUlZWVvAbWU/N9ZGPfATIJeDv6OjwrKdSSokNbWlp8angpFs0NjZ6/1QFNCr5fwhZNj772c/ypz/9aTiaMmjomYWFCULdIpwLfX13IAjHkOZcZ2enjyeQH/3GIDKpERERERERERERmcOwMKnahYZpUV5++WUArrzySh9hq4heMZ4dHR0+M4C092233dZr5+HOBpym/pa3vCXvtzs7O/2uMazlnDXMmTMHcBHF6VKY2pmGTLSY166uLu8Pk66tvmrVKu97NNBI3oiIiIjNAVp3Kisrec973gPAjTfeCOT8BcvLy4smqpffolKhhVkVirFLWUXoPyhWeM2aNZ4pU0YRWdrq6uoKIv9DtjTNkra3t/t1Wz6NWfffVZYP+Zbus88+3H333QAFUf7d3d3ccMMNQC66v7u7m89+9rMA/pjSz7W0tHDMMccAcP755wO5lH9ZgjJwhJktlPUDho4ND9lZpROcOXOmH09hRoGBwmzCAOvzi6LVv/vd7/o8lYcddhjgcjUOJy688ELfUXIx6CMlxFDbvAfdkTIvKNWO0jS0tbV55VQCp6enx7s2SFlXAMqkSZO8kB0EMtMfGUHsj3wMh49I7JN8xP7Ix0b1RzF3Jq1F9913H+Dy3T766KNALgXiAQcc4BVWBeyJCOju7t4UJXXE+yN0ZxDOP/98n8s2rDQHTqmQciqFrbu7u6ibBLhAoSuuuCLv+sWCtfpAJubL/PnzC6ryXX755YDbnKSDnv77v//buwwor/cpp5zijyuft5S+jVD4MtEfkBlXwKI/Hs39ERERERERERERmcOmMKkRERERERERERERw4LIpEZERERERERERGQOUUmNiIiIiIiIiIjIHKKSGhERERERERERkTlEJTUiIiIiIiIiIiJziEpqRERERERERERE5hCV1IiIiIiIiIiIiMwhKqkRERERERERERGZw4goqcaYecaYI/o4drAx5qWRuI+IiFKHMeZMY8x9/Ry/zRjzwZG8p4jsII6PiIj+kZ4jxhhrjIl1xDOKfpVUY0xL8NdrjGkP3p8+FDdgrb3XWrvjBu6jqJJrjDnNGHONMWabZKCNepHlkeizzRnJs1afrTHG3GqMmTHa9zXSMMYcZIx5wBiz1hiz2hhzvzHmTRv6nrX2WGvtb/q5br9KTFYQjINmY0xT0hdnGWOi9Yc4PorBGPM+Y8yjiexYkijkB23iNe8yxnxkqO5xOPFGnDOp9WKZMebXxpi60b6vUkEprLf9Dl5rbZ3+gAXACcFnvxvumxuA0vl24K/DfR8bg4H2WUYU6lG/hz5wQtJ/WwDLgJ+M8v2MKIwx9cAtuHZPALYELgTWb+J1s/q8+8IJ1tpxwEzgu8B5wOXFTjTGDLhgdqkjjo9CGGM+B/wQ+DYwFdga+Blw4ije1mjgjThntF7sDewLnD/K99MvMjjPMr3eDtkOyxgzyRhzS7KDW22MuTe1g9vTGPN0svP/vTGmJvneocaYRcF15hljzjPGPA20GmOuxQmcmxNt/wvJeWXAkcDtwD3J15uScw40xpQZY843xsw3xiw3xlxljGlIvivm9WPGmMXJrvt/hqov+uifQ40xi5K2LQWuNMZUG2N+mNzD4uR1dXJ+AaNhArOEMeY4Y8zzya759fD+jTHHG2OeDHbTuwfH0v2btQnjYa3tAG4AdgEwxrzdGPOEMWadMWahMebr4fnGmA8kz3uVMeYC04+bScaxA4C19lprbY+1tt1a+3dr7dM6wRjzvWTn+5ox5tjgc8/8JGPofmPMD4wxq4DfAz8HDkzmSdPINmtwsNautdbeBJwCfNAYMydhTC41xvzVGNMKHGaMmW6M+aMxZkXSL5/WNYwx+xnHsq1LGJdLks9rjDFXJ2OmyRjziDFm6ig1daCI4yNAIte/AXzSWnujtbbVWttlrb3ZWvv5DcjZ8catWyuS/rrFGLNVcuwi4GDgp0l//HT0WrlxeCPOGWvt68BtwByTsqyaATLixpgG43SFFclacr5xukR10tY5wbmTjWMhpyTvS3rdzep6O5RmgHOBRcBk3E72y4ANjp8MHAPMAnYHzuznWqfhWNJGa+1p5DOSFyfn7AfMtdauBN6afNaYnPNgcv0zgcOAbYE6IC1kDgNmA0cB542AQjMNx3zMBD4GfAU4ANgT2APXpoHuAi8HPp7smucAdwAYY/YCrgA+DkwELgNuklBOEPZv96Y1afhgjKnFCdmHko9agQ8Ajbj7/4Qx5qTk3F1wzMnpuB1hA45hKkX8B+gxxvzGGHOsMWZ86vj+wEvAJOBi4HJjjOnjWvsDc3Fz8gzgLODBZJ40DsvdDxOstQ/jZMzByUfvAy4CxgEPADcDT+Ge++HAZ40xRyfn/gj4kbW2HtgOuD75/IO4sTIDN1/OAtqHvTGbhjg+8nEgUAP8qY/j/cnZMuBKnEzeGvfsfwpgrf0KcC/wqaQ/PjVM9z9seCPNGePM1McBazbhMj/BtW1b4BDcevMha+164Ebc2imcDNxtrV2+Oay7WV1vh1JJ7cLd7MxkF3uvtTZUUn9srV1srV2Nmxh79nOtH1trF1pr+xv4GzL1nw5cYq2da61tAb4EnJrawVyY7LqfwQmq04pdaAjRC3zNWrs+advpwDestcuttStwJrv3D/BaXcAuxph6a+0aa+3jyecfAy6z1v47YVl+gzMDHhB8dyD9O5r4c8LirMWx5f8HYK29y1r7jLW2N2GNrsUJEoD3ADdba++z1nYCXyV/k1QysNauAw7C3f8vgRXGmJsCtmK+tfaX1toe4De4edcXk7HYWvsTa213hp/3xmAxbqMH8Bdr7f3W2l5gN2CytfYb1tpOa+1cXN+dmpzbBWxvjJlkrW2x1j4UfD4R2D6ZL48l/Z9ZxPFRgInAyn4W/j7lrLV2lbX2j9baNmttM06BO6SP65QqNvc5o/XiPuBunMvHRsM494dTgS9Za5uttfOA75Nbk68h1zfgFP5rktelvO5mer0dlJJqjNnaBAFCycf/B7wC/N0YM9cY88XU15YGr9twzGZfWDiA2ziO/pXU6cD84P18oIJ8Yb0wdXz6AH53U7AiodSFYvc40Ht4N64P5htj7jbGHJh8PhM4NzE5NCWDb0bqugPp39HESQmLUwN8CrjbGDPNGLO/MebOxBSzFreDn5R8ZzpBu6y1bcCqEb7vIYO19gVr7ZnW2q1wTPl0nM8dBHMpaSf0PZ+y/qw3FlsCq5PXYdtmAtNT4/7L5Ob7f+HM5C8m5snjk89/C/wNuM44U/DFxpjKYW/FJiKOjzysAib1Y0LtU84aY2qNMZclZst1ONexRrP5+GvC5j9nTrLWNlprZ1prz2bwrO4koJLCsSKG8E6gNlmHtsERbWLvS3ndzfR6Oygl1Vq7wOYHCJHsPM611m4LvAP4nDHm8EHeV1ojz3tvjJmGYwce7+N8cLvHmcH7rYFunGOwMCN1fPFgbnYjkL7PYveoe2gFanUgaXPuQtY+Yq09EZgC/JmcKWYhcFEyafVXa629tp/7yCSSHemNQA+OOboGuAmYYa1twPnPyYy5BNhK3zXGjMHt9kse1toXgV/jlJGN/voG3pcMjIte3xLHmEB+WxYCr6XG/Thr7XEA1tqXrXMdmgL8L3CDMWZsYvW50Fq7C/Bm4HiciatkEMcHD+JYq5P6ON6fnD0X2BHY3zqztlzHJFdKsT883qBzpjX5Xxt8Nq3YiSmsxLHE6bHyOrj1CLfOnpb83ZKw77AZrLtZXW+HMnDqeGPM9onv01pcQ3uH6PLLcD4iwrHA7dZ6d4IVyW+F51wLnGOMmWVcSopvA79PmYQuSHbSuwIfwgUOjCSuBc43zgF7Eo4yvzo59hSwqzFmT+OCzL6uLxljqowxpxtjGqy1XcA6cn39S+CsZBdkjDFjjXOAHjdirRoiJPd/IjAeeAHnR7XaWtthjNkPZ24RbgBOMMa82RhTheuvvvzwMg1jzE7GmHNNLoBjBk4oPtT/NweEZcBWSR+VBIwx9QmLcx1wtXXuOWk8DDQbF5wwxhhTblywyJuSa5xhjJmcmDmbku/0GmMOM8bsljBn63CL1FDJrWFBHB/5sNauxcnO/2eMOSmR6ZXG+eteTP9ydhyOeWsyxkwAvpa6fHrtKQm8kedM4tLxOnBG0qYP43xqN/Q9KaEXGWPGGWNmAp8jN1bAKW6n4FxIrgk+L/l1N6vr7VD6pM4G/gm04Ha2P7PW3jlE1/4OTsg0GRfFnuePmlDNFwH3J+ccgHNi/i3OfPMa0AH8d+q6d+NcFP4FfM9a+/chut+B4lvAo8DTwDM4ZvhbANba/+AiVv8JvExuJyy8H5hnnInqLNykwVr7KPBRnPP/Glz7zhzmdgw1bjbOjWQd7rl+0Fr7HHA28A1jTDNuoRF7THL8v3FCeQluHC5nE9PyjBKacQEt/zYuCvch4Fkc67OpuAN4DlhqjFk5BNcbTtycPOuFuOCXS3CbyQIkC8zxOBPcazhW5Fc4h35wQZvPJePqR8CpiW/YNJzAXYcTzHfj5EaWEcdHCtba7+MUivNxpMVCnOnyz/QjZ3EuEmNw4+UhXLaYED8C3mNc5P+Ph7URQ4M4Zxw+CnweZ4LeFRckNhD8N46JnYtbc6/B6RIAWGv/nRyfjsskoM9Led3N9HprrM00A10A4/yOlgLbDtZZ2zh/kteASpvBKLuITUfCnjcBs621r43y7URERERERGyWGM71thQrUUwALhjlaMKIDMIYc0Ji6hsLfA/Hmswb3buKiIiIiIjYvDBS623JKanWpRG5dLTvIyKTOBEXELEY535yqi01U0FERERERET2MSLrbcmZ+yMiIiIiIiIiIjZ/lByTGhERERERERERsfkjKqkRERERERERERGZQ18VOgaCTfYT+OtfXRap4447rt/z1q5dC8A///lPAN797ncX3kzitmD6LFFdgKHO6bXJ/XHffS7L1LPPPgtAdXU15eWu8MkOO+wAQFtbG2vWuNLEBx10EIB/P23aNBobGwf78yPeH9bagufV2dnJ/Pmu4Edvr0u9t3q1K5aybt06urq68s7v7e2losINY11r7NixAMyaNYvKSlcIZdq0wlzO3d0usYO+n0LmxscoYzhy4G1yn/zgBz8AoLnZ5dS+5JJLOOAAV4nwXe96FwCvvvoqVVUu7afmyqRJrnDK2WefzZQpUwb785kZI33Jv9WrV/Ovf/0LgK22crm329ravJzYZ599Cq6zETI0jUz0R09Pj5ebaaxatYrf/e53AOy8884AvPjii7z++usAfPe73x3MT/aFTPRHW1sbc+fOBfDt7OnpAaC8vJzaWpfz/t///jcAb3/727nzTpc9cqeddgKgrMzxWQcccAA1NTWDvf9M9EcxXHuty7n/1FNPUVfnirPp/6pVq7wOctFFFwEwbtyQpD/NbH+MEor2x6b4pG7UF1999VUAvv/97/PYY48B8NprLlOBFozy8nL22GMPIKegvPDCC6xc6dL16V5nz54NOCHzne98B4CGhgb/PU2oDSBzA+RjH/sYgF9Udt55Z99vc+a4YjLjxo3zStUHPuCKfHR2dgJQU1PDm9/85sH+/Ij1R7EF9fbbXXrCBQsWsGDBAgCvrLa0uMq7vb29fvGR8tnV1eWvo8/0/MeNG8fee+8N5MbMtttuyzbbbFP0flL3NKrjo7XVFU259dZb/QJz//33A7DXXnsBbnzMmzcPwCvvb3rTm1i82BXTUZ9OnjwZgL333pupU13Fw7e//e0AA50rkDEl9dFHHwXg4IMPBuB973N5pqurq7n0UhdXee+99/pzJFeOPPJIAH71q18B8IlPfIJvf3tQpb5hlMaIZONAnt3ZZ5/N008/DcCECa58+8SJE+nocNWZtThv6PdKQab21y9SwM444wwvJw499FAAlixZ4ufW5z//+bz/eTdTYkTIN7/5TQCWL1/OqlWuYqU2J0uWLAGcDHnyyScB/P/f/e53/OQnP8k7X0rrJz/5Sf7+d5dO/IILLgByc3AAyMSau2jRIj8npKx/61subW5XVxe77bYbAFdddRXg2qw1t73dVVzV2Nl+++3ZZZddgBw5shHIRH9kCKOjpD744IMAfPjDHwZg3rx5fidWX18P5JisCRMmMHGiq6wlIdrY2OiVMC3WErYNDQ0cdthhgBtI4AbKAIV45gbIxz/+cQDfP2PHjvXCUzva/fbbzwuTPffcE8ArpmVlZey4446D/flh749iQl6LpJTxhQsXegEyZswYICdQGxsbvbLxyCOPADkhAznGdYsttvDf13XFOh933HH+9axZs/q8L0ZpfKit3/ve9wAYP348M2e6Kn1NTU1AjgHu7OzkiSeeABzLDPkLhhRXKabh9SVszznnnKIscxFkSkl9/vnnATj8cFd5WbLk9NNP92Ni+fLlgGNZ1S9XXnll3vcvv/xy3vve9w72NjIjQ1588UUAbrvN5ReXUtbV1eUtVpKjvb29Xvk45phjAHwfHH744X7DPwhkpj9+/vOfA3D99S7/uBTT3t5eHn74YSCnVFhr/UZOCscLL7wAwDvf+U6+/OUvA3g2fiMwKv2h8f+Rj3wEcOulLA163nfccQcAW2+9tZelUmQvvvhibrjhBiC37mhcHXHEEfzpT3/y1wW4+uqwIFO/GJX+eOYZV2xLltj169f78a/18rnnngMcQSRiY/z48YDb1IkwkZwRy7p48WKvb2i9Ouuss/zrDWDE+iPUiUIWPQ0xxm9605sAx8KLRFSfzZgxgx//2NW1UB8NEYr2R/RJjYiIiIiIiIiIyBw2xSe1AGlGqqWlxfvAiPFZsWKFf63d/2mnnQa4HYu+KzP+kUce6Xc7YlenT5/ubr6iwu+UP/QhV/nt+uuv3xgTZiYgX1Tt5uVT9+STT3oWLGyTdnH6rK2tDSjud5klpMfHwoULvSuH2PS99trL71pPPvlkAH9OTU0Nn/70pwE8u1heXu7Z9/XrXUU2sSaVlZV+F/jUU08Bro/FFIlJ1f1soj/ekODWW28Fcu4JY8eO9e3X/Yo17erq4tRTTwVyO/y5c+eydOlSAO9rtvXWWwOwbNkyP7bUxzfddJN3MykliBlIW4IuueQSdt99dyDng9nV1eVN+rJSiEUQw1RKEAsuk/Tzzz/vx7TY89DCsN9++wHw8ssvA84lQkyJZKquNXnyZC9f5WL0mc98xs+xLOOVV14B4LzzzvNzROxnyIKqr+Sf3NLS4uebsOWWWwLOxebEE08Ecn30tre9bbiaMCTQmNbcWLdunV+H1XZZZSZNmuQZVK0/zz77rJfHGh9inZctW+YtOZqDWUZzc7O3JEh+lpeXe6ZTblX77rsv4Hy0ZYXQ+rpq1Srvt64+0joRMqayUv3iF7/gM5/5zPA1aiNQzFJeTD+SZWnXXXcF4OijjwacBVJ9JEvlb3/7W2/hlXVb2AjXoAGjtLS5iIiIiIiIiIiINwSGlUl97bXXeOCBBwC45557AOf79I53vAPIBW+I1ejo6PBMxxlnnAE4ti29e5Fmf/nll3v/Q+0EVq5c6dmzQTi6jwrUR9qpKLq/q6vLs2Fh2/WZdrkKnikvL/cMQJag55DeYS1btszvxLTLra+v94zoJZdcAjgfGHC7VjGparO11l9XrPqnPvUpALbbbjt/LTGvLS0tnmksdp+jPVbEpMrXesqUKX68i/0QK1RZWekZYs2hyZMne+ZUPmP6XmNjox8zaufTTz+9oSwHmUaazZkyZQr/+c9/gFxwVWVlpfedUj+p/WKpSwmyPElWzpkzx88xRR1rjN92221+bm277baAY8XELmluvec97wEcSys2Vpauj370o9x4443D26ghgPwsV61a5S1QYhUlB6ZPn+5ZwZBd3H777YHcXNFcaGxs9NcQe5R1JlUMsea/tdYzftXV1UCOjW9sbPTrq9rZ2dnpz5NPv8ba+vXrPUsvWbJu3Tpvycka5s2b59lj/e/p6fH3Lnmg8dHc3OzZRMmWiooKz8xLFof+nJKfYltXrlzpr6d+HC1IzoXWwmJ6kXz7ZX1S0G0xXHbZZWy33XYAnH/++UAu8Gw4rNiRSY2IiIiIiIiIiMgchpQ6SbNQDQ0NvOUtbwFyfpN77LGH1+SXLVsG5KLG5s2b53e58oGqr6/315XvjI6dcMIJ/OMf/wByEe6rV6/2TGqpQJGFab+vjo4Oz4xoB7d27dqCna/6M4ssKuT849JM3eLFi/0YUM5CY4x//da3vhXIRRp+61vf4utf/zrg/M4ArrnmGs8K/PSnPwVy/lOtra3+mDBt2jTv06vME2JRJk+ePKrs+4oVK3x6LLEhZWVlPkeuWB7d49ixYwuikcMxELJB4BgSMQLC+PHjvS+VWLVSgFgOjS2xfyFrJMa5GOuRlkGlgrlz5/p26XlVV1d7+ar+EHt60UUXeV9NHevs7OTAAw/Mu27I/MhSIzk6f/58HyGt9DxZhNjj6upqn8lAskBWqra2Nu+zrHE/depUzwRqjoXphPQ6LUuyiiuuuALIMZ1dXV0+jZ/S+2n+vPrqq34d0f/Fixf785Qm8qijjvLHtCapT6+55hrOOuus4W3UINHa2lrgf2qMKcjUEDKN6it9Vl1d7eWLPhNj2NPT4+eVPuvo6PAsvawXo40woj99vz/5yU/83DniiCPyvhf6mIYWN2UW+uEPfwjkmNThwLAqqc8//7wPklq0aBHglCwtrDInyVG/qqrKU+YyLy1dupSTTjoJgD/84Q+AcwEAl4ZIZjxR+Zdddhnf//73i95PVqFFVIuClKiuri4vEBT0sHr1ap+GSSZvKa0SzFlC6MIhhDn79LxDhVvKlSaH0qGsXr3aK6nC008/7c37av9Xv/pVwLkHaPFROqJly5b54hEy2/3mN78BXKCWJqKCr0YSmiOQUxoWLlzo70VjXf87Ozvz0oIIEsAStqFAlXuFrjFjxgyvqJWSkipFSmNFSmdPT0/BghK6hEhA63xtBkoFCxcuLHB7Kisr8/2hdqbT9oWYNm2aH19pxauiosJ/N5QnpaCkrlixAnBzN72JkWxdvXq13wRLsZ84caLvN/Wj5lVbW5u/htadrEPriVIpzZkzh5tuugmAm2++Gcilorryyiu9290tt9wCOFkpuXnIIYcAOVeK448/3pMpSoWY5aC6devWFbibVVRU+I2HxniomGoN0DocBlppvoTjS2uX5G59fb0PTM2KkhrKwjRZJL0K4P3vf3/esa6uLt+uUJ/6xCc+AeT0NBVVOeecc/oletRvaTeE/hDN/REREREREREREZnDsERKaCdy3nnneVOqTCvf+c53PPMn06RSTK1cudKzbEqLE5pdZKJSyqbf/OY33lR+/PHHA7lUQ6UE7cTktCxmeeXKlZ7lUoqMn/3sZ75PFByg4LEsoqamxu8+r7nmGiCXZPpXv/qVD/IJy52KOVTCYLX9xhtv9Lt9BVC9//3v9+UvxfZ85StfAdzOWayCdsyPP/44J5xwApALsAoZxNFgUIUXX3zR7/ZlLqqvr/dsmJ63GLSxY8d6k5sc9Nva2vxxjZ0wXUravD1r1iy/61c/lwLSadjCZNV65mEaps0FL7/8sh+jYXqgNCMRBtfJxUPMYVjwJF1S2Frr50rIssoMnmUoEXl5eXkBY6NxMn78eM8iK1H9dttt5+VPOgAoDMzTvMo60tYmyKXnkhlfMq+2ttZbIcWITp8+3Vt1lL5MTOzb3/52v6aXAtrb2wtkhLW2gBGVtc8Y4+eJGMEwoFYyJUw9JVO51rKqqqrMWWhCC4tkg8bCvffe69dkVbEUwsCv0G1KbgHqNwVinnPOOb6vQveATXGji0xqRERERERERERE5jAsTKp257/61a8K6oiHO5t0Cc/Gxkbv9yMmaYcddvC7l5deegnIJTOfP3++Tzj90Y9+FHC7glJKqdPV1eV38fJf0Y52/vz5nm2Wb9AvfvGLAnZVu5S072dWIJ9itVPBbgceeKAvOaiAobFjx/pdqhgBBQXdcsstfPGLXwRyTvtTpkzh9NNPBwqDYIwxBal5Fi5c6JnD//u//wPgl7/8JeBSFv33f//3UDR5UFDqJMixwgcccIC/d7FZ2tH29vb615pnlZWVnmnXrlU752nTpnn2TT7Ou+yyS1G/xaxDPmRixTT2e3p6iu7W00mtN8YnKktYunSpZ5HF1nR3d3s5kU5ZV15e7p+/LDUVFRW+3yQrdX5nZ6efp2IO6+rqfH9nGUonNmbMGG+501wIfU5luVJsREVFhWfP0r7OTU1NfmzpWClCMlf9oT4wxvj1VeNk7dq1Xk6ItZeP7iOPPOKZ1FJI8dje3u7brLHe1tbm03OJKQyLYOg5y6IQ6hGSxRpr1dXVfi7pd1paWnz/jTbSJeLDgLHQ6qw1cGOh1I5h+jfpL2FBiGK/P1AMixYXBiVI2RQlftNNN/kcqHJmlzCsqanxUWNaOJubm/2gkSAR1fyLX/yCL3zhC0DOifumm27yVXVKIcq/qampIHpOQmPlypXeqV0Pfu3atd4dIFxYIGcCzRLa29u9AqX7++xnPwu4fLDalGgBXb58uVdA9T3hG9/4hhcq55xzjv9c7h967jJllZWV+b4N8/ylTZe/+MUvADehR1NJXbNmjW+flILQxCiFVP/DOsyac2PGjPELkIKjdK2qqiovOCSgTj31VC9kSwkKhEub9K21/rNiFXHSymqpuQSsXr3aC3ottM3NzQVKpJSLMGhM54SmSl1Li2pra6t/HQZjZWXR7Q9aHxoaGrw5V2Nb8qW1tdXLAsmZsWPH+n6QLFUfrV271o8ZKWpZRzHlUZ8pUFXHmpqaCuqvF5M56h8p/5CbX8VqwGcFnZ2dfvxrTPT09PjnrDbofU9Pjx8fxUguXUNzqaamxm8WNWZqamoKsqiMForlLf3b3/4G5NziampqfJCd9C65zmy55ZYF5vu1a9f6NVbzS24yxx9/PBdddBGQC0wuRp5tzAYnmvsjIiIiIiIiIiIyh2G1h2+11VbeHB+mt/jTn/4E5DTtX/3qV4Db1SmVgczAEyZM8PkylSpIpuG2tjbPKooR2XbbbX3wVSkwqatWrfLsmXYXet/T08O0adPyzl+4cKE/rh2wdoPa6WQJ1lpvhteOUyaTp59+2u/CxbTPmjXLvxaLvM8++wBw7bXX8uc//xnA5+UbM2aMrzaWrqYEheaFYubgU045BciZxEYLYTUxsaHl5eV+HOuzMHBK7dOO9oknnihIsaPddHV1dUHbFy1aVHK5QiHX3v6Y0DDwoVgNa6DP6mNZxapVq/z8EXOzfv16/1w1B0KmQq81HiorKz1zpPkn5qenp8f3rZhDa61nVrKMcP5LFqbz4o4dO9bLIcnR6upqz5pJlqoPOjo6fD9n2azdH7q6unxwlPpBzFd3d3eBu1hLS4u3eoltlkyRqxRkm0EVwry/YkHLysry3GEg92zDIESNgTC3qPoqdP3QefqstbV11JlUuY5deumlAFx33XVAcXk3ZswYnypU0FiQzIDcfJk9e7afJ9JBNEdef/11X73qoIMOAlwAvSw7Rx99NJAfqLmheRWZ1IiIiIiIiIiIiMxhWJjU973vff61Es+rcsXEiRN9dSg53V544YWAqwAkDV6VDNra2nyd2HTi9w996EN873vfA3Ka+RNPPMFtt90GwN133z0MrRtavPLKKwVsmIocFEuufthhh3l/GO1mtGvLYlLl2tpaH9ShXZf8Xv75z3/6Hb18ncLduVhyMaWHHHKID8B75JFHADdmlI5KachU5ay+vr7AH6a6utr/ppL6f/7znwfgL3/5yxC0eOOh5zdu3Di/WxVb2tnZ6XfF2gWrYk5XV5cfMxoLL730kmee5dwv/836+nq/2w/9xsW06j6yWoc7hGRAMSY17YcV+qaq3WLOQh+7UkBLS4uf52IO29vb/dwKE5aDm09iQzQP29ravMwJmRJw81XMepjiKkwRl1WEfrjqD419/Z82bZofM/LphULGVdfad999efrpp4H8dGfDUaN8uLBmzRovV9MVDJubmwvSMXV1dfn+0DxTe0vFf1332dXV5cd46HeqMaD/et69vb0FFRLDBPi6huZPd3d3Hiuo7xXzhx8pXHrppT7AWDJd6+D48eN9NToFJEMuHVuYigvceimWVJaVurq6Akb58ccfB5yl57DDDgNyOt8HP/hBbw1UPNIFF1yQ9zv9oXRmWkRERERERERExBsGQ8qkirlRrfVPfOIT3hdCddX3228/H/Evfxf5O/X29vpSn9oJK/E/wF577QXkosR/+9vfenZVfkannHKKL/lWClizZo3fpaldYs6KlVTbd999PcOh85WUN6spqHRfSnasXdeKFSs8k6Xd2pIlS/wuXuVQH3vsMQDOP/98P2bkpwy5WtXyq1FWgIqKCj+OxMouXrzYR/qmfTyVKmukobH7yCOPeKuBdqq9vb0+ib9273rf2dlZ4BvU1NTkP1M79T1rrffnfvbZZ/1v67u6j1JgUvtKrF4s8XZNTU0BEyhmqNR8Uq21BWx7Q0ODnzNizEJ/OrU1TNGULh0b+uHpfM2P2trakkhkLzna3d3to5X//e9/A/m+umLNNM7DogXqF/Xx9OnTC/z1QhlSCuju7vZzWyx86K8tFEter7V6ID7gWYIYz5D51Xiuq6vz6036PPnuQn62HY2tkKGF/Pmlvlm/fv2oMKmao2effba/X7GfGv9h5otwXUln+RAqKir82FGbmpqafD/Jqqu5tPPOO/tr7LDDDv58jS1F/ocFd9JpstIYUiVVD1X5wsaMGcPXvvY1AN75zncCcPjhh3uKWArm1VdfDbgUQjJJSXGtqKjwD1/fk+DZcsstuf/++4GcCfmSSy7x1LVSLchZN4tYunSpd3vQg9ZkkrN6iLAucLrGvUw5WYNcPjTYJQCXLVvmlVRNmOrqaj+xPvjBDwK553fhhRfyjne8A8i5lDz88MO+frCqosjcP2nSJG+qU26/zs5OP4nUf3KvUPWykYaUh2nTpnmlSeaRiRMn+v5IB4EZY7xSInNme3u7F8DpGtSTJk3KCwjR+eoH3YfGY5YR1pWHfJO+xlexhSIdMDHaAQ4DRRi4Ifmg53vQQQf5PIWSqTrW0dHhF9SwhrnGgWSH+mPJkiU+8OGBBx7w1woVuaxB7VI7e3t7/aZUcybMoyslVbKnvr7ezxn1g9LUveUtb8mr1gOOXMmykpo2oba1tXnlVM9R/7u7uwuCxsrKynx/SfbIdFsquWJDM346vVioUwhhn6VTQYb5qItVaEu7B4QVz0YScnOsrKz0OejluqPx3N3d7V+H7gmSB5ob+l9TU+PXD/VBV1eXb7P6VJu6MWPGsGLFCiA39xoaGvI2vJBbq0866aQYOBUREREREREREVF6GFImVSZEOeSGqV/ErnZ1dflAGJl/xWYsXry4oHb6K6+84lkwadwyg99zzz3+PKWumjFjRtGAo6xizZo1vvJJumawWJEQEydO9CZv7WbEumU1JYjqAl911VVA7n7nz5/vn73+H3jggf57YkHlzlBXV+d3Z5dffjmQb5qSKfvEE08EXBCRak+LZamoqPCmCrGJuubjjz/uiwuM5BiSyWT69On861//AnKs+vTp0wtq0cus09vbW7ALLS8v97vadIWqzs7OgoIAK1as8NaLLLNlafTHgKYDGYoFTqn96cChrEJMhzHGj2XJ1N12282n+Eub7cIUVHLr6Ozs9HJW54XMstyqHn30UaCwAELWoPkTsmOyBqg/9L+srKyg/8JAF7FhITuWbn8pzRNwY1zjXMFismqVlZX5sRAWQ5BlRmNBTFhWrXVppNdSyLF+s2bN8mNG54VV/HSeWL+QOdR/sa29vb15KTHBjTUx0SNZ/VJyvKuri9mzZwP4dJxCW1sbBx98MJBbV8eNG8eiRYvy7lNjvLm52VsZNG8qKyt9u7TW6PzKykqv14UuFZI90g2VhjQyqRERERERERERESWJIVXvVa5U/wHPTAkf//jH/WuVMn3llVcAt+sXy/bwww8DTpPXrkCfqa75008/7XeEH/7wh4HS8ZkRamtrvf+I/H7Cetrp8mETJkzwzuxpJ38xA1nDvvvuC8Cdd94J5HZYY8aMKWCyOjs7C+qy6/0WW2zhzwvZRO3uf/e73wG5sTBhwgTPUotV7Ozs9H2q6+v7bW1tPm2ZUmWMBNQH9fX1niHUs91ll10K5lDoO5ZOERMmo06n1Vm3bp33pdM57e3tBb7QpYA0QxCypmJF+mMAB3JOliB/4YqKCj825Ge4ww47FGWOBI0Nfa9YOqmwdKqCjkKGI8splyQLismHtK9db2+vZ4/FEq5evdqfn07k/swzz/jPxDKVQhBZiMWLF+f560K+T7oQ+vbqPLFiYhAbGhoK1qSBJGQfaYRjXM9evpVbbLGF1yXEsIeBUXqt73V2dvr5lbZS9fT0eGZS8rO6utr3n9aukSipG1ohFTyc1gl22GEHv/4qdgfwaam0ZmhchOuJ+rSxsbGgcI7WkGnTpvk2SzdbtGiRlx+yUMqq+vOf/zyvTHMxDCsH3dPTk+dsC840pSCq66+/HshVnArr4Cr4pbKy0gvodPS1sghAvnK6MXVhRxuh077od1VqgMK2bLfddn6ApGvQZxGhY77ymZ533nmAiz5X20PzggSH8unKjHHdddf5jY3MEytXruQDH/gAkBM42uhMmzYtL4AEnNCV6SFUDsGNV0XyjqSSGgo3KaTql9raWi/wJCBDtw6Ne21wmpubvUBQf4SmKr2Wc/uMGTO8+0i6hneWkVYUiiln4WfqQ/0PZYMUEvVvFqFA0rKyMj+WpaTW19cXVJvTcw4VDsnRmpoaP0bSC1D4mdximpqa/PWz2FdhgAu4OaF5nA5g6ezs9MrKgw8+CDj5IhmlBVaL+4oVKwrcRkolV6iwdOnSgvyoYWaDsBIT5Af+pGVlb2+vlx3F3NGyhq6uLj/uJSvHjBnjn6HM8qE7iIiC8Lmng1b1fu3atX59UgBuGNQpfWYklNQQap9ICRGB1dXVvPrqqwC8/PLLABx88MHeZUFB6brfyZMn+7VFz3vp0qV+nqj/NH+eeuqpAteQp556yt+X1mZd/+c//znnnHNOv23J7vY4IiIiIiIiIiLiDYshZVLTrF9oIgqDW8QKyNwi0124yw0DqPRau1sxSe9+97sLfjOszV4KTGpFRYVnssSQiE074IADCnKIVVZW+lya2rmJ1dh///0z5+4QBh7oPsWkXnrppf5+tVP/z3/+43f5l1xyCZDbDd55553885//BPLzQX75y18G8Dl2v/rVrwJuB6wdXhi4p92t/qs/Ib8Kx0ghzGmXTjfV09Pj+0jjI6z+onaFKUAE9am+393dXcC4hVWr0mxBltGXFSGUOSETkmZaQ9kgpkWMSBYRps8RGyZXlvB4usZ4WVmZf+bKOb1+/Xrffo2RdLAZ5Myjy5cv99cXyxTmrx5taF6EuWLT7JnmVVdXlz8/DI5S+8X+iA2aMmWKT2eldD6lUH0rhIKgICfz5BoRVkwSwjR9GgOhtUnXyzKTGro6hQFh4NaC9GeSn1qLISdL2trafD+k84iGc0lyfN26dX59Sp8/nBBDCjlLWegCA87idtRRRwH4yoQtLS2eLVV/KPCwra3Nrw967qHlQe2TLlJVVeXZd83BefPmebcRzUf194033hiZ1IiIiIiIiIiIiNLDkDKp6R1Z+F479KqqKq+FKxWVGIyysjK/A5EW/uCDD3o/BjkGi4nddttt/a429CkpBQZVqKmp8Ul4tbsI64mn00otX77c982uu+4K5FI1hdUysoxvf/vbgGNS08zF6tWrPUsm52rtepctW8YXvvAFINfWpUuX5jmMQ85pfNasWQUBAosXLy4IzJJv39ixY/2xkYTuo6Kigt122w3IMVZh6hLNIbFaYRCI2LWKigp/XOeH7Kn6W+zxkiVL/C53NOtNbyxCxqMvbMhPVSiFQBiNX2NMQUWXEOkk5b29vf656hqhb7LGhlijsH+mTZsGODkt+aq5mSUmNV0RJ1xjJFdkjamurvZzXExSU1NTAXOoOTRp0iTPKilmIMtBZMXw5JNPenmp5x4GrIbp+SC/IEi66ENNTY2Xr3PmzAGyabFUm0J/7TDAWGNFz1nPv7m52Y/7kGnXeelCIWvWrPFzQ2x9a2trAdM4ElBBI8hZpzWe1aYFCxb44jeSe11dXf75Stf6xz/+Abh5rvVRbOkWW2zhx4d0EfXVuHHjPOMq+bT77rv78Zb20VXa0v5QWrMtIiIiIiIiIiLiDYHhzzCbIEznIA1brI78Jnp7e72mraixpUuX+lQJ6cjEffbZxzMq2imEKWhKAWHdbe36tZuBwl1qTU2Nj1zdfffdgVxd+izuaKHQV1n/161b59Nvqe3t7e3+Of/5z38G8AnuFy9e7BkclWw8/vjjfdJxpQJRpGJlZaX/bY2r1tbWvHJtkB/9PhJJl/tCsWT7YVm/dFqTnp4eP4d031VVVd7vJ82qtbW1eUZAZWhfffVVP35koSgFhMwiFGdI0ym4QoRzJV1iNYtQe8eNG+f9zUIfWh0XgxRGvKsfxM5XVlbmja/w/HDMSL5cffXVBancsgTNBz3nqVOn+jZLVob+qhrnakvIjKZjAJqbm/11xSiFPp6lgJUrV3r/0fRzrqio8GyfmMaWlhbv665j6oOamhrvm5tliAG21nrZr/RHIZuutULys66uzq8/kgtdXV2ekdS4kA7S1NTk55z6WJZNoCAGYDghv9Lwd+WnqjWhpqbGz32xoVVVVb596hcVfaiqqiool93c3OzHRTo7wpgxY/yapDEWxklonddcGshYGrEUVMJHPvIRzj77bCDXgVowTj75ZJ8qQVWlZs+e7alhpTJQx9xwww0cc8wxQE5JLTU0Njb6SaEJo/rtxWCt9ZNHZhdNvqwqqRqsmuAvvfQS4O5XAz+sPa9B/rWvfQ2Ad7zjHQDcddddvo1f/OIXAXjve9/r87B+5zvfAeD8888HnDlK15LQWrVqld/YaCJKOHd1dY3KONLEDc3OesZNTU2+39ICr6qqyi+qaWEUItwc6FloXtbW1hbUYS4FSOEqZnpNK64bUlI132S+zCI0fsNAjbAefbqt6pfQhUMBDcXGWbqqGeRyZJaVlfnrZrHaUjqdVnV1td+Ehf0GznwZ5gMFp6hr3khp1zltbW3sscceQC6oMp23OOuYMGFCQYW1YpsSKWptbW2+yqOet/qnoqLCb5KyDLWrurrat1mb8dAlULJfMrizs9PPqzC3brjJAfJSJ+q3pHhVV1cXuAWMBMLAV7VBKRulhFZUVPi26pyamhr/nNU+jYXu7m7fvmIkhuZQ6C6RTmtnjMkbP+GxgaSyi+b+iIiIiIiIiIiIzGFYmNRizIU07QkTJnitXjT8+973PgAOPfRQv2sNU4LILKPk7tr9t7W1eXNx+NullMx/22239Q7Jcu7XDqcYysrK/K4kTEuTZWhHKlPCkUceCcDOO+/sd3Uyn9TU1Hinb7VLZv8//OEPfrco1vSMM87wDLsqkSkwa8mSJZ49kumyrKzM79523nlnIOfEvWLFiqLBKMMN7cCfe+45f28KoFqzZo3vGzECeu5jxozxO/rQXKPxnx4X5eXlfh7q+k8++aR3uJdVohSQTp2UNtOGKJaWLrTw9Ge5yBqstQVsecjshan4IJ810v/QDSasWa7rC3KtCVN4ZZFJlUVJZsmlS5d6i4jmdpjGMExfB44llGzSMfVVZ2env4auP5CgvSwgXb0Ocs85tMrotdad1tbWArZZcqmqqspbwrKMsJiF5GXoEqj2aFzomdbV1RUU/AjlR3hdcPJWzHLouqZ1Kl1RcTgR/la6AIXue/369f55hwUJ0kyxxg7k1pFQjvSlW1VUVHgZrN/8z3/+4/telk2xsgMpjBGZ1IiIiIiIiIiIiMxhWCi4Ysn8tUs78MAD+dznPgfAqaeeCuR8RSA/jQi4Ml3atWh3oB3+vvvuW7CzzzqrmMaUKVPySoJCbjfT3Nzsdx5CZ2dnwW4kSyUKi+GKK64A4Ctf+QqQc8Cura0tCBaz1npmJO0fevLJJxckJ77hhhsKAoRC316xTBp/U6dO9Y7t2i3qHjo6Orx/60hC/nNtbW3eX1AWhTBdkNoQpjXRDjhMy5ZOGyNUVFR41lD+hh0dHZ5pSNd5zjLStdaFYon7w9K8ki8hk1oKPnZ6zj09PQVFF1auXFnAhIa+x3qtuVBfX1/ggxr2RzoooqysLNP+yiqPLd/1o446iueeew7IjQ/NibAd4VhIl++WTF2xYoUv0X3mmWfm/V7WIZlQXl5eMCfCVHtpdnX9+vUFfuphoJECWbOMMHBK41l6QxgYlg6ubGhoKAjeLisr8/ELuq4sVzvttJNfjyVvKyoqPEM7koUfQiY1tJr0BT3TMNi2WBrRgaRcC63XOl/9Ul5e7oOU1R/q94EwzcOq0YUN1s2VlZX5aGwtzlo4p02b5muyywR68MEHeyXiN7/5DQCf+tSngPxKCTLrWmtLwswvjB071ptzX3/9dSAXdT537lyvrAidnZ155l7ofyCONr7//e9z0003ATmTuxSM9vb2PIdugEWLFvlNS5j3DeDmm2/2CqjQ1dXlFyRBNYnDc5V79qWXXvLj6Pvf/z5AXu7V448/fpAtHTz0PHfYYQf+9re/AblMBd3d3V4pkfDUWF+1apUXNBoDM2fOLJj4EiChWVNCaerUqX6DUEo1yWV6HWgmj7QJL0QpKKlSOKy1edGy4JTKdIBGaJrToqHo49bWVr8AF6vEpU2iglcbGxsLzIdZgnL+KmAScpHOaWVh7dq1BTliw0pskkPqs1WrVvn5dtZZZw1rO4YaYTBQOheqXOjCbCphIF462FUyqK6uLi/7TNbR2dlZEExYV1fH/PnzgfxcoeDmUrEqlpI3aVLstddeKyo30ybvkUBI6oSZCdL3kx7j3d3dfeZzDXPXh+RjMXJASAeudnV15WWkgZzON5Aqf9HcHxERERERERERkTkMK5MasprSxpcvX+4rTaVTAS1btsxr9Nq5/Pvf/+bQQw8F8KbYv/zlL4ALlFGwzO9//3ugNIKl0ghTV0CO5Xr55ZcLmNTKykqfWyyscJFVvO1tb+Pee+8FCnd3VVVVPkhKzxtyTKFM0m9961sBuO222zyL9M53vhNwpjfl0f3ABz4A5ALQQrOo+mzSpEneLCjG9Uc/+hHgXAdGA2Lyenp6/FhQH6xbt84zWmFKFHA7WrGmauu4ceN8P6fT6VRUVPi5pvl44IEHelZBTG0pQC4jMmH3FSwWHoMcexYyq6UQOBWaydLWhJCt0bMPK0iF1f50rXQ/hCms0kz8lClTvIwJAyqygnSQT1VVVQGDFabkSqfrCj8TK1bMLUQolloxiwiflZ69zNxiWbfeemuf/k/m2QkTJuSxsOH3lyxZUhJrrJ5ndXW1t0yGz0ypLtXONLsIhXMphBjV1tbWgrHQ09Pj1/JiQeTDhdmzZ/vX+n3dezEXsRB9uSeEFes2BdL1tFbL5fPzn//8Br8bmdSIiIiIiIiIiIjMYUiZ1P5SP4k9bW9v56STTgJy7KfYnVmzZnn/zHnz5gFw3333cdxxxwE5R1/508ycOXNEfT6GC+nE00LILgrGmIL0Mf2lrBpt7LXXXv7+tLPXM37llVe8n5z8Qz/zmc8UJJxWfd8tttjC7+rEqNbW1vrxo92grt/R0eF3kmKkL7zwQn7wgx8AOUY+zUyPNMScT5gwwVsNtPsPn612++qXGTNmeBZWLMjYsWP9OEpXqqqqqvJzUwz2+PHj/WfpgJwsI73jD5kA9VMxX+20r11NTU0mqyilIV/99evXFzyn5uZmH1CYZknCRPxiysN+SSf97+npKWCOqqurvU/sSKbUGSiKpRXTnCqWYkuyQ8eqqqp8u9Ipe4r5zA0kkCQLkG/x2rVrve+/+kUyr7e317OJaldnZ2dBhaUw+b2YV6VOFDuWJaidLS0t3joVQsG8ssQU81kXGxmuuaGc1fu0vtPQ0ODHk9a3kcD+++8PFGdvH3/8cQD23ntvPy7EJr/lLW/JtGWgNGZbRERERERERETEGwpDyqSmdxShT6rYwksuucQzXfJzWrBgAeAivuQHIuarsbHR+1OIXVU0Z01NjY+IL2WoPbfffjuQ88WUr2CIRYsW+V2a2q4SdlnFGWecAeRSUGmHus0223DnnXfmnfv2t7/dt0vPWwxsmDZFu3/I9ZfYQe0KGxsbfdL6WbNmAa6PtQu+66678n57tHzNTjjhBP9aDOE3vvENwDFXjz32GJDrN7GrtbW1/n7DMVMsYTc4hkTMiHbTY8aM4dJLLx36Rg0zxOyJNZWMqKio8KxjMcifU3Ooq6vLM0JZhpiv5ubmvLEPrsiJItvFEmoM1NfXF6TwC8vpinVXf7S3t/uxJDQ1NXmrzh133AHAhz70oSFs3dAgTEiu8SGWPGRSte6E1j2NHzFlulY6k0IpYZ999gHcuqrnq+ctVt0Y4+Ws2t7b2+vn0D//+U9/DchPV6Q1PYvQ/d97771FUzTKUqX/Q4lnnnnG9698MY866qgh/52Nwd577+1fK3tOmPozyxixpKIyyT7++ONecOjBacFsaWnxi4gW5mXLlnnlTWYqCcynn37aB8mEKKWKUwDHHnsskFPG1FfFTE077rijd3/Ya6+9gJwwyip0v3/84x8B+OhHPwrkKoiFqK2t9Q7goSP4UCGsqqQNkRawLKTy0j1885vfBNxiqeBAbVrCimtp82tVVZUXyjLZyTxcW1vr05RoY6QArVLDBz/4QSAnaNXmt73tbfz4xz8GcoGWU6ZM8WnH3vOe9wDw85//HHDz6cADDxy5Gx8kLrroIsDJSuV7FCZMmMADDzwAwGWXXQbkgvHWr1/vFXkpt5WVld6crQ2b5tp73/teP26EM844g7vvvhugIJAzSwg3mIcddhiQWyvkylNRUeEVB5ElYS5ZKXFSZFWVLkSprCsKLnzhhRf8eJDSqdyvp5xyik/Xpcp9Rx11lF+jb731ViBXoe64444blTR9GwtVf9ppp52K5n/uK6Ap/Dx8zum0SiHS4+HYY4/1G985c+Zs5J1HpBHN/REREREREREREZmDGckUCREREREREREREREDQWRSIyIiIiIiIiIiMoeopEZERERERERERGQOUUmNiIiIiIiIiIjIHKKSGhERERERERERkTlEJTUiIiIiIiIiIiJziEpqRERERERERERE5hCV1IiIiIiIiIiIiMwhKqkRERERERERERGZQ8kpqcaYecaYI0b7PiIiSgFxvkREvLFhjDnTGHNf8N4aY7YfzXsaafQnB40xBxtjXhrpe4oYGDZJSTXGHGSMecAYs9YYs9oYc78x5k1DdXObE5JJ0m6MaTbGNCX9dpYxpuQ2CsMFY8z7jDGPGmNajDFLjDG3GWMO2sRr3mWM+chQ3eOmIM6XQiTPWn+9yRzR+9NH+/6ygig/NozNXX5A3jhoMcYsM8b82hhTN9r3NVwYCflgrb3XWrvjBu6jqJJrjDnNGHONMWabRPmvGIp7GimkxtMaY8ytxpgZo31fIQYt4Iwx9cAtwE+ACcCWwIXA+qG5teHDKA6kE6y144CZwHeB84DLi51ojCkfyRsbbRhjPgf8EPg2MBXYGvgZcOIo3taQIc6X4rDW1ukPWICbI/rsdyNxDwNFBu4hyo8+sLnLjxROSObL3sC+wPmjfD/9YlPmzUDlw3BhAPf+duCvw30fwwyNpy2AZbg1Kjuw1g7qDzc5mvo4diZwH/A9YA3wGnBscLwBJ1yXAK8D3wLKk2PbAXcAq4CVwO+AxuC784Ajktc7J9c+LXl/PPAk0AQ8AOye+t55wNM4xaBisG0fZH/5+w4+2w/oBeYAvwYuxQ34VuAIYDrwR2BF0s5Pp777KLAON7AuST6vAa5O+q8JeASYOpJtHUTfNAAtwHv7OF6NW4AWJ38/BKqTY+Nxyt+KZKzdAmyVHLsI6AE6kuv/dBTbGOfLRswR4FBgUXIPS4HfbmAcnAncl7qeBbZPXh8HPA80J334P8F5meqHDfVN8FmUH/aNIT/6GgfA/yX3bMOxCdwFfKTY3EjNiwbgqqT983EKb1nSZ03AnOB7k4F2YMpozJticyB1fFLSF03AauBeoCz47v8k97MW+D1Qkxw7FFjUz71fi5tn7ck4+EJyXlkydybhFGibHG8BDkyOn5/06/KknxuS726TnP+xZEwuIZBJoziejgP+k7x+O/AETkYsBL6e+u4HkratAi7Y0PMZ9D1uQuPqk5v7DXAsMD44dibQBXwUKAc+kTwIkxz/E3AZMBaYAjwMfDw5tj1wZDJJJgP3AD9MdypuF7kAOD75fK9kIOyf/OYHk3Org+89CcwAxoz2YAg+X5D0z6+TyfOWZHDXAo8BXwWqgG2BucDRyfceBN6fvK4DDkhefxy4Ofl+ObAPUD/S7d3IvjkG6KYPQQZ8A3goGSuTcQLxm8mxicC7k/aOA/4A/Dn47l0kwnqU2xjny0bMEdzC0Q38b9K2MRsYB2fSv5K6BDg4eT0e2Dur/bChvkl9HuXHG0B+9DFHZgDP4TZwg1VSrwL+krR9G+A/wH8lx64ALgq+90ng9uT1iM+bvuZAcPw7wM+ByuTvYHIydB5Obk7HWbJeAM5Kjh1KoZKad+/Ffhs4AHgweb1NkWfwYeAV3NyrA24Efps6/1qcXN8Nt1EYciVvI8ZTLW59uirol91w8mR3nEJ+UnJsF5wyfhBOvnwPt4ZlR0lNbnRnnHBchBMSN+FMLWcCrwTn1SYPZFpyfH04cIHTgDv7+I2TgCdSnXph8puHBp9fSiJ4gs9eAg4JvvfhkRwAfQ2G1OcPAV9J+vGq4PP9gQWpc78EXJm8vifph0mpcz5Maleb9T/gdGBpP8dfBY4L3h8NzOvj3D2BNcH7u8jIIhPny8DnCE5AdpKwHRsaB2xYSV2AU8DqU+dkrh821Depz6P8eIPIj2ActODYwvk4l4adGYSSilMuO4FdgmMfB+5KXh8BvBocux/4QPJ6xOdNX3MgOP4NnMK9fR/fPSN4fzHw8+T1oRQqqR/e0G8D3wQuSF5vU+QZ/As4O3i/I06RqwjO3yl1T5eP4njqwpEju/Vx7g+BHySvvwpcGxyrTcbSkCupm+R0b619wVp7prV2K5zJaXrSEHAmOp3Xlrysw/lTVQJLkgCAJhxLNAXAGDPVGHOdMeZ1Y8w6nOlpUuqnzwIesNbeFXw2EzhX10yuOyO5J2HhprR3mLAlzjQB+fc3E5ieas+XcUoLwH8BOwAvGmMeMcYcn3z+W+BvwHXGmMXGmIuNMZXD3opNwypgUj/+P9NxAlmYn3yGMabWGHOZMWZ+Ml7uARqz6JMX58tGY4W1tiN43+c4GADejTNlzTfG3G2MOTD5vBT6oT9E+fEGkR8BTrLWNlprZ1prz8aZoQeDSTjZku6bLZPXdwK1xpj9jTHb4BT4PyXHRnXeGGO2DoOqko//D8dc/t0YM9cY88XU15YGr9tw8rUvDOTej6N/f9Ri466C3BxM/87GyLOhxEnW2kacq8+ngLuNMdOS536nMWaFMWYtbh3R2jKd4N6TNWvVcNzckEWGWmtfxO3m52zg1IU4ZmhSMtEarbX11tpdk+Pfxu0wdrPW1gNnACZ1jbOArY0xP0hd96Lgmo3W2lpr7bXhbQ6udcMD4yK7t8T5I0L+/S0EXku1Z5y19jgAa+3L1trTcMrK/wI3GGPGWmu7rLUXWmt3Ad6M8xv6wIg1anB4EDcmTurj+GKcUBS2Tj4DOBe3Q90/GS9vTT7XmMnUMxfifBkQ0r/f3zhoxe3mATDGTMu7kLWPWGtPxM2XPwPXJ4dKoR+KIsoPjzec/EihNflfG3w2rdiJKazEsWfpvnkdwFrbg5snpyV/t1hrm5PzRnXeWGsX2PygKqy1zdbac6212wLvAD5njDl8sD/R3/tEvmwBPN7H+VB83HXjzObCjNTxxYwSrLU91tobcX7YBwHX4Kx9M6y1DThXCs2LJcBW+q4xZgzOdWbIsSnR/TsZY841xmyVvJ+BG8gP9fc9a+0S4O/A940x9caYMmPMdsaYQ5JTxuHo57XGmC2Bzxe5TDPOD+mtxpjvJp/9Ejgr0f6NMWasMebtxphxg23jcCFp9/HAdcDV1tpnipz2MNBsjDnPGDPGGFNujJmTLEwYY84wxky21vbiqHqAXmPMYcaY3RImYB1OCPUOf6sGD2vtWpz54P8ZY05K2I1KY8yxxpiLcX475xtjJhtjJiXnXp18fRyOSWgyxkwAvpa6/DKcT9CoIs6XIUF/4+ApYFdjzJ7GmBrg6/qSMabKGHO6MabBWtuFmxeaEyXXD1F+5OONID/6g7V2BU6xPCN5zh/GBVRu6HtSQi8yxowzxswEPkeub8ApKqfgXCquCT7P3LwxxhxvjNneGGNw/tk9DN3YTY+DY3H+uVJOVyS/FZ5zLXCOMWaWcWnCvg383lrbHZxzQTJedwU+hAvoGhUkz/FEnM/+C7i5sdpa22GM2Q94X3D6DcAJxpg3G2OqcPI2TY4MDQbrJ4DbwV+Pmxytyf/LcAEiZ9K/f1gDzqdlEW4wPQGcmhzbFefw34JzXj6XQn8R+a1NwC1OcoI/BheN2oTT9P8AjEt/bzT+kt9vxykMa3G7/0+Si9L+NfCt1Hem4wb6Ulzk6UNB26/GOa634JznT0o+Pw3nG9SKm1g/ZpQikgfRR6fjIo5bkzbfimNzapJ2LEn+fkwuMnM6zv+qBef0/3EC3yBclOV/kv778Si2Lc6Xgc2RvOj+1PE+x0Fy/Cs4dmghjlGW710VcHsyBtYlbT4o+F6m+qGfvonyo/8+2mzlR7E5kvr8WFwGhybg+8DdDCxwanwyFlYk8+arJBHxwfmv4FxKqlKfj+i82dA1gXOSc1pxsvKCvr6LU6quTl4fSh8yM/jsRJxfexMuS8ANwHtS53wj6ccmXFBVWdKfC5PPryYJmKUwun8pSdaAURhPylrQDDwLnJ4cew/OBaEZlzXhp+qzYFwtIBfd/zpJcOpQ/inyLSIiIiIiIiIioh8Y5/u8FNjWWrtukNfYBrepqLT5zGpJImGKm4DZ1trXhvLasVpJRERERERERMTAMAHH0g5KQd1cYIw5IXFVGItLQfUMjpkdUkQlNSIiIiIiIiJiALDWLrfWXjra95EBnEiuQMZsnAvakJvmo7k/IiIiIiIiIiIic4hMakRERERERERERObQV/LjgaDUKdihTpewyf3R3u5yMt9www0A3HHHHcyaNQuA5cuXA7BixQq22GILAHbccUcATjzxRACmT9+kPMCZ64+VK1cCcOeddwIwd+5cqqqqAJg/3+VI3nLLLTnyyCMB2HVXlzq0sjKXe1yWApeVZKOQuf4YZQxHepHYJ/kY8v64+uqrOeaYYwCYNMnl4W5tbeVPf3I52Q85xGUymzFjRvELbBwy2x9dXV0AXH755V5ONDe7lJ8HHXQQ9fX1fd9ElCFDhRHvj56eHsrKHBdX7Pk1NTUB8PnPu8x9++67L+97n8u0pPExffp0fvzjHwPwyiuvAPCDH7iU0+Xlm1TzIY6PfBTtj8ikRkREREREREREZA6b4pO6WWrtm4BB94d2+fvssw8ARxxxBADd3d088cQTAKxa5SqONTY2cvzxroKhmMbXX38dgCuuuIKxY8cO9jZGpT96e12uZe12FyxYwNFHHw3Aiy++CEBDQwPgGFK1ecKECQC0tbXR0dGRd81TTz0VgGuvzRU/GQQbkpnxkRFkikn9+te/DsC3v/1tALbbzuUub2pq8s+6pcVVSzzllFP45S9/CeTGxu233w7A0qVLqa0NC/VsFDI7Ro466igAXnvtNbq7XYYbWSHKyso8cygm6IEHHhiKn81cfzz0kKuVofbdd999rFixAoCKCmdIPOOMMzjjjDMAxzJDTr5ATnYIpSZD7rvvPv7yl78AcOONNwIwe/ZsAN70pjd5+VpTUwM4q90999wD5OTze97zHgCOPfZY/91BYMT6I3xmel5iRp955hlWr3aVhMeNG5d37PLLL6enpwdwVjqABx98kKeeegqAX/ziFwDsv//+gFuvGhsbAdhrr70ANmYNzsT4yBCK9kdUUocOm9wfn/rUpwD8onnKKad4s5wmztKlS/ngBz8IwK233grkXAF+85vfbMrPZ6I/pk+fzn/9138BeLeG8847D4C6ulypZQme9vZ2r9Rec40riKKFd+HChWy1lavcllaGB4BM9EeGkCkl9c1vfjMAL7zwApDbyBhjaGtrA3KKxpIlS7zyMXnyZADWr18PwCOPPMK22w66oFDmxsjCha6ctpTU6upqv4iGY3/qVFc+XIvzO97xDgA+9rGPbcrPZ6I/5s6dy7/+9S8AbrvtNgDv8rB06VJvslV/nHrqqV5OvPTSSwDst99+gHODKDUl9be//S0Av/71rwFYvXq1b0N1dTWQk4fd3d1+8yK0t7f786TIiwgwxnDAAQcA8LOf/Wxj739U+uPxx13l0ueeew6A8ePH+zarXyQ/Jk2axIMPPgjkZEtvby8f/vCHgdwa9OyzzwKuf0QgSaYceuihfjxtAJmYLxlCNPdHRERERERERESUBjYlcCpiiNHZ2QnADjvsADhTnXbt//73vwHYaaed/C5Yu7+XX355pG912LDbbrvxj3/8A8ixQdrtWmv9DlguEu3t7b7fZLJTEMgDDzzAySefDORYE2vtYAIgSga9vb0FbPGnP/1pAO/8vzlAbRSzIVNlb2+vHyMKRKyrq/OsvMbSf/7zH8C5zGwCk5o5yIypgBBrrbfMqK+6u7s927x27Vpgk4MuM4UbbriBmTNnAnDwwQcDOevKW9/6Vu6++24gx5Zus802noEW0y5GdcqUKZ5VLIV0jY899hj/+7//C+RM2ePHj/fyMu32VFZW5tcTYcyYMQUyZMyYMf57jzzyCJBj3WUCzyquuOIKAPbcc0/AyQWxngqyXbBgAeBc5+Q6pAC7+vp6lixZAuTkhtDZ2en7Rmzzn//8Z28Vjdh0RCY1IiIiIiIiIiIic4hMaoag3b6YoOeff55XX30VyO2Ky8rKePTRRwG8r1mYcqlUsfvuuwPOf1A7UrHHYsna2tqK7vDlt6t+E2N0yimn8OSTTwK5AJvNnUkN2Z7HHnsMyAVL7LTTTpx99tlAzsd5E1OojBoUQKegIDFFnZ2dvm0aN2VlZaxZswagIEhq4cKFnlHbHLDHHnsAeObn2GOP9b546dRLAPfee+8I3+HwYfHixYAb02KSZWXRmGhsbOSOO+4A8L7sXV1dng2TnF22bBngmLVSYtp/9KMf+dea262trV5uysdU8wYo8M/s7e317KpkpY5VVFT4VGbPPPMMAK+++qpnH7OGF1980d+v2r527Vr/Ws87tMRo7kimlJeX+/GhvtK40nd0HjgrhoLzxMxHDB6RSY2IiIiIiIiIiMgcIpOaIcgPSj5QL730kmcEdtllF8D5u4gB0M5NjGopQmmi5Fc7efJkzwyHPnT6L0ZA0doVFRUF/oba/U+ZMsWzJsJGRPeXJEKWWD6Z6s9f/vKXPr2Z/J5LFfKl1DgQEyKGBHLsmbW24Hg6TdXmiqOPPtqnz5HfaU1NjZ8zmxM0FhobG72PodIIad6vXr3aR7/Lb7Wzs9On5NL4EPP+6quveia1FCwwc+fOzct8Ao7902chgwquvVpHxByG0DwJE+JrXulazz33XGaZ1AceeMDf+7p16wA3JtTmdPrC6upqPwb0vd7eXt/WdF/V1NT468ofvLy8nOeffx7IFcsYSVx++eU+Q04xiAXW/7DNQzHG1Vdz584F+l9r3vWud/Hxj38cyFk20siEktpfDstigSDCH//4R9797ncXXKsUhEkxKP+cFDdrrW+7zP7l5eXevC3l9Bvf+MYI3+nQQQuGzC6VlZUFwjJMh6L+kNJRVVXlhaa+p/PLy8v9YiXzsEw/mxuKBXWoj9Q/a9as4aCDDgLgtNNOA/LNg6WEtIlNcz6sMBOmHUunINP56UVqc0NTU5OfD2r72rVr8/KAwiZVVcoMZKIvKyvz5lnJzeOOOw6AefPm+Q2/lIqampqCXJoKLJs4caK/fin00apVq7xLi5TU8vLygs1ZGDgVkgCQC5KCQplaV1fniRPNqSwH7j766KPsvffeQG4sPPDAAz5fcjFXOfWD2tfb2+sJE8mbMPBKOXilqDc0NDBv3jxgdJTUj3zkI17mF0spd+aZZwL4NFn19fVe0RZCdzC1NR18F0L9snbtWv9abkZHHnmkd7cTlC7z5ptv9sHNfWHzppUiIiIiIiIiIiJKEplgUovtTNNmhvAzUdkvvPAC3/3udwF8Woz+drlhqo0smn0/+9nPArldRn19fUF6kJ6eHr8rVhLhbbbZZsTucajx2muvAbmdWG9vr2cAtaMNd27Fnps+K3ZM5lxVnlG1rs0NGvfh+FcqGu2EJ0yY4JkDjbHrr7/eMyFhsQRwz6LYdbOA9LwoxnLpnNAikUYppBXaFNx4440+eEPm8MrKSp5++mlgUEUuMgsxfGPGjPGvxa4K06ZN8wGZBx54oP9c40Z9pMCX2bNne7ZdcinLWLt2rbdKad739PT44g1qSxgwqWevzzo7O/1rHVPAkDHGs81ah7LMpK5evdq7ckyZMgWAX/3qVz6IUOyn2tne3l5Q3KCzs9P3pcaAZGVZWZln3cOATVnuRgNf+MIX/Jx/17veBcDb3vY2wK23mhshWyqmPC3nu7u7fdslK/S98LOQfVb/yb3o1ltv9WPlvvvuA+Cwww4D3PqzIRlc+pIpIiIiIiIiIiJis0MmmNRiKMaMfOADHwBy5Q633nprn47p3HPPBeDiiy/uM61O1tmCnXfeGcj5mq5fv94zX7r3cBejHYv8DEsRSqI9fvx4wO3I0k7cod9hmtkrKyvzYyW9Aw4DrVRCdnNlUsP5Iqd9le7Trr+9vd33pXa5a9as8czL3//+d8D5EEG250s6AETtD32aJSeWLVvm/ezSYyt9nc0Nr732mmeLli5dCjj58vrrrwP4FG3y2ytlyK+uvLzcM17yz9SxqVOnevkqH0X5qEJufIhpPvjgg71feykEG8oPFXIs18qVK/1c0PoRytFi1rp0ijrJ1lWrVvmAGzGUSv2VJej5zZo1qyANWW1trWc6te5IBlprC3SPsD90LfXZ+PHjvbVOfV9TU+PH0Ysvvgi49H/DDRWpKC8v553vfCcA3/zmNwG46qqrgPyiHXq2oT9qWnfq6ekp8PuHQgZVbGt1dXVBTMA222zjGXz1owoRHXPMMfz+97/vt12ZV1Ihp8hIyMoks3z5ci84ZMIZO3asV/be9773AbnFavr06Rx77LEjcPebhtDxPf3AjTF+YJSC+ak/9Pb2FtRUX7t2bUGt8f5MzcVMBWGFKk0wuRVsrgj7SMFQEgSaN1VVVV6Bk9AYN26cX2xU4evyyy8H8PWqswwtnuHiq2euymMLFizwSqqOaYxsrkqqZOaECRMKIpLLy8u9PFGuy81BSZ0/fz7gNmUKeFIWCM2PdevWeaVUinp3d3deRhAgb7wsWrQIyLaSGpqX0xv5MBBVylN6HkBxt6k0ARDm2NX1lVc0S9AG/ZVXXmHfffcF4K9//SvggujURslBrbnhmhpWKUxH94fuAdtvvz2Qq0Y1Y8YM72byyiuvACOjpCof9p577ukj6/XsFZTd1dXlia8wsDatRIYbF10jNPeHCjzklNSGhga/7siF4LXXXvOKsALJbr/9dsBVQ9T5fSG7VElERERERERERMQbFpllUkNzg1hSaftKI9TS0uJ3ONLsd955Z58bT5S/tPiOjg5mzZoFjMzOZrDQzqWsrMz3Q7i7Te9iShVi+CC/skmaCdjY4JYwAEBQIMTmhmKBL0qJIod+7Zi7uroKzm9ubvamLjECF110EeBSm51yyilALggrK0g7+ovpWb16NdOmTQNg//33BxyDonQraXNWmG5nc4KYpLKyMh80oz6rra311iWxiZsDxIY1Nzezzz77AIWmaGutnxdilIwxXlbINUYBJ2vXrvUsUZYRytK0vCwWJBWyhEIob8W4ps3c3d3defmHIZtp3GRFPeqoo3xbNf7Xr1/v3b/EnEt+9Pb2FuRJDV2IwmuAc5dSGkylndp///39Z5KtIwHJ/U9/+tNeZxLDLgZ8xYoVPtBabgrWWt8u6VN6puHYD038xdwPwa016WMzZ870lkzJJY2nF154YYPjJzKpERERERERERERmUPmmNRiQTDaFYjpEbbZZhsfJKLdYk9Pj98diXHV7njNmjWZrsMsPxb5UY0ZM8bvbLQ76e7u9gyAzlP/iDkqFchvDjY+xVHod5pmBcJraVcc/tbmhHS/WWt9ihGN+5D50DzRznrMmDG+j8Qqyk+4ra3N+3NlDdr5ixUTi7ZgwQLfHvmfX3DBBXl1yUOUul93XxCDWFNTU1B3vKqqqiBNUSlDSfn1jN/85jf78SBoTPT29vo5IDkaplpTf8hv9e677/b+8mFwSNYgFi/0nxTGjx/v2aqxY8fmHSvGHPb29hbMEzFfu+++uw9W1u9IXmQRYfGWMGj26quvBnKVxWRh7ejoyFtrIb+ITDod2cqVKz1rr/+jBT2H7bbbzlvDVFlOfp9TpkwpCBptbW3Nq9QXorOzsyAuppgfv/ojZFLVV5WVlT7ORL7iL7zwAuDGrfSYvhCZ1IiIiIiIiIiIiMwhc0xqmhl6+OGHfbSxStuJBdppp528Bi9GtaWlxUesSmsP/UnSaYqyBCVYl6/I2LFjC1jCsrIy30fa2Vx22WVA6TGpfflRiekolsw/fU6xSFQhTCycxTQpQ4E0e/zMM8/4HXW6lF93d7efOzo2duxY/5mYJbGTe+21F+9973tHohkbDTGBmgNiC40x3t8yjFgXm5wug7ihyNJShdj0zs5OL//EBNbV1fnXYcqiUoWYGMUe1NbWFmRvkBzo6uoqkCvGGM8kqT+0dlhrvV+fjmWRSdXzrq6u9u0Smzx9+nTvE6iUS2pLmIKqPzkr+bLVVlvxz3/+E8jNuSxmyAifbVpGtrW1+X5Ilwzu7u7O8+EHN3bURsmPMFZE8jNMY5XGSBRDCeeyMg1oHIdxLHpuYdo+yQMx5hsrF8JsB+l4gdbWVt+X6RiA6upqnwavLwxaSR2uOsbqJDk2v/rqq3zlK18B4B//+AeQq7C0cOFCP1j0WVdXlw+SEd2crlucVfz0pz8FctR5eL/hawkVCaErr7wSgCuuuGJE7nOo0Nzc7MdPOLDTqadCh/60439olkqnVSkvLy8aIFBKsNYW1KnvD7fddpufQxpH2vQYY7zZRddsbW0t6FMJlNGsmrIhaOxLQdEc7+7u9iauYrIp/Vm6hv3mApl/W1tbvUlT46K2ttbLSG1IShnpDUh9fb1XHKS8hRVx0oE/oQzRBk9K7YIFC/x4CgMxswbN8TDYVhvRSZMmFVSFCtfEYmt5On2V/q9ataogqCqL6E8vqays9ONCacWklIVBUuG1QncRyK+QWGzTMhoV+vQcFy1a5NOCyQUhbJPGsf4Xq8gX5iEPXwN5rgHpObR+/fqCtpeXl3slWHJbpNHKlSu9fOoL0dwfERERERERERGROQyaSR3KnYJ2r9ddd51nULVL22677fyuRWypdoXr16/3ici1U9huu+18TfswACk8J2sQ45s204bpmELmULsX7eDEjs2fP5+ZM2eO2H1vKjo6Ooqyg2mmIxxrYcAUuF1amiUNd4VpU9TixYvzqm6UAvpjUNM74CuvvNInsRdbIIYpDAAQc9Db2+s/E6um8aRE1FmEduQaK2qPtbaAHTXG+LmvuSUoyHJzQ5jWJT0Henp6/GebA5MqM78CnOrq6vwaoUDZkGkPKwPp+1pHBMnWiRMn+mCjLLtGiBkPmcAweKxYURjoOwVV+nyd19PTU1AQwBjj+6YUUrqF7LHuV8x7Q0NDXtELnS9I3oTFHtJBZqMF6TuLFi3y9yx5J52orKzMs5ph0KTaVSyZfzErZHrd0TmdnZ0FaQ7Hjh3rx6I+07x88sknN5geMjKpERERERERERERmcMmB06FPnP9+YCFxxQwc8MNNwDw+OOPA06L33HHHYFcOqYnnnjC71qkhSsBdVdXV16KCXA7AO2olcRavhevvfZaQQqJLEC+tvJ9KpZgO9z9p/tUqbluvfVWzj777GG/36HCmjVr8lKHgWtTun3hLi+dKLisrMx/JiZaTvFQyEI+++yzJcWkhvMmXU87hHyPent7PfMT7m7BsSxpv7ry8nIfPJROHaL5k0WIISiWFkWJ+4WGhoa8YMQQ6febCyTfxowZ44MnwprrYss3h/aLQQpTbGn9UPvC0o6hPyG4OdBXmcedd97ZB6Gk2aMsQX59NTU1fi6IDW5tbS0INFX7Qv/CkD0L5THkZElDQ0PBd621fmyVApPa3d1dUCZZFoUpU6b49hWLe0iXCO0rddNoIIzV0ZwXwmBr3XsoO9NyNLRm9ldUp5ilNx2s2NHR4a+ndHG616effnqDVvlN7uGwVvZAcOmll/oodilXYfUTmfvDa8qcowGi85csWeInpzpp5cqVvnNkwhG13NXV5WlvVaXKAu6//34gd78HHXQQAPfcc49vuypkzZ8/3ysPb3rTmwAXXAbw4osvjtxNDwHWrVtXoJCuX78+r7oJ5FdHCc38+p6UqnBS6H/avLt8+fJha89wIz3P/vKXv3DqqacCObN1qICno5G7u7vzcv+BE0ZS7vWZhHSWNnJppJXUUICma6yHWTLSpu/NQUkrBsmN6upq31dS1KuqqvwivTlkN5D8EyZNmpQXJBaivLy8QAELXajkGiMZYoxh/vz5QE4ZljtNliBzdUVFhX/OImuampoKKjMK3d3dRTOlhJHtkBtPqgEP+YpsVl3piqGjo8PP+/TGP+yfYmb8tCl7NAKk+sLOO+8MwIMPPug3oWmErj7FsjKkFdGenp4CIil8rTUjzDks6Prl5eVeBmkciYy8+eab2XrrrfttV3a3hhEREREREREREW9YDAlXnd5xaMf5+uuvs2jRIsCxguBM/XvuuSeQY2xUzzVMkSPNfP369V5r1+5H7OkWW2zBdtttB8BTTz0FuB2lqkfofDFFY8aMyWTaDJmTFixYAMDb3vY2wDGlYkdVj7y3t5c99tgDyJm1Vb1B7gKlgnXr1uXVzwbHJmvnlmYCQyYx3MWn86qKkQ5ND4JYhtFEOm1Hsd17MTPS3/72NwA+9alPAc6SoIpQYZ5H7VbVt6EpJm2yHDNmTEHtZPW32JksQgygxkH4nMUgCfX19QWpiNKBBZsblBczlJ8yaZaVlfm5leW0SgPF4YcfDuTkfFlZWcHc0hgPj4XrVrofNF523XVXL5ezHGSn9lVXV/v0Q3J76erq8m1NB39Za4uuiekAVcmU7bff3s89XXP69Om+79Pud1lEW1ubl41igMMKbGn5HFblEkJGNSuBU2eeeSYA3//+972+IMtxmAs7LQPDdTWdcqxYyrZijGpooUv3VXt7e0FeZsmi1tZW3vrWt/bbrsikRkREREREREREZA6DZlKlQX/961/PqwMOuV1GWN1DnzU2NnrNPb17raio8MekyYcskxha7eT23HNPv2uUr8ycOXO8xq/dnd6vXLkyk2lE5Eeo3YYqR3V3d3vm67nnngPcTlbsz1ve8hYAfvnLXwI539tShNpezO80hPoj9CFKVxEKHd91LfkxbyjdxXChWKqX/tontLa2cuihhwL4mtl6v+OOO/odaciMyY9M/aB5WVNTk1d5R99L13LWTlhsUhaRDg7rDw0NDb4tpVrUYWOxcOFCwI0fBStIVk6cONEHFmW5YMNAIctZiLSvYViBKs2ipZl3yMmJnXfemQ984ANDf9NDDM310Edf/TJv3rw++yNkWUOk2cTQUiGLnfz7KysrM7mu9oXy8nI/BnTfYZB1OlAoDDZKy+eenp7MWGePOuooAP7nf/6nIA2Z3re1tflYA42Fjo4OP2bSgVPheeE4SRd+CX2Si6UoE6Mrhldyp7a2lo997GP9tisyqREREREREREREZnDJifz/+AHP+hTSL300ktAbocV+rppx7Ju3Tq/S5Wmre9tueWWPoG4NPPu7m6foF5+VnPmzAGcj54YEkW/hz6H8q0Tk1RRUZHJaNYTTjgBgD//+c8APpp0p5124q677gJybamvr/fRy/L31Q4ni23rD/Pnz/cshtrQ1NTkd3pp35aenp6iu/4wHRXkGPpwN6hrPfDAA0PZhAFjIFGgq1at4rHHHgPg9ttvB+Daa6/1u0/tOOfOnQu4dB7pnX11dbVvt/pBFoXQJzWcG2l2Vd9vaWnx6eJ0D1mBxnqxFHfpMoXjxo0rYFB1vtq+uSGUn+k69pAr2LC5MsvpNDhCKD9khejq6ipgz0qtX8JS2lp35et3//33F8ifUDb2l1orXfyjurraM7Raf8aMGVM0UjyrqK2t9bJR/aL+a2trK5qovpgfM+Snb8oK7r33Xv/sizHA6TEeZtQpNu6LtS9tDQyzR4RZM3ROuviK9Lzp06dv0I950EqqHKkbGho4+eSTi56zdu1aT/Pq/JaWFj+J0maWrq4uHwwUOummK0CoQ9auXesdnnWssrLSCyaZxdVBYWWMLOGAAw4AYL/99gPgiiuuAOCcc87xZk2Zc9ra2rxD/AUXXADkhNG55547cjc9BGhra/M1fMPcnGprOjVKGAiVrtet4+H57e3tBSZr5WcbLfzpT3/i0ksvBXImD82RUBho4m6zzTY+YOP5558H8DnwmpubC3KhFhOaOicUVKFwlgKveRYKKvVf1pTU9OYmRFpJraurKzBbZjm91lAgzFcphV4Bp3V1dX6+ba4puNIuLGGwiF5rLejp6Sla3Q6KBxtmEeGGXq+V5gcKFZNimzuhWK5qfW/evHmeNLrjjjsAJ6c3Jg3laKO1tdW7Diptk+ZDsQqGYRqmtOm7rKwsc8GHDQ0NXodQMJXWjGK5sru6uvod4/0FRxVL7VfMXU9yRnqg5O+//vWvDbYn+7MvIiIiIiIiIiLiDYdBM6liJ+fPn+/N09KOlQJo/PjxnrkKNXWZ7RVwJeazrKzMfxYmck9r8mEi2XSi/7a2tj4dmXt6erzT94EHHjjIlg895s2bB+Qqb7373e8GXJJqffaud70LcKYb7YpOP/10AG666SYAbrvtNo499tgRu+9Nxd///ncuu+wyAN773vcC8JGPfIS//OUvAD7Jb7FKKEJvb2+B6SGs5S32SKa9dHLvkYLM+F/72te8k77aJ1eVkLURE7ZixQrvPqPPlLJswoQJeTXrwTGq6ZRS4Q5Y80S76TDlStrZvqysLFPJqkNoDqQDPKCQSa2trS04L33O5ga1r6enx7MYGj9VVVVeVm+u/SALS5pB7O7uzitkoWNphrGY2b8vtjVLCAPDBMnDYjDGFBQ3MMYUzBddc8mSJUXTtmWp8tKGUMw1QTIzDAAK+0PtSxeOCc/LElSB8Mtf/jIA559/PuB0Mz2//tJGhWkfjznmGP9dgIcfftib67VeaXxUVFT4MRNWilQaNwW4//GPfxxwWyKTGhERERERERERkTkMevujncTs2bP9rkssqNirlStX+jr0odaulCj6rx3+mDFjCsqThekipO2Hu10xAvps8uTJ/hohc6Bzsli3fZdddgFy5RzFlM2ePZvTTjsNyDGBYdohsaza/ZVC3eQ0Pv7xj+e9nz9/fkFJtzAwSmMg3A3rtcaJxoRS7sDoMajCgw8+CDhmVCyg/D0V2FRZWel36mI6w51pugTwsmXLCvy1y8rK/C64WAJqHQvTxKVZE/V3lseTUoqlfQ+hMLVXRUVFAdtRakGGG4sw/V6aOezt7fXjrNQChAaKtN9dGCSSLg7S09NTNEgz/F7WEfpcK05DWLJkiY/1KJZiSAjLT6fLnOrY8uXLveVH6OzszGSsR1/o6uryOoGee5g+My0/Kisr/XlaW/T9LKWgCgtYSN5Jf5BuceGFF/oCQWGAcnrdCeMYfv7znwM5v9KQrVdfSd6MHTu2oFhAZWWlL+F+1VVX5d1zaNnoC0PC0YfVgsL/EQNDWrmS8jJv3jxPzUtBmThxot8MSMlXkM2GKjdkDcWCEsKcdGHVC+jbrBJWJ4PcxNH7Df3mSOC4444D4LrrrvM5b9NZDKqqqvxrLaRVVVUFEfnp/0CeY38xc6b+p82YZWVl/voSNOrnGTNm8MQTTwD5QRhZgJRULRahMpE2d4b5ctUncrnYXCFyoKGhwedE1X/JDchlYtncIBO3nnPo0pJWPDs7OwsCYrKieAwUoZJVrG57OhCqWKBTeI5kiz7TNZuamnx2HaGnp8cHwO6+++6b2JLhR7iRl6zQeAnXH8nRzs5Of55kZPj9rASN9RcMJ/P/TTfdxDPPPAPkAt9eeOEFr5ym3cGstXlZMMC1PU0Whe5DcvFUZcxjjjmmz8p+A3ETieb+iIiIiIiIiIiIzKF0vJ3fABA7pFyv7e3tPPnkkwC8853vBOAf//iHT78js46ckkshVUqIYve71VZb+UCyMPUU5KeK6a+Ck44Vq8A1WuY73ct9993nA8N+85vfADlXgHnz5g3o/tTeMBBqKBG6ncjRPWuQ+TLNmm611VYF5s7q6uqC3H197ew3F4Spl9TmYmnKxJKUMtIBTS0tLQVuMKE5PJ22LHxf6kxqV1eXT1EoPPbYY97FSG4uYfvUb6G5X32aTk93zz33cPnllwM5829vb6+/fhaRdgcrLy/3/SAGUG0JWfWwMpmYU8kNjZNx48ZlJvhwoAFcu+22W97/rKO0tJqIiIiIiIiIiIg3BCKTmiHsu+++AN6xuauriz333BPI+ZHtuOOOPiBIu+Gjjz56hO90+BDWEi/GkIbpyoR0dRk5gS9dutT774pdy0IgxIknnpj3P4R8u+RTuGTJEl599VWguM+RfMXCqmpiANQf4U4/vdsOAxPTzvCTJk1iyy23HFQbhxuyJojpUeBGZ2dnQdBM+Fk6pc7mjrFjx/qxIbaotra2oGJXKSPNpHZ3d/sxnC5QEVbgElpaWnwfpf3bS6V/Qv9zWeSEBx980BcMUbyI5ktvb29Bf4TWGTGHmj9h0JRkaltbW4H1IstYsWKFtyDoOYcVqCQvdc64ceO8dVPnqb3Lly8vWGMihhaRSY2IiIiIiIiIiMgcIpOaIShyULv6mpoaH0kp5rCsrMyfFxY1KFWkS5nOnj3b+6SqdJ12r32lOUlHyWtHe/jhhxfsbrMSidkXlCIti6nSsgQ9RxUSkcXhySefLPA3ra+v94UTxBKpHOLmijB7g+SF5sfq1au9Neaggw4anRscQhQrZaoMKWqn+kDHQ9TU1ORl04BcOsWwJGaWIdavqampQE4qsnu4sHr1al+yOZ2eKgtIxz40NjZ6f0yVe1aftba2+kh/fW/u3Llsv/32QE7uyBKxxRZbbPYllkcbZhPMn6NvN900DLUdZ5P745ZbbgHg9ttvB5wZSkJWitrYsWO9OUeLztve9jYAzjjjjE35+cz1x7PPPgvAQw89BDiFRKb8MAWIXu+9994AHHXUUYU3s/HVYjLXH6OM4bB7Dl74pOTWKJllMztGpKR96Utf8iZe5VU+4ogjvIuQFt8hCiTLTH/cd9997gKpeR/WmZdMraqqKnCN0f9iwZcbgRHrj0ceeQSAW2+91buNHX/88e5L1hak8SsWeDoQhAqfxtPSpUt9pcMNXCsz46M/KFWb0pctWrSoIBhtiFAS/TGCKNof0dwfERERERERERGROWwKkxoRERERERERERExLIhMakREREREREREROYQldSIiIiIiIiIiIjMISqpERERERERERERmUNUUiMiIiIiIiIiIjKHqKRGRERERERERERkDlFJjYiIiIiIiIiIyByikhoREREREREREZE5lKSSaoyxxpjtB3DeNsm5pVs3dAAo1f7o774H2qYi3zvTGHPfpt9dxOaKUp0vESOHUhwjUZ72DWPMPGPMEX0cO9gY89JI31PEwDCkSqox5iBjzAPGmLXGmNXGmPuNMW8ayt8oJbxR+sMYc5cxZo0xpnq072W4YIw51BizaAiu0xL89Rpj2oP3pw/FvZYq3ijzZTBIFtl2Y0yzMaYp6aezjDElSTQMFm+EMRLlad55wy4vrbX3Wmt33MB9FFVyjTGnGWOuydJmpS+UqgwZspszxtQDtwA/ASYAWwIXAuuH6jdKCW+U/jDGbAMcjKsb/I7RvZvsw1pbpz9gAXBC8NnvdF4WhN1I3sMbZb5sIk6w1o4DZgLfBc4DLi92ojGmfCRvbCTwRhgjUZ7mY6DycrgwABn4duCvw30fQ4jSkyHW2iH5A/YFmvo4th1wB7AKWAn8DmgMjs8D/gd4GlgL/B6oCY5/HlgCLAY+jJvA2yfH3g48AawDFgJfD763TXJuxVC1M/ZHQVu+CtwPXALckjr2a+D/AbcCzcC/ge2C4+F9H5Tc76FFjlUD38MJqWXAz4ExfdzPmcn9/DTpuxeBw4Pj04GbgNXAK8BHg2PVwA+Tfl2cvK4GxgLtQC/QkvxNH4K+mwcckbw+FFiEExpLgd/2dT9BO+9LXS/ss+OA55N+fx34n+C844EngSbgAWD31D2dl4y99UM5VuJ8GZqxEny2XzIm5+Dm2qW4BbMVOCIZ638EVgCvAZ9OfffRpN3LgEuSz2uAq5O+bgIeAaaOdvvfKGOEKE83ag6kjk/CbWKakvu5Fyjb0PMnkb2p3wll4LXJvbYn9/qF5LyypP8mJX1pg/YcmBw/H5gPLAeuAhpS4+ZjSd8sIZDRwzR/CvqPEpAhQ9kB9clN/QY4FhgfHNseODIZoJOBe4Afpjrv4aRDJgAvAGclx45JOmBOMrivIX/CHQrslgyI3ZNzTxoOARL7o2g7XwHOBvYBusLBmAz6VclgrsAtHNcFx23SF8fgBOp+6WPJ6x/gBOEEYBxwM/CdPu7nTKAbOAeoBE7BCaUJyfF7gJ/hJtKeuMn3tuTYN4CHgCnJc3kA+GbQr4uGos9SzzlUUruB/03GxZgN3M+Z9K+kLgEOTl6PB/ZOXu+FE5j7A+XAB5P7qA7u6UlgBn0sXHG+jPwffSzQuMXxE7i5thZ4S9KWWuAxnNJTBWwLzAWOTr73IPD+5HUdcEDy+uO4+VWbjI99gPrRbv8bZYwQ5elGz4Hg+HdwCndl8ncwYAbw/PPuhSIysNhvAwcAD/Y1DnCbnVdwc68OuBH4ber8a3Fjbrek7/ps3xCMraL9R8ZlyFB3ws5JQxclA/smimjQwEnAE6nOOyN4fzHw8+T1FcB3g2M7EEy4Itf+IfCDvgbOSP5t7v2B2613AZOS9y8C5wTHfw38Knh/HPBi8N4CX8LtNOekri2Ba3C7upAxOBB4rY97OhO3MzXBZw8D78cJnR5gXHDsO8Cvk9evAscFx44G5iWvD2X4ldRO8tmd/u7nTPpXUhfghEV96pxLSRaK4LOXgEOCe/pwnC+jLz/6Giupzx8CvpL021XB5/sDC1Lnfgm4Mnl9D85UPil1zodJsetZ+tucxwhRng5qDgTHvwH8pdhz28Dzz7sXisjAYr8NfBO4oK9xAPwLODt4v2PyfCuC83dK3dPlwzh3ivYfGZchQ+owa619wVp7prV2K9yudDrwQ2PMVGPMdcaY140x63BU8KTU15cGr9twmjnJNRYGx+aHXzLG7G+MudMYs8IYsxY4q8i1RwVvgP74IPB3a+3K5P01yWch+mqH8Fngemvts338xmSSHV3i7N0E3J583hdet8lsSTAf12/TgdXW2ubUsS2T19PJ7099b6SwwlrbEbzflPt5N24Rm2+MudsYc2Dy+UzgXPVl0p8zUtddyCjgDTBfhgNb4kybkN/OmcD01HP+MjA1Of5fOGXsRWPMI8aY45PPfwv8DbjOGLPYGHOxMaZy2FsxQGzmYyTK0wHCGLN1GFSVfPx/OOby78aYucaYL6a+tqG+CzEQGXgc/fujFmt/Bbk5mP6dkV5vhEzLkGGL6rLWvojTzOcA38btGnaz1tYDZ+B2dAPBEtwiKmydOn4Nbjc9w1rbgKP7B3rtEcPm1h/GmDHAycAhxpilxpilOJPQHsaYPTbiUu8FTjLGfKaP4ytxvkC7Wmsbk78G6xzp+8KWxpiwzVuT84uaYIwZlzr2evJ6MW5ipr8H7nkNN9K/0d/9tOIWGwCMMdPyLmTtI9baE3Gmtj8D1yeHFgIXBX3ZaK2ttdZe2899jDg2t/kyHEii2rcElCIofG4LcexY+JzHWWuPA7DWvmytPQ03Pv4XuMEYM9Za22WtvdBauwvwZpz/8gdGrFEbgc1pjER5unGw1i6w+UFVWGubrbXnWmu3xQWdfc4Yc/hgf6K/94m83QJ4vI/zoXj7u3HuIkJ63C1mBFEKMmQoo/t3Msaca4zZKnk/AzgNRyWPwzkTrzXGbIlzUh8orgfONMbsYoypBb6WOj4Ot5vrMMbsB7xvU9syFHgD9MdJOFPPLjhfpD1xprh72bgBuRg4HPiMMeYT6YPW2l7gl8APjDFTAIwxWxpjju7nmlOATxtjKo0x703u66/W2oU4M8R3jDE1xpjdcbvBq5PvXQucb4yZbIyZhPPF0bFlwERjTMNGtG1T0d/9PAXsaozZ0xhTA3xdXzLGVBljTjfGNFhru3CO7b3J4V8CZyXskDHGjDXGvD210Iw43gDzZchgjKlPWIvrgKuttc8UOe1hoNkYc54xZowxptwYMydZlDDGnGGMmZzMr6bkO73GmMOMMbsZF9m7Dmee7C1y/RHHZj5GTiLK002CMeZ4Y8z2iUK9FtefQzV2l+F8MoVjgdsDhnlF8lvhOdcC5xhjZhlj6nAbqd9ba7uDcy4wxtQaY3YFPoQL6Bp2lJIMGUomtRnnw/BvY0wrTnA8C5yL81vYGzdwbsU5EA8I1trbcD5Ad+Co/DtSp5wNfMMY04ybBNeTDWzu/fFBnG/KAmvtUv3hokBPNxuRvshauwAnWL9ojPlIkVPOw7X1IeNMef/E+ff0hX8Ds3GswUXAe6y1q5Jjp+H8gRYDfwK+Zq39Z3LsW7hoxaeBZ3C75G8l9/giTujMNc70MRJmmf7u5z84H6x/Ai+T2wkL7wfmJf11FnB68r1HgY/intMaXL+eOcztGAg29/kyFLg5uc+FOB+yS3ALWwGstT04BmNPXFTuSuBXgJSCY4DnjDOV/uj/t3fm0VGV5x//ZmYSEgJJgCCQiERZXHBBxbVqqVYtSq2n1WO1VdRq1aJWjutR63LqadWK1qVq5bTVUqmI/hRRWwWpKIIVd0QkbGIIBiGBMEkmmcnM/P64fp/7zjuXkISZyQ19Pv8MTO7M3OVdv88G4KfJZDICYCiA5+BMLisALIRjvvMDu3Mb0fF01xn97bU0wQnqeTSZTP4nA98LOL62t357rtfBSj2VTCZb4Nybd7495mg4vs4z4PhurgPQCuAq63sXwnkWbwC4L5lMvp6h890RvW4MYeSboiiKoiiK0gHfbhjqAOyTTCa3d/M7quAs/PItZVWx8HWlAUVRFEVRFB8xEE5Uf7cWqErXUCVVURRFURQlR6iS2nl0kaooiqIoiqL4DjX3K4qiKIqiKL6j0xGDHvR2CTbTuRD1fqSi9yOVbt2PZ555BoWFhQCAgoICAEAikZ7NIxAIyCutI3369En5W2trK37wgx905zSA7OQO1TaSSpfuR0ODk3978+bNWLx4MQCgqcnJa37VVXYQcSq33XYbAGDixIkAgEgkAgAYN24cBg4c2JXTMPFFn/ERej9S6dH7wTbe0tKCRYucZCgVFU5SgSOOOKJT31Ff7yQ1WLbMydg0cuRIhELOMmrYsGFdOR3AR+2D17Vq1SoAwAsvvAAAuPjii7HvvqmJH2bPno33338fAHDZZZcBAPbZZx9kAM/7oUqqoiiKoiiK4jt2xSdVd3Wp6P1IRe9HKl26H1999RUA4I477kB5uVOB0VRLCf+d921BmGQyKf+mkpqf71Ska2pqwjXXXAMAGDRoUFfPX5XUdHqkjdx1110AgHg8DgCorKxEMBgEAEyfPh0AcMghTpGiiRMnijJaVFQEAJg6dSp++tOfAgBOOskpyPPRRx/J9++3334AHFW1i+gYkorej1Ryfj+ampqwbt06AJA+MmDAAMRiMQBuf6Gieuyxx+JPf/oTACAcdqq9jhkzBpWVTqVXKocrV64EAAwdOhQbNzpFolpbnYrWlZWVGDy4oyqzgi/ax7XXXovPPnOq6HLe2bJli7xSSe3b1ylwmEwmUVvrFBU76qijAECU1YULF2LMmDEAXIufOV/tBM/7sSvmfkXJOdxU5eWlt+f33nsPANDY2AjAMY/36+dU+xs+3Kk+t8cee3T43V7f2xNwsBg8eLCcO839XJzk5+fLwMgFKQAZgLkQ5f+3bt2KzZs3p/xN8Td81pxgV65cKRPC+PHjAQB77rkn2tudAOGrr74agOMmAgCLFy/GAQccAAB4/PHHATgT6yWXODne2We4MI3H46irc0qc83Xo0JSKu4rSa9i4cSOKi4sBAP37O0X14vG4jH8XXeTksb/77rsBOJu1NWvWAHDcAng8XWu++eYbAJB5JRwOo6ysDACwbds2AEBNTU1nF6k9CueOmTNnorTUyc/PhSX7fH5+Ps47zymwtnDhQgDAunXrRDipqalJ+c4rr7wSr7/u1CPowuK0Q9TcryiKoiiKovgOVVKVXkM8HhdFiSxYsAD/939OBcTt253cyjRrjhgxQgJJuMstKSnB/vvvDwC44AKnJDbVU7+oqIBroi8qKpJ/U0U27wGd9m2zP+AqqDwmFAqJyvy/TkeKPACsWLECAPDSSy8BAG688cbcnJiF3d7ffvttCdD4/PPPAQD77ruvtPM999wTgBsQtXr1agkYOeywwwAAv/rVr8Rcye+PRqMAnD7G/sPgkMGDB8txtrKrKH6E431LS4uopmzjgUBAAoXYX5544gkAwJo1a+SzZPTo0SgpKQHgtn8qqolEQsZZKrbt7e3yHVRZ/cj8+U712nA4LMG59jyyZcsWHHjggQBck348HhfLHdVYfn7r1q0ZP09VUhVFURRFURTfoUqq0msw1ZvZs2cDcFJl0Fdzr732AuA6wdfV1YlfEXeI27dvx8svvwwAeO211wC46UemTp2a7UvoNFS/iouLxZeK7/FaotGo7Hh5b2KxmCivbW1tKd8ZDAZl19/b2ZkSujO8As3IunXrxLeTCiV91zryac4ktmJJn7jFixdj7NixAIC///3vAICqqirxO+Xxxx9/PADn/NnOGUwViUTk+xhUxd+LRqPSzqg8ffXVV9h7772zcp29kRtuuEGsMFSZVGH2F83NzQCcYB+OFRz7+vbtK32eiiqfW1VVVdozjEaj4stvjzvmsfTnLCoqkv7lZyX1v//9LwDnvthpDTkGjBw5ElOmTAHgxkQUFxdLe6eVjkpqbW0tvvzySwDOvcwEqqQqiqIoiqIovsN3SipX6ObOtKu704cffhiAG8134YUXAnB2OpmKOFN6liVLlgBwohAZmUjV64MPPgDgRB4yCpM76wEDBki0PPniiy8AOEnR/RKVaWYoYOS2rZCa6ePstFP8LODu8AsKCsSHqLeTKf9h83vmzJkDAJg2bZqME7x3LILw4YcfZuR3d4Y95jEKv3///pI26t577wUAzJs3D4ceeigAN+qYCulRRx2F559/HgBwxRVXpH032whV04KCAlFRyOrVq0VJ3d2VQi+F/s033wQA3H///QAcP0amJCK727zSVUuFfTwj5J944gncc889WTjDzhEMBqUPc/w0rU3EVFa59uDngsGgHG+vTwKBgPyNY2s8HvdVfMOO4DwZCARkTuG1cM7p27evrKP4XiQSER/dr7/+GoC71orFYjI3Z0pJ9d0ilYOg12DIm8S/eTWEtWvX4rHHHgPgDhxnnHEGAGfgVrPM7gEnzXA4LE7qNOcwv11ZWZksLmjib2xslEXtkCFDAED+T8d3P8B2WlRUJAtPtndz0OXgwmsPBoNigjHfA5wFLBcjisv5558PwE37NXDgQAnC4+u5557bMyf3LU8//TQA4Lvf/W6aqb6+vl4CoZhKiumjSktLcdNNNwGAbMAaGhqkzdvuBKWlpbJwJW1tbXJv6FKzu2LPKXPmzMFDDz0EwO2Tjz76qPzdnk/8lMaus3gtSPlvji9sT3vttZfn9dnvjRw5EgDwyiuv4Mc//jEAN6dmLjDHPq8KfXYOT/MecPwkyWRSjuNcw/tSUVEhAgjvQVFRkbQLP7N69WoAznlzXrDz5odCIbl2c4HO4ykM0dyfSCTw9ttvA8jcmLl7bf8URVEURVGU3QJfKKlUSEOhkFSHeOuttwAAkydPluPsHY4XkydPFlWAwQ5UHBKJhCqovRhTpWAaqbfeekuqY9DcwmCpCRMm4OijjwYASVNVVFQkSitfqQ6xooYf4E4/FAqlOambShf7jqnomKYoACnpQnrDDn9XsdWAjpStu+66S8YcBjmMGDFCzPo0kTOQKheYqdaoYDEt1B577IENGzYAcNU7nj+AtKCneDwuibnN4Cj+m2Ml29TLL78srgNUi8rKyuQ3dicllWqbbfoF3BRkr7zyilTc+cMf/pB2nD2f9DYVFUi3uJjXdMIJJwBwk7aXl5dLm2S99srKSgnco1o6adIkAMAjjzyC9evXp/wtm7Bdm0FSnBc4TwwbNkz+bo+VeXl5aeOHeRznCL7G43Fs2rQJgGvyHjhwoARk+dlyy2caDAZT3MQApBSJ4djD8aBv376ioNpzZiKRkMCpTKFKqqIoiqIoiuI7elRJ9UpO/pvf/AaA69Q7a9Ys8Wk57rjjALh+VyZMI7RhwwZJWv373/8+S2fec5jBX1TbuHNpaWkRnxn+bc2aNeK/edBBBwFwnZ2ZPqU3E4lEJFkzd4Ms2TZgwABUV1cDcBP3z5gxQ3xRGZzkl2ApEyqkgUBAnjdVMjOxOvsQn7fpf8XP0WcqGAz2SqWnq5gBDzvijTfeAOD4HLJf8HNPPvkkrrvuOgC5VVCJed4MmKKaV1lZKYF+VDgmTZqEtWvXAoCoVkxS3tDQIN9n+5oCbpui2jpmzBhRaqmyTpw4EYsXLwbg+MT2Jmx/S9Ma05GCyvrtZ555Jk477bRd+k2/w3ZBpSwYDEp6IiatZ/uIRqNSTILjytq1a6V9zJ07F4Bbgre2thZ/+9vfcnEZAFwfcs4FZkDb8uXLATgWNPrMsv3TSrWjZ2YHUzFAceXKlaIsMyCXllv+FuDPVFRMp1dTUyNxO7wPM2bMAOAUv7GV5UAgIOsMrrU4lzY1NWHVqlUZPU9fLFLZkL755hsZXDkAr127Fvfddx8A4J///CcAt4HcfPPNErlqSu50dCec8E16WzQm71V7e7sMKq+++ioASPRkVVWVyPQ090UiEVmUcjFWW1sLwOlgtgO5nzHdNTiBxuNx/PznPwfgXgNNEdXV1TLhsl1NmTIFY8aMAeBWGfEy7/Q0fMZNTU0ycPL6OJAEg0GZWPi8CwsL5Ti74tT/Cvbi1FyYfPTRRwCAH/3oRwCAsWPHSlvi337xi1/IZpn0lNmOiwROfLW1tTIp/uc//wHgjIfceHFDz7yOpaWlaVWlAPd6+L2ffPIJAEhfAtzo3IMPPlj6lJ/Nl17Yiw7z/4xCTiaTePHFFwFAopZpvuYr4C5oCgsLUxa99vf3lsUp4RxgmnwvueQSAM5G3zwmHo/L/MtA09LSUhFCxo0bB8C9t1u3bpX3cgHHQ875xcXF2LhxIwBX5AoGg+IS5rVRsUUgEz5vLj4rKipkccqKTPvvv7+sX9hm/LRI5XjHtUIymcTJJ58MwK1iRwKBgBxH035ra6uMJd/5zncAuMLXypUrMz7f+H9loiiKoiiKovzP0aMSi63e7bHHHrj77rtT3qutrZWVP3cxrJwSDoelVixVoxNPPBGjRo1K+Q5+zmvX5GdMBYivpsmOcj3NLm1tbWmOzEcffbSoiFRBFixYIH/vDQqqF1Q8tm3bJpV3uKsjBQUFcs10idhvv/3w5JNPAgCWLl0KALjllltycMZdgymCNm7cKP+2FZr+/fuLUszAlsMOO0yumW2FO9tEIuFp8t3dycvLw8cffwwAOOaYYwA4QXWAow7QpH7kkUcCcPNhmlA5bG1tFVWRbiXZgEoolRgGM33xxRfiukJF66abbhLFj/2f5tqDDjooRXnnd/KzNNNRNTWDtm699VYATj+hiZepqHprBaply5bh2WefBeBeQ1VVlYylBx98MADg008/BYCUnLFMs2PS21RTL+w54JlnnpH0RJxrad415xevPKJsT7TW5TpQ0w4WDQQC0mbZR2KxmMyZnV0T2GmYeO2DBg2SgClaOBobG6XP2TmH/QCtJqZlhe2Y8wlpbW0VVZpqcCAQkPUWFenDDz8cAPDUU0/Je7wfdBXpLr1zhaIoiqIoiqLs1vjOWc12Oq+srEyr7sFjTj755JRUCQDw29/+Nu07uVsKh8Oiyo4YMSILZ++NlyM93zN3mmYaDPt4+rtMnz4df/7znwGkKz9egTHBYFCUHwZVePna+IXOJsPmjr6srEx2+fPnzwfg1i3fvHmzqCVU11esWCGpN+iPZ/rQ+MXnjkqxmUia8P6Ew2GceOKJAIB//etfABwFw06pxd18Xl6epxrkRzIZfLJs2TJMnDgRgFs5ikriO++8I751TFNmwnt35513AnD8vxkUctlll+3yue0IjlN8NVNQ2T5+o0aNEqsAg0N4fZFIJM2SEgwGRVHne+xDZrunX+LFF18sbYljCV/5O37F7s+vvvqqKEhMobR161YJIrXHxpqaGkl359UW6QNJS80DDzwgQTnXX399Ji8lK5iBuAyWueCCC2RuoWLG+xKJREQ55GskEpE+RLWN/YYKda6gkss5f9OmTTLe8z223c6STCal/fA+8L5Eo1H5G9XZ9evXY/To0QC842F6GirL7BulpaWiMt92220A3PmnsLBQ7hf9cIuLi8W699prrwFwAypLSkqkfzEIUZVURVEURVEUZbfDV0qqGSlpp9YB0tWtRYsWiUpAlfD555/HNddcA8BRUAA3invevHk455xzALjKSC4wI0Ht9DgdRcIlEgnJXkAFLD8/X2p28x499dRTABxFgLtb7vCHDRsmOxuqHvx/Q0NDSroMv9CZZOxmdD9TUHF3R/+6wYMHiwLFV7P0aUVFBYDcquqdxa4VDaT7mDY0NEhqIkaBv/TSS2J5sNOr+K2mNPuCVzlGuwQskFrC0C5w4JWlYt68eQCAc845R4o6sN/R53PDhg2itJCVK1fi9ttvB+D6LbPE36xZs7qckqg78Hf5LNnevZLpX3LJJZKyj6oHfW4B95rZB8yCD2wjY8eO3eG5JBIJ+V4eT38z2//fb9hzxo033uh5HBO902LFcXHy5MmYNWsWADexfX19vRSboa8zlbNx48Z5pkjsSTqyTpn9hb7O+++/v4yTjIznnFFaWpqW2WDAgAEyXrF90MLJ+TlXcA4wy7ny3DhWtLa2pqWc4rWYY4sJ3+M8TMXWPJbz0KpVq2RO4XzsJ5h+j8poeXm5pO7iWMhxJ5FIyP2jr2leXp7447McM78rGAzKXMS1y/e+971dOl9fLVK9OpI5oJLPPvsMgGNKoBmK1Sy2b98uKYaYr4s3PBQK4fLLL8/OyXeAORnbZoPly5djzZo1ANwGQtNRKBSSTsFGv99++6WklwGcqjmAk9+RnZPfHw6H5ftovmOH/OCDDyT1RKbprrm2s8dz8IzFYjKB0mTHAfXhhx+Whd2FF14IABgyZEiaEzwHVD/Bwa29vT2l0gfgTizt7e3SnhjYAqQuxAF3cRsOh301aHq5tRBOdlxY2Zj3AEjd7DH4km4wxx9/vDxrDqD83FlnnSX3hxvY9957DxdffDEAt8oQB+GlS5fKuWXT1P3ggw8CAH79618DcM3xZkok0tjYKJMmr4Vm/z333FMWXPYxJgyU8QqIuuqqq/Dwww8DcO8f76dfF6n2nLEz9x1Ougyso+vQT37yEwm0ZD7dBQsWSMDuqaeeCsANHIlGo75L+bazMZUbXW6AqqqqZNHCsdVc4NGkT1N+3759xTzMV3MjnUts94Lhw4enbNhIV9MO2uISX2n2B9w5pq2tzdduVfbG4ayzzpJNF+HCtLm5WcZgXhPnI8B1s/z3v/8NwHGBYjq38ePHZ+R81dyvKIqiKIqi+I6cb/k6GxhjYh/P9BaxWExUMO787r33XlGSmD6DBAIBSXqdC+xiBQAkXRJ37scdd5yoWzS5ccdSXl4uZqjf/e53ABz1lEUNqKzR/Nje3i5qCdXZs88+G8888wwA1wzOeuTTp0/PmpLa1WfckfLqFcxkpiWjssXk5qx+UlhYKCaYiy66CICjGlD14I6Qqoj9Gz0J22ksFpPr4+7WTM7OnSytBW1tbaJ+sB3xnra3t4vS4QdMk74dHMbde1NTk1yHaaKzCxxQ6bz00ksl/RKTd7e2tkpQA1VEKqlLliyR45jg/8Ybb0xLyUIls3///jlRSajacZxgIQ6vIIRbbrlFrt+LjhLx837QXWrZsmXy24T9xfwu9is/YY4hO+rHS5cuFRMlx0izdjnVRCrEZ5xxhgSF3HzzzQAc1wCOpZyLaN065phjdqj+9xSJREKUQPYhWpsGDhwo94pm2RUrVkj/oArP5x6NRuU9Wm/effddmbtY+ZFWPqY78hOxWKzb4zzXG17jKMcMP6uogLuW4CvgWgS4FuG4YKbcY7s2iz5QVaebzMyZM1O+NxOokqooiqIoiqL4jl1WUjtSRhOJhPhscPXdncANW2Whf1RbW5vs3LiznTFjhuzq6uvrAbh+I5FIJOvJ65PJpKeCCjgqz0knnQTATYUzc+ZMSfdBX1oTllyjctjc3Cz+t3TaZ4DIsmXLRDGkj4npf/boo48CcMs/jh49WsqZmT6NPUFH7cLc9S5atAiAqwqNHDkSb775JgA3tQafd15enijsbBPRaFQUFCru1dXVACDBNX6A/o7Dhw8XHyK2Z96P4cOHSxvjjrasrExUVapr3NkXFxf7SkklXn2S6v/1118v/th89oDrszpz5kwAbnDk0KFDZUygAlBfXy/jDz/3xRdfAHD8VVlWmEpBdXW1KE+8r1SgiouL0wpmZBpaQwBXkWL/j0ajaUpdNBqVNsL+TqVsw4YN8jeqYwUFBWmlUtn/6+rq0pRUwH1GDJjiM6mrq9vlFDOZwivFHws1UF0vLCwU32PeWy/4DBKJhPjWUWVauHBhWhJ4tqHDDz9cVO9sYvvcdqQiBwKBtFKfDPQaO3asBHrxHvXr1y/Fjx1ASv+h5YnFU9atW4eFCxcCcOcWKo777LOPWCFyWRrU9Dn1KmHL66E1zbw/XuORXZKadGTB6E1wDmT74LwJuKoxx85YLJYWxMtiMiaZSiOY0UWqnfszFAqlSMOdgZ/ld4VCITHV0SzH/99www0SrclF2UMPPZQSaQa4UWm5GDw6qt388ccfi3mIQVJ1dXXygFkv3OvhMs/j008/LVHpNLnxOr/66itZ1JrQ8Z/5HTnoJhIJWbD19CKVxONxmUzttvPZZ59JLksuFD799FPpMJs2bQLgLj5bWlrSgluqqqqkHjUXfXa9Yj8xf/58WUhcffXVANysFb/85S9lQc5J8+uvv8Ydd9wBABIkyHY/e/ZsXy3EzYBCu88wuv7kk0+WNjp79mwAzkTIBSuDI9kHGhoaZODk4mno0KFizmYQ4R//+EcATtAAg4Y4YZnBmnblp6FDh2bdJWTJkiXyXGla5T3wCnoKBoPSpnneZtUxjoe8L+FwWPqWHfn/5ZdfyjWb4yUDUng8z6OhoaFHF6lei5Fly5ZJ8AY39Ny4DBo0CH/5y18AAM899xwAJ6CUgXJ0FaIAsHDhQqm8xcWtFxyTw+Fw1jJodMadwYsNGzZg2rRpACA5thksNWTIEHFpMfPGcizlRo/POBKJYO3atQDcBVp+fr4sZBi8yr60cuVKca/gs8gFHT2DQCAg/dnOImS2J3OxaufPNfPH7izIszdhb74ikUha1az8/HwZX3g8N8JmxiA72Ky7qLlfURRFURRF8R27rKSauw3uXsx0FXSovfLKKwE4gUOsj2yrQECqAgA4O3s6dFMFevrpp9N+2zRjUnnid/H/2Uy/w93ohx9+KOYTqnh8HThwYFqt9VGjRomZmoovVULT9MAcdnPnzpX7xnQ0kyZNAuCYf+3giHA4LOoAgwHMCly5wE734ZX7kgSDwbSd19y5cwE4SimvnYq0WX+Y98E0Pdjm2uHDh0sKM6oEvO9+ZMuWLXJ+rCpF0+/YsWPTzE9btmwR8xrVQ7anuXPnSkqjXFgVdoaXWY3jBJ/b8OHDxcRMdXXYsGFy3UyZxDbuRXNzswQgsW47rQnLly8XVZHf2adPH1Ga2DepZOaCysrKtIAVnr+XKvHJJ5/gzDPPTHmP46fX8V4V5zhu9OnTJ6VPEVbXohJHvI7NJV6K2XPPPYcpU6YA8K54RJeo6dOnAwDuuOMOqSr12GOPAXDHkDlz5qQFiXm5uNHlqF+/fvJdmcb8TebF/fDDDwG4rhylpaViemcFqWHDhskzp6JM5b26ujqtjbS0tEj7oWWOnx8yZIgohnS1aW9vl/5CdzbOtUuWLMm6i11n4TXE4/G0VJAkLy+vw/O1VdP8/HxRjXuLkuplqWWAtu3KZF6TnVsWcMdw3oNsjAf+aD2KoiiKoiiKYtBtJZU777a2Nll9czfH3VRxcbE4WXP1/dFHH4mSavs/AK4CQAXjiCOOwM9+9jMArh+ZF6yXDLireVtlymb6KSpTRUVFeOeddwC494O/e+qpp4oqxooyNTU14mPLpPxMhUNnZiB1p0IFlIFWTNy9dOlSUdm4wykoKJBnQLWZ51VfX5+T2sq26tCRj0oymRQfwbfffjvl8/X19XL/zGAgtj+moKJaPX78eEn6T2WksbFR1DeqdWxzbW1tXfahzjbTpk3DtddeC8BNX8a2cMopp6Qdf95554layFRlvKbx48f7rhoOuemmmwBA+g7VqFWrVsmz5rmHQiHpA3y+tDTwHplUVFTglVdeAeBaH9j/SkpKpK8w6CMcDksfoRWEylMuVKHGxkZpk1RuvCpNMbinqKhI2jT7v5eSymvyKpBiVsDzqm3O72ewke2n5gd4TqNGjeqwH/NZsrY4AOlj9GdnOxo0aFCadcpLveXzoS9nNrn00kvx17/+FYA7hnEujUajMt4zBePIkSPl2dPKwvMcMmSIPG+qae3t7TJ38ns5xkYiEQmKYvvLz88XX28mdTf9lP1S3IDtI5FIdMpP0ksx5Od4P4GetyZ0FS8llTEpbAtmqj67MqAZEM92we9sbGzMuI+6KqmKoiiKoiiK7+j2Foc7CtOHgTsm+lOtX78+zZ/niiuuwOTJk3f4vdzpMb3F2Wef3aGCSugDY/oD2T4U2YxC5c7dTI7Pa+Hr3nvvLarpscceC8CJHuYOz/QjBZxIdPqE8D6ff/75HaoEjKTkTicUCslOj5/jrmfz5s2eaa8yDa+Pu1H+v6GhQVRPKud1dXWiDlA5e/fddwE4UadMr8QUQps3b5ZrpprNe0W/LcBtkxUVFWkJ3enz2NTU5Dsltb6+XiJmmRGC12cmWSfbt28XdZz3iG0hU2XqMs17770nvnVsj+wzDQ0Nkl6OaVGSyaQoeoyqZnaP0aNHi6/h1KlTATgZP+iLR99BKkRmeh72i7322ktSs9k17nNRVnb58uUp0dSAW6zBhGPDiBEj5LzsvmaqWHbkv4kZrWumwLLh2MQI71zXZiemGkT1jud93nnnpanBHaXDmTRpkqirDzzwAADXBxhAWvvw+g4qsNkslctrevHFF8VXlBH5HAsqKiqkv9AyUF1dLfOOrfrFYjF5pmZWHlqs2OdozaCfO+BdWpRjNX1UgfQyzT2FmY2A/Z/XbPqrmiopkGo9sVN5Af4sq90Vtm/fnhaXQWtdQUFBWgGIeDyeNr7wPtbU1KQ8+0zQ7UXq888/D8BxpKcpiLnXOGAmk0m5MDaKQw45xDPFCeB0Kpq6mW6KuT2B9EArLwf2oqIi6ZBsSGat2Vxi5q7MFV1pILkw/a5du1bSBNHExJRP33zzjQy2XGyUlZVJDsbXX38dgFsRqry8XAKmOBmUl5eLiZMuH/z/1q1bxdWC6abC4bAMKpyY2D42btzou0o64XBYFpcMDCM0z9nHc6P0wx/+EICbsi0XG5KuwI3J5ZdfLhOZmcuTr3w+PKa5uVnaBJ8X/7Zt2zYJtLzqqqsAOLlQZ82aBcANHuQg29TUJAtXM2WVnT7l008/Tfl8NvFKM+UVlMFzMze6HGvMPIfETDvFcZm/ZS7GvRaxhJVpOOF7LZ4zTTKZTHseXkEfp59+ury3ZMkSAG4+aq+F5Z133gnAmXxvuOEGAKmLU+KVZ9OrehfgjGnZgm5vsVhMNt08J46LbW1t8my4+Y5Go7LI5OaC59/c3Cz3lvNrMpmUtsJ+wjHzwAMPxGGHHQbAvS9e95a/V1FRIS48PT3+mGmnbLcdcwNnX49XWirT/M++ZPep3kJra6uMsRRA+H9z7cT70KdPnxTTv/k3zr2ZRM39iqIoiqIoiu/otpJKxa68vDwl8THgKiQjRozwlJG5u2VNZAZ4lJWV4ayzzgIA3H///fIZrta9Aq1samtr0wJiGDCUzcApxZv3339f1AYmw2ag14YNG8SsSgUjPz9fTEbc2bOttbS0iKJE097mzZtFvaCrB1W2zz//XI7je4FAQL7DVk2qq6s9q+30JP3795fk/WaAF+Bek0lZWZko1Qxg5P3PpsrTHXg9EyZMkHGCLgpUviORiJw320N5ebkcT6sMzf/r1q3DLbfcAsANhpk9e7aopFTgqdgmEglReKimNDU1iVJFVYrtKBdWEaZI2hlUhurr69OS7NtptYBU1Y/XahfOiMfjHSqpHLNzyc6S1zNg8vvf/z6AVHWLig9TaD3++OO47777AEBS0U2ZMqVT/b6jBPG8z9msRjZhwgQAzrhmF3eg21R5ebmoWqZJm22F7Z9zqumeQPUzGAympWbimNPU1CTzOy1xkUhE2hF/m8+rsLCwQ/eRXMBzMosN2Up4R2kRd1acgZ+l2tzblNRt27bJ8+O12i5x5nvJZDLFndCE7dD8rl1FlVRFURRFURTFd3RbSWUwBxP6mlB9qKmpERWE6ZRqa2tlF8hV93XXXQfA8bnxCm7aUdoXr5X666+/Lrs+OvVzF0nfWSX7MJipsLBQnvcjjzwCwN2RtbS0yC6U6qZZ1pIBQFTLVq1aJf6t9H2JxWLyWaonbC99+vRJ28UzIAdIV5G8/NF6GlMZYbvmjt3LslBcXCx/5z3iNfdUkMuOoIpz+umnix8yU0pxbGhsbJT0NhxXtm3bJtfEtsRnPm3atJT0VUBqkQuzJjvgqCr0Z6XfaSgUEj+6I488EoDr89rW1tajgSBmInKOa3V1dXK/mAqJfchMN+XlR8nj2Id2FDi4Ix/MbMLnvXr1arnnpkUEcJ4jFT2mrkskEmI9u+222wC4SvsLL7wgQXannXYaADflX1ew5yQqqNkMrGMg4NSpU/HGG28AAB588EEAbkAg74FJKBRKU7xMP9vOBP6YwWP0f6XPq+mjaKcm2rRpk6Sc7Clsa25RUZGcJzHbtV0qtaO0c4FAQI63v7O34KWkmvE/vH7T+sLj7JRcZvvLlJKalQRm7KgHHHCA5CWkqSLb9HSHUBwYrW1W6mHD5+Kpvr5eJg8GVdXX18tChQE/ZjYCLszMQZd/t2uwMzAKcDvRgAEDZALj53hcripwdYWSkpKUqFsgNSeijTlhkK64y+QSns/IkSMlcpnPZty4cQCcZ8IIZjPQkotN06zI99kOeHyfPn3SolDZFvv16yftgBkA+vXrJ3kD+dvcMPkp+wMX+c3NzWmLbxIIBNIWmOZ77Bdc8La3t3suRHe0ODUXzZmGLh8vvfSSLFJ5nRwbCgsLZRHCBVtlZaVEoTMwkwF2S5cuxT333AMAGa0MRTFm3rx5OPfcczP2vTuCrjx8Ja2trRI0RqFg/fr10r9s83ZbW5sEPHOTXlJSInM4Ny/sP4FAQMYdfsfq1avl3/wc3WWKiookk01PYS/CvRadZiCUnbXArELl5QpjL+z8jn1969atS8tyQNrb21PyKwNOW+B79v3wCubdVdTcryiKoiiKovgOf5SCUHY7qCasXr1aTFM00XIn3tbWJqond2R5eXmirhLu4ktLS+Xf3B2bFXLsFGWDBg0ShY07v5KSElHruGvk///xj3/gqKOOAuCP2vZAqmmK6lFHFBcXp6UF4SvvhV8w8/bajvpm2in7WZiBTbZq3LdvXzmeO//8/HwxE1MZYhtLJBIpdc8BxzzINspck2yf2arL3llM1ZLXUlBQIEoo7xvvrZlSiu+ZldXMgCke393zyTTM93n77bdn7Te6i93ubr311h46k1QKCwslBRtfs80JJ5yQk9/pLhw/TfXYDJwE3HYci8U8lVRit/dEItGhytob2LZtW1r+V9Ocb19zXl6ejNe2SwRzeZvfsauokqooiqIoiqL4DlVSlawyatQoSYBu+pYCTrohpjPhDiwcDqc5+XPXVlhYKGog1c9hw4alJH4HUmtQU20y603T95QqkqnU+U1tLC0tFWWYu3+7XrKJV81s7mj9dm3EDGikgknVuKmpSXwOzcIgdgoU0z+X18n7U1RUlJY6is++vb09LS3P/Pnz03zxeM+zmWIoE/A6qaiaSjzbUTAYTFN/qJZEo1FRqUk2/U4VJduw/Ztt3p5jiNeYaiqCXhWneptPqs2cOXNkjKXvth0HYb6XTCbTUo1xXNzRfd0VVElVFEVRFEVRfIcqqUpWMev8csfJSGm+ZuM3ia0YRSIRUVXNnSHPLxe12bsKVUXeP6qGXqljotGovM+dPV/9UkO7I2xV3MzQkCu6k5KoJwkGg5Iyi9lUmCbIrGFvljDl+1RNqcDyc4qyu2AXNzBLfdpzQCKREJXU9LfkPEIrjVke1S4r63dstfiaa66RojpM99fZOAiOG7yPixYtyuCZOugiVckqPWEm9PpNmiH69+/vy4VoR3Axb9e1t82ygJPOiSYbDsp0jcjWpkDpWS677DI8++yzANxFphlUxU0NF6YNDQ0pKYIAN6jx0EMPlVyrirI7YC9OTbcnuv3wmEQiIQsuU1yxxQ5zQWrnGvY7tkn+lFNOwSmnnAIAUt1wwYIFABz3OAadMqgyEAjIfePGl2MGq4lmEjX3K4qiKIqiKL4jz8tRWFEURVEURVF6ElVSFUVRFEVRFN+hi1RFURRFURTFd+giVVEURVEURfEdukhVFEVRFEVRfIcuUhVFURRFURTfoYtURVEURVEUxXf8P5RmDjRcZBKXAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 864x345.6 with 40 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_rows = 4\n",
    "n_cols = 10\n",
    "plt.figure(figsize=(n_cols * 1.2, n_rows * 1.2))\n",
    "for row in range(n_rows):\n",
    "    for col in range(n_cols):\n",
    "        index = n_cols * row + col\n",
    "        plt.subplot(n_rows, n_cols, index + 1)\n",
    "        plt.imshow(X_train[index], cmap=\"binary\", interpolation=\"nearest\")\n",
    "        plt.axis('off')\n",
    "        plt.title(class_names[y_train[index]], fontsize=12)\n",
    "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(784, activation=\"relu\"),\n",
    "    keras.layers.Dense(28, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 784)               615440    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 28)                21980     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                290       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 637,710\n",
      "Trainable params: 637,710\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dense_1'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[2].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, biases = model.layers[2].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02051247,  0.03019074,  0.03162231, ...,  0.06511657,\n",
       "        -0.075742  ,  0.05334122],\n",
       "       [-0.07449015, -0.05751086,  0.06797951, ...,  0.02668607,\n",
       "         0.08034188, -0.03590588],\n",
       "       [-0.01390947,  0.01968379,  0.06596687, ..., -0.0614449 ,\n",
       "        -0.02134842, -0.06096332],\n",
       "       ...,\n",
       "       [-0.04213622,  0.0690424 , -0.07531954, ..., -0.01384425,\n",
       "         0.07878809, -0.04980563],\n",
       "       [ 0.01280473, -0.00479205, -0.02628516, ..., -0.05621847,\n",
       "        -0.03051617,  0.03182768],\n",
       "       [ 0.05597045,  0.01320585, -0.0208343 , ..., -0.06963408,\n",
       "         0.0026973 ,  0.0667529 ]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 28)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.7091 - accuracy: 0.7678 - val_loss: 0.5047 - val_accuracy: 0.8278\n",
      "Epoch 2/30\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.4821 - accuracy: 0.8318 - val_loss: 0.4347 - val_accuracy: 0.8558\n",
      "Epoch 3/30\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.4389 - accuracy: 0.8460 - val_loss: 0.4041 - val_accuracy: 0.8616\n",
      "Epoch 4/30\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.4130 - accuracy: 0.8552 - val_loss: 0.4136 - val_accuracy: 0.8544\n",
      "Epoch 5/30\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.3933 - accuracy: 0.8613 - val_loss: 0.3823 - val_accuracy: 0.8700\n",
      "Epoch 6/30\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.3761 - accuracy: 0.8670 - val_loss: 0.3666 - val_accuracy: 0.8736\n",
      "Epoch 7/30\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.3614 - accuracy: 0.8717 - val_loss: 0.3678 - val_accuracy: 0.8724\n",
      "Epoch 8/30\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.3490 - accuracy: 0.8765 - val_loss: 0.3616 - val_accuracy: 0.8726\n",
      "Epoch 9/30\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.3370 - accuracy: 0.8805 - val_loss: 0.3422 - val_accuracy: 0.8748\n",
      "Epoch 10/30\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.3276 - accuracy: 0.8818 - val_loss: 0.3633 - val_accuracy: 0.8660\n",
      "Epoch 11/30\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.3177 - accuracy: 0.8865 - val_loss: 0.3352 - val_accuracy: 0.8788\n",
      "Epoch 12/30\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.3088 - accuracy: 0.8887 - val_loss: 0.3532 - val_accuracy: 0.8756\n",
      "Epoch 13/30\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.3008 - accuracy: 0.8931 - val_loss: 0.3315 - val_accuracy: 0.8822\n",
      "Epoch 14/30\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.2923 - accuracy: 0.8951 - val_loss: 0.3239 - val_accuracy: 0.8826\n",
      "Epoch 15/30\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.2862 - accuracy: 0.8972 - val_loss: 0.3205 - val_accuracy: 0.8824\n",
      "Epoch 16/30\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.2789 - accuracy: 0.8990 - val_loss: 0.3073 - val_accuracy: 0.8854\n",
      "Epoch 17/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2726 - accuracy: 0.9020 - val_loss: 0.3058 - val_accuracy: 0.8870\n",
      "Epoch 18/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2663 - accuracy: 0.9039 - val_loss: 0.3228 - val_accuracy: 0.8822\n",
      "Epoch 19/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2607 - accuracy: 0.9052 - val_loss: 0.3095 - val_accuracy: 0.8870\n",
      "Epoch 20/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2552 - accuracy: 0.9075 - val_loss: 0.3000 - val_accuracy: 0.8890\n",
      "Epoch 21/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2488 - accuracy: 0.9112 - val_loss: 0.3188 - val_accuracy: 0.8860\n",
      "Epoch 22/30\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.2440 - accuracy: 0.9106 - val_loss: 0.2985 - val_accuracy: 0.8908\n",
      "Epoch 23/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2406 - accuracy: 0.9137 - val_loss: 0.3334 - val_accuracy: 0.8806\n",
      "Epoch 24/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2350 - accuracy: 0.9149 - val_loss: 0.2958 - val_accuracy: 0.8928\n",
      "Epoch 25/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2294 - accuracy: 0.9186 - val_loss: 0.2864 - val_accuracy: 0.8970\n",
      "Epoch 26/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2248 - accuracy: 0.9194 - val_loss: 0.2989 - val_accuracy: 0.8930\n",
      "Epoch 27/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2198 - accuracy: 0.9207 - val_loss: 0.2999 - val_accuracy: 0.8916\n",
      "Epoch 28/30\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.2154 - accuracy: 0.9229 - val_loss: 0.2871 - val_accuracy: 0.8940\n",
      "Epoch 29/30\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.2118 - accuracy: 0.9242 - val_loss: 0.3092 - val_accuracy: 0.8902\n",
      "Epoch 30/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2073 - accuracy: 0.9253 - val_loss: 0.2911 - val_accuracy: 0.8942\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3217 - accuracy: 0.8872\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.32174962759017944, 0.8871999979019165]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = X_test[:3]\n",
    "y_proba = model.predict(X_new)\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 975us/step - loss: 0.8715 - val_loss: 0.7233\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 810us/step - loss: 0.7953 - val_loss: 0.5874\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 776us/step - loss: 0.5190 - val_loss: 0.5009\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 779us/step - loss: 0.4770 - val_loss: 0.4708\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 782us/step - loss: 0.4612 - val_loss: 0.4577\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 790us/step - loss: 0.4454 - val_loss: 0.4554\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 776us/step - loss: 0.4350 - val_loss: 0.4399\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 768us/step - loss: 0.4291 - val_loss: 0.4368\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 782us/step - loss: 0.4253 - val_loss: 0.4307\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 776us/step - loss: 0.4153 - val_loss: 0.4239\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 782us/step - loss: 0.4095 - val_loss: 0.4215\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 859us/step - loss: 0.4423 - val_loss: 0.4263\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 793us/step - loss: 0.4051 - val_loss: 0.4281\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 793us/step - loss: 0.4002 - val_loss: 0.4139\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 787us/step - loss: 0.3949 - val_loss: 0.4064\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 774us/step - loss: 0.3941 - val_loss: 0.4067\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 801us/step - loss: 0.4064 - val_loss: 0.4391\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 775us/step - loss: 0.3991 - val_loss: 0.3987\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 776us/step - loss: 0.3840 - val_loss: 0.3969\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 768us/step - loss: 0.3808 - val_loss: 0.3944\n",
      "162/162 [==============================] - 0s 578us/step - loss: 0.3661\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "X_new = X_test[:3]\n",
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pd.DataFrame(history.history))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.6302361],\n",
       "       [2.5070558],\n",
       "       [1.655421 ]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.Concatenate()([input_, hidden2])\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "model = keras.Model(inputs=[input_], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 8)]          0           []                               \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 30)           270         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 30)           930         ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 38)           0           ['input_1[0][0]',                \n",
      "                                                                  'dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 1)            39          ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,239\n",
      "Trainable params: 1,239\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
    "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name=\"output\")(concat)\n",
    "model = keras.Model(inputs=[input_A, input_B], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 2.0228 - val_loss: 0.8982\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 892us/step - loss: 0.7812 - val_loss: 0.7377\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 892us/step - loss: 0.6967 - val_loss: 0.6822\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 887us/step - loss: 0.6553 - val_loss: 0.6468\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 879us/step - loss: 0.6271 - val_loss: 0.6201\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 887us/step - loss: 0.6063 - val_loss: 0.6008\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 884us/step - loss: 0.5896 - val_loss: 0.5824\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 879us/step - loss: 0.5768 - val_loss: 0.5700\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 894us/step - loss: 0.5649 - val_loss: 0.5580\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 881us/step - loss: 0.5557 - val_loss: 0.5479\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 876us/step - loss: 0.5473 - val_loss: 0.5389\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 890us/step - loss: 0.5400 - val_loss: 0.5332\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 876us/step - loss: 0.5332 - val_loss: 0.5250\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 879us/step - loss: 0.5256 - val_loss: 0.5181\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 887us/step - loss: 0.5199 - val_loss: 0.5136\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 879us/step - loss: 0.5144 - val_loss: 0.5065\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 881us/step - loss: 0.5103 - val_loss: 0.5040\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 876us/step - loss: 0.5062 - val_loss: 0.4989\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 868us/step - loss: 0.5024 - val_loss: 0.4950\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 881us/step - loss: 0.4983 - val_loss: 0.4917\n",
      "162/162 [==============================] - 0s 646us/step - loss: 0.4721\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "\n",
    "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\n",
    "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
    "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]\n",
    "\n",
    "history = model.fit((X_train_A, X_train_B), y_train, epochs=20,\n",
    "                    validation_data=((X_valid_A, X_valid_B), y_valid))\n",
    "mse_test = model.evaluate((X_test_A, X_test_B), y_test)\n",
    "y_pred = model.predict((X_new_A, X_new_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
    "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name=\"main_output\")(concat)\n",
    "aux_output = keras.layers.Dense(1, name=\"aux_output\")(hidden2)\n",
    "model = keras.models.Model(inputs=[input_A, input_B], outputs=[output, aux_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=[\"mse\", \"mse\"], loss_weights=[0.9, 0.1], optimizer=\"sgd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 1.0624 - main_output_loss: 0.9198 - aux_output_loss: 2.3454 - val_loss: 0.8410 - val_main_output_loss: 0.7878 - val_aux_output_loss: 1.3201\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6766 - main_output_loss: 0.6123 - aux_output_loss: 1.2550 - val_loss: 0.5714 - val_main_output_loss: 0.5131 - val_aux_output_loss: 1.0962\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5574 - main_output_loss: 0.5051 - aux_output_loss: 1.0288 - val_loss: 0.5349 - val_main_output_loss: 0.4903 - val_aux_output_loss: 0.9368\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5205 - main_output_loss: 0.4793 - aux_output_loss: 0.8912 - val_loss: 0.5149 - val_main_output_loss: 0.4791 - val_aux_output_loss: 0.8367\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5025 - main_output_loss: 0.4692 - aux_output_loss: 0.8020 - val_loss: 0.4932 - val_main_output_loss: 0.4621 - val_aux_output_loss: 0.7734\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4864 - main_output_loss: 0.4571 - aux_output_loss: 0.7501 - val_loss: 0.4784 - val_main_output_loss: 0.4508 - val_aux_output_loss: 0.7271\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4745 - main_output_loss: 0.4485 - aux_output_loss: 0.7083 - val_loss: 0.5026 - val_main_output_loss: 0.4807 - val_aux_output_loss: 0.6998\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4652 - main_output_loss: 0.4400 - aux_output_loss: 0.6920 - val_loss: 0.4562 - val_main_output_loss: 0.4314 - val_aux_output_loss: 0.6793\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4531 - main_output_loss: 0.4294 - aux_output_loss: 0.6671 - val_loss: 0.4540 - val_main_output_loss: 0.4305 - val_aux_output_loss: 0.6655\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4442 - main_output_loss: 0.4213 - aux_output_loss: 0.6507 - val_loss: 0.4403 - val_main_output_loss: 0.4172 - val_aux_output_loss: 0.6480\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4375 - main_output_loss: 0.4155 - aux_output_loss: 0.6360 - val_loss: 0.4495 - val_main_output_loss: 0.4287 - val_aux_output_loss: 0.6364\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4321 - main_output_loss: 0.4109 - aux_output_loss: 0.6229 - val_loss: 0.4281 - val_main_output_loss: 0.4058 - val_aux_output_loss: 0.6287\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4229 - main_output_loss: 0.4018 - aux_output_loss: 0.6125 - val_loss: 0.4260 - val_main_output_loss: 0.4043 - val_aux_output_loss: 0.6206\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4160 - main_output_loss: 0.3955 - aux_output_loss: 0.6002 - val_loss: 0.4122 - val_main_output_loss: 0.3904 - val_aux_output_loss: 0.6080\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4102 - main_output_loss: 0.3898 - aux_output_loss: 0.5936 - val_loss: 0.4071 - val_main_output_loss: 0.3859 - val_aux_output_loss: 0.5981\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4040 - main_output_loss: 0.3843 - aux_output_loss: 0.5811 - val_loss: 0.4019 - val_main_output_loss: 0.3814 - val_aux_output_loss: 0.5864\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3998 - main_output_loss: 0.3804 - aux_output_loss: 0.5736 - val_loss: 0.3926 - val_main_output_loss: 0.3721 - val_aux_output_loss: 0.5775\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3919 - main_output_loss: 0.3730 - aux_output_loss: 0.5619 - val_loss: 0.4132 - val_main_output_loss: 0.3942 - val_aux_output_loss: 0.5841\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3865 - main_output_loss: 0.3679 - aux_output_loss: 0.5536 - val_loss: 0.3849 - val_main_output_loss: 0.3650 - val_aux_output_loss: 0.5645\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3814 - main_output_loss: 0.3631 - aux_output_loss: 0.5462 - val_loss: 0.3912 - val_main_output_loss: 0.3724 - val_aux_output_loss: 0.5608\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([X_train_A, X_train_B], [y_train, y_train], epochs=20, validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 752us/step - loss: 0.3761 - main_output_loss: 0.3582 - aux_output_loss: 0.5372\n"
     ]
    }
   ],
   "source": [
    "total_loss, main_loss, aux_loss = model.evaluate([X_test_A, X_test_B], [y_test, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_main, y_pred_aux = model.predict([X_new_A, X_new_B])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeepModel(keras.Model):\n",
    "    def __init__(self, units=30, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = keras.layers.Dense(units, activation=activation)\n",
    "        self.hidden2 = keras.layers.Dense(units, activation=activation)\n",
    "        self.main_output = keras.layers.Dense(1)\n",
    "        self.aux_output = keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_A, input_B = inputs\n",
    "        hidden1 = self.hidden1(input_B)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = keras.layers.concatenate([input_A, hidden2])\n",
    "        main_output = self.main_output(concat)\n",
    "        aux_output = self.aux_output(hidden2)\n",
    "        return main_output, aux_output\n",
    "\n",
    "model = WideAndDeepModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 2.7948 - output_1_loss: 2.6042 - output_2_loss: 4.5101 - val_loss: 1.3687 - val_output_1_loss: 1.1114 - val_output_2_loss: 3.6841\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.1567 - output_1_loss: 0.9364 - output_2_loss: 3.1397 - val_loss: 1.0097 - val_output_1_loss: 0.8386 - val_output_2_loss: 2.5500\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.9344 - output_1_loss: 0.7797 - output_2_loss: 2.3268 - val_loss: 0.8794 - val_output_1_loss: 0.7569 - val_output_2_loss: 1.9818\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.8403 - output_1_loss: 0.7199 - output_2_loss: 1.9246 - val_loss: 0.8124 - val_output_1_loss: 0.7124 - val_output_2_loss: 1.7124\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.7872 - output_1_loss: 0.6829 - output_2_loss: 1.7257 - val_loss: 0.7692 - val_output_1_loss: 0.6783 - val_output_2_loss: 1.5867\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.7504 - output_1_loss: 0.6540 - output_2_loss: 1.6176 - val_loss: 0.7377 - val_output_1_loss: 0.6512 - val_output_2_loss: 1.5165\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.7220 - output_1_loss: 0.6299 - output_2_loss: 1.5505 - val_loss: 0.7119 - val_output_1_loss: 0.6277 - val_output_2_loss: 1.4697\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6986 - output_1_loss: 0.6094 - output_2_loss: 1.5012 - val_loss: 0.6906 - val_output_1_loss: 0.6077 - val_output_2_loss: 1.4368\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6792 - output_1_loss: 0.5921 - output_2_loss: 1.4632 - val_loss: 0.6719 - val_output_1_loss: 0.5899 - val_output_2_loss: 1.4094\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6628 - output_1_loss: 0.5773 - output_2_loss: 1.4321 - val_loss: 0.6572 - val_output_1_loss: 0.5763 - val_output_2_loss: 1.3856\n",
      "162/162 [==============================] - 0s 720us/step - loss: 0.6379 - output_1_loss: 0.5508 - output_2_loss: 1.4214\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001C8321F3CA8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", loss_weights=[0.9, 0.1], optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "history = model.fit((X_train_A, X_train_B), (y_train, y_train), epochs=10, validation_data=((X_valid_A, X_valid_B), (y_valid, y_valid)))\n",
    "total_loss, main_loss, aux_loss = model.evaluate((X_test_A, X_test_B), (y_test, y_test))\n",
    "y_pred_main, y_pred_aux = model.predict((X_new_A, X_new_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 2.2438 - val_loss: 0.5949\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 0s 815us/step - loss: 2.0338 - val_loss: 0.5366\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 0s 826us/step - loss: 0.4517 - val_loss: 0.4193\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 0s 823us/step - loss: 0.4083 - val_loss: 0.3897\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 0s 815us/step - loss: 0.6478 - val_loss: 0.5063\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 0s 826us/step - loss: 0.4413 - val_loss: 0.4077\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 0s 812us/step - loss: 0.3874 - val_loss: 0.3900\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 0s 818us/step - loss: 0.3679 - val_loss: 0.3715\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 0s 826us/step - loss: 0.3638 - val_loss: 0.3678\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 0s 837us/step - loss: 0.3544 - val_loss: 0.3602\n",
      "162/162 [==============================] - 0s 596us/step - loss: 0.3443\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", optimizer=\"sgd\")\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))\n",
    "mse_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"my_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001C80EDD9AF8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.3017472],\n",
       "       [2.4227128],\n",
       "       [1.5772861]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.9409 - val_loss: 0.6386\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 0s 881us/step - loss: 0.5680 - val_loss: 0.4812\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 0s 870us/step - loss: 0.4532 - val_loss: 0.4607\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 0s 868us/step - loss: 0.4254 - val_loss: 0.4232\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 0s 859us/step - loss: 0.4100 - val_loss: 0.4145\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 0s 868us/step - loss: 0.3989 - val_loss: 0.4074\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 0s 865us/step - loss: 0.3895 - val_loss: 0.3956\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 0s 845us/step - loss: 0.3818 - val_loss: 0.3985\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 0s 865us/step - loss: 0.3756 - val_loss: 0.3876\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 0s 868us/step - loss: 0.3708 - val_loss: 0.3860\n",
      "162/162 [==============================] - 0s 603us/step - loss: 0.3656\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", optimizer=\"sgd\")\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_model.h5\", save_best_only=True)\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid), callbacks=[checkpoint_cb])\n",
    "model = keras.models.load_model(\"my_model.h5\")\n",
    "mse_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3647 - val_loss: 0.3828\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 981us/step - loss: 0.3622 - val_loss: 0.3732\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 859us/step - loss: 0.3592 - val_loss: 0.3683\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 862us/step - loss: 0.3534 - val_loss: 0.3664\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 812us/step - loss: 0.3512 - val_loss: 0.3702\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 858us/step - loss: 0.3495 - val_loss: 0.3589\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 862us/step - loss: 0.3435 - val_loss: 0.3564\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 865us/step - loss: 0.3407 - val_loss: 0.3560\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 868us/step - loss: 0.3361 - val_loss: 0.3506\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 823us/step - loss: 0.3347 - val_loss: 0.3513\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 815us/step - loss: 0.3654 - val_loss: 0.3575\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 0s 865us/step - loss: 0.3355 - val_loss: 0.3411\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 833us/step - loss: 0.3283 - val_loss: 0.3414\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 879us/step - loss: 0.3266 - val_loss: 0.3364\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 868us/step - loss: 0.3246 - val_loss: 0.3332\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 826us/step - loss: 0.3219 - val_loss: 0.3366\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 0s 857us/step - loss: 0.3197 - val_loss: 0.3293\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 0s 822us/step - loss: 0.3176 - val_loss: 0.3359\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 818us/step - loss: 0.3159 - val_loss: 0.3366\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 823us/step - loss: 0.3131 - val_loss: 0.3363\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3139 - val_loss: 0.3290\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 0s 843us/step - loss: 0.3107 - val_loss: 0.3291\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 0s 892us/step - loss: 0.3096 - val_loss: 0.3199\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 0s 821us/step - loss: 0.3085 - val_loss: 0.3572\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 0s 826us/step - loss: 0.3073 - val_loss: 0.3261\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 0s 868us/step - loss: 0.3048 - val_loss: 0.3189\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 0s 854us/step - loss: 0.3018 - val_loss: 0.3184\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 0s 854us/step - loss: 0.3025 - val_loss: 0.3143\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 0s 834us/step - loss: 0.3019 - val_loss: 0.3226\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 0s 813us/step - loss: 0.2999 - val_loss: 0.3215\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 0s 823us/step - loss: 0.2988 - val_loss: 0.3157\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 0s 827us/step - loss: 0.2988 - val_loss: 0.3241\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 0s 818us/step - loss: 0.2977 - val_loss: 0.3309\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 0s 821us/step - loss: 0.2954 - val_loss: 0.3163\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 0s 851us/step - loss: 0.2961 - val_loss: 0.3083\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 0s 807us/step - loss: 0.2949 - val_loss: 0.3118\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 0s 821us/step - loss: 0.2933 - val_loss: 0.3525\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 0s 906us/step - loss: 0.2938 - val_loss: 0.3194\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 0s 881us/step - loss: 0.2933 - val_loss: 0.3064\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 0s 881us/step - loss: 0.2907 - val_loss: 0.3147\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 0s 837us/step - loss: 0.2912 - val_loss: 0.3076\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 0s 832us/step - loss: 0.2918 - val_loss: 0.3229\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 0s 823us/step - loss: 0.2890 - val_loss: 0.3375\n",
      "Epoch 44/100\n",
      "363/363 [==============================] - 0s 832us/step - loss: 0.2899 - val_loss: 0.3429\n",
      "Epoch 45/100\n",
      "363/363 [==============================] - 0s 821us/step - loss: 0.2915 - val_loss: 0.3115\n",
      "Epoch 46/100\n",
      "363/363 [==============================] - 0s 834us/step - loss: 0.2885 - val_loss: 0.3067\n",
      "Epoch 47/100\n",
      "363/363 [==============================] - 0s 868us/step - loss: 0.2901 - val_loss: 0.3047\n",
      "Epoch 48/100\n",
      "363/363 [==============================] - 0s 810us/step - loss: 0.2885 - val_loss: 0.3073\n",
      "Epoch 49/100\n",
      "363/363 [==============================] - 0s 829us/step - loss: 0.2876 - val_loss: 0.3123\n",
      "Epoch 50/100\n",
      "363/363 [==============================] - 0s 948us/step - loss: 0.2874 - val_loss: 0.3136\n",
      "Epoch 51/100\n",
      "363/363 [==============================] - 0s 826us/step - loss: 0.2873 - val_loss: 0.3054\n",
      "Epoch 52/100\n",
      "363/363 [==============================] - 0s 857us/step - loss: 0.2846 - val_loss: 0.3044\n",
      "Epoch 53/100\n",
      "363/363 [==============================] - 0s 818us/step - loss: 0.2881 - val_loss: 0.3050\n",
      "Epoch 54/100\n",
      "363/363 [==============================] - 0s 840us/step - loss: 0.2861 - val_loss: 0.3065\n",
      "Epoch 55/100\n",
      "363/363 [==============================] - 0s 974us/step - loss: 0.2882 - val_loss: 0.3014\n",
      "Epoch 56/100\n",
      "363/363 [==============================] - 0s 859us/step - loss: 0.2853 - val_loss: 0.3003\n",
      "Epoch 57/100\n",
      "363/363 [==============================] - 0s 881us/step - loss: 0.2877 - val_loss: 0.3043\n",
      "Epoch 58/100\n",
      "363/363 [==============================] - 0s 829us/step - loss: 0.2876 - val_loss: 0.3025\n",
      "Epoch 59/100\n",
      "363/363 [==============================] - 0s 829us/step - loss: 0.2872 - val_loss: 0.3029\n",
      "Epoch 60/100\n",
      "363/363 [==============================] - 0s 826us/step - loss: 0.2853 - val_loss: 0.3064\n",
      "Epoch 61/100\n",
      "363/363 [==============================] - 0s 826us/step - loss: 0.2858 - val_loss: 0.3211\n",
      "Epoch 62/100\n",
      "363/363 [==============================] - 0s 821us/step - loss: 0.2845 - val_loss: 0.3034\n",
      "Epoch 63/100\n",
      "363/363 [==============================] - 0s 832us/step - loss: 0.2837 - val_loss: 0.3019\n",
      "Epoch 64/100\n",
      "363/363 [==============================] - 0s 815us/step - loss: 0.2833 - val_loss: 0.3015\n",
      "Epoch 65/100\n",
      "363/363 [==============================] - 0s 829us/step - loss: 0.2832 - val_loss: 0.3078\n",
      "Epoch 66/100\n",
      "363/363 [==============================] - 0s 876us/step - loss: 0.2829 - val_loss: 0.2995\n",
      "Epoch 67/100\n",
      "363/363 [==============================] - 0s 865us/step - loss: 0.2809 - val_loss: 0.2987\n",
      "Epoch 68/100\n",
      "363/363 [==============================] - 0s 829us/step - loss: 0.2805 - val_loss: 0.3323\n",
      "Epoch 69/100\n",
      "363/363 [==============================] - 0s 829us/step - loss: 0.2799 - val_loss: 0.3035\n",
      "Epoch 70/100\n",
      "363/363 [==============================] - 0s 823us/step - loss: 0.2820 - val_loss: 0.3041\n",
      "Epoch 71/100\n",
      "363/363 [==============================] - 0s 829us/step - loss: 0.2807 - val_loss: 0.3159\n",
      "Epoch 72/100\n",
      "363/363 [==============================] - 0s 950us/step - loss: 0.2814 - val_loss: 0.2988\n",
      "Epoch 73/100\n",
      "363/363 [==============================] - 0s 862us/step - loss: 0.2804 - val_loss: 0.2973\n",
      "Epoch 74/100\n",
      "363/363 [==============================] - 0s 868us/step - loss: 0.2798 - val_loss: 0.3012\n",
      "Epoch 75/100\n",
      "363/363 [==============================] - 0s 837us/step - loss: 0.2798 - val_loss: 0.3096\n",
      "Epoch 76/100\n",
      "363/363 [==============================] - 0s 829us/step - loss: 0.2796 - val_loss: 0.3115\n",
      "Epoch 77/100\n",
      "363/363 [==============================] - 0s 829us/step - loss: 0.2768 - val_loss: 0.2977\n",
      "Epoch 78/100\n",
      "363/363 [==============================] - 0s 815us/step - loss: 0.2794 - val_loss: 0.2977\n",
      "Epoch 79/100\n",
      "363/363 [==============================] - 0s 823us/step - loss: 0.2781 - val_loss: 0.3025\n",
      "Epoch 80/100\n",
      "363/363 [==============================] - 0s 823us/step - loss: 0.2778 - val_loss: 0.3065\n",
      "Epoch 81/100\n",
      "363/363 [==============================] - 0s 823us/step - loss: 0.2788 - val_loss: 0.3031\n",
      "Epoch 82/100\n",
      "363/363 [==============================] - 0s 823us/step - loss: 0.2781 - val_loss: 0.3197\n",
      "Epoch 83/100\n",
      "363/363 [==============================] - 0s 826us/step - loss: 0.2777 - val_loss: 0.2977\n",
      "162/162 [==============================] - 0s 603us/step - loss: 0.2893\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", optimizer=\"sgd\")\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "history = model.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=[checkpoint_cb, early_stopping_cb])\n",
    "mse_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintValTrainRatioCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        print(\"\\nval/train: {:.2f}\".format(logs[\"val_loss\"] / logs[\"loss\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312/363 [========================>.....] - ETA: 0s - loss: 0.2770\n",
      "val/train: 1.08\n",
      "363/363 [==============================] - 0s 901us/step - loss: 0.2788 - val_loss: 0.3000\n"
     ]
    }
   ],
   "source": [
    "val_train_ratio_cb = PrintValTrainRatioCallback()\n",
    "history = model.fit(X_train, y_train, epochs=1, validation_data=(X_valid, y_valid), callbacks=[val_train_ratio_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.\\\\my_logs\\\\run_2022_03_07-14_51_17'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "root_logdir = os.path.join(os.curdir, \"my_logs\")\n",
    "\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_logdir = get_run_logdir()\n",
    "run_logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss=\"mse\", optimizer=\"sgd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.8470 - val_loss: 0.5185\n",
      "Epoch 2/30\n",
      "363/363 [==============================] - 0s 981us/step - loss: 0.4897 - val_loss: 0.4583\n",
      "Epoch 3/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4491 - val_loss: 0.4351\n",
      "Epoch 4/30\n",
      "363/363 [==============================] - 0s 973us/step - loss: 0.4288 - val_loss: 0.4264\n",
      "Epoch 5/30\n",
      "363/363 [==============================] - 0s 978us/step - loss: 0.4175 - val_loss: 0.4103\n",
      "Epoch 6/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4043 - val_loss: 0.4052\n",
      "Epoch 7/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3958 - val_loss: 0.4004\n",
      "Epoch 8/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3861 - val_loss: 0.3883\n",
      "Epoch 9/30\n",
      "363/363 [==============================] - 0s 848us/step - loss: 0.3788 - val_loss: 0.3766\n",
      "Epoch 10/30\n",
      "363/363 [==============================] - 0s 857us/step - loss: 0.3705 - val_loss: 0.3702\n",
      "Epoch 11/30\n",
      "363/363 [==============================] - 0s 884us/step - loss: 0.3645 - val_loss: 0.3624\n",
      "Epoch 12/30\n",
      "363/363 [==============================] - 0s 834us/step - loss: 0.3572 - val_loss: 0.3653\n",
      "Epoch 13/30\n",
      "363/363 [==============================] - 0s 842us/step - loss: 0.3510 - val_loss: 0.3566\n",
      "Epoch 14/30\n",
      "363/363 [==============================] - 0s 839us/step - loss: 0.3475 - val_loss: 0.3619\n",
      "Epoch 15/30\n",
      "363/363 [==============================] - 0s 837us/step - loss: 0.3429 - val_loss: 0.3515\n",
      "Epoch 16/30\n",
      "363/363 [==============================] - 0s 837us/step - loss: 0.3400 - val_loss: 0.3462\n",
      "Epoch 17/30\n",
      "363/363 [==============================] - 0s 859us/step - loss: 0.3347 - val_loss: 0.3493\n",
      "Epoch 18/30\n",
      "363/363 [==============================] - 0s 912us/step - loss: 0.3341 - val_loss: 0.3442\n",
      "Epoch 19/30\n",
      "363/363 [==============================] - 0s 989us/step - loss: 0.3316 - val_loss: 0.3577\n",
      "Epoch 20/30\n",
      "363/363 [==============================] - 0s 839us/step - loss: 0.3307 - val_loss: 0.3363\n",
      "Epoch 21/30\n",
      "363/363 [==============================] - 0s 829us/step - loss: 0.3272 - val_loss: 0.4250\n",
      "Epoch 22/30\n",
      "363/363 [==============================] - 0s 828us/step - loss: 0.3252 - val_loss: 0.3368\n",
      "Epoch 23/30\n",
      "363/363 [==============================] - 0s 832us/step - loss: 0.3262 - val_loss: 0.3391\n",
      "Epoch 24/30\n",
      "363/363 [==============================] - 0s 840us/step - loss: 0.3216 - val_loss: 0.3338\n",
      "Epoch 25/30\n",
      "363/363 [==============================] - 0s 830us/step - loss: 0.3213 - val_loss: 0.3352\n",
      "Epoch 26/30\n",
      "363/363 [==============================] - 0s 830us/step - loss: 0.3178 - val_loss: 0.3261\n",
      "Epoch 27/30\n",
      "363/363 [==============================] - 0s 834us/step - loss: 0.3160 - val_loss: 0.3348\n",
      "Epoch 28/30\n",
      "363/363 [==============================] - 0s 892us/step - loss: 0.3148 - val_loss: 0.3321\n",
      "Epoch 29/30\n",
      "363/363 [==============================] - 0s 839us/step - loss: 0.3163 - val_loss: 0.3966\n",
      "Epoch 30/30\n",
      "363/363 [==============================] - 0s 832us/step - loss: 0.3133 - val_loss: 0.3330\n"
     ]
    }
   ],
   "source": [
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "history = model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid), callbacks=[tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
    "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ozing\\miniconda3\\envs\\handson-ml2\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  1/363 [..............................] - ETA: 1:03 - loss: 6.5192"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ozing\\miniconda3\\envs\\handson-ml2\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 1s 1ms/step - loss: 1.2080 - val_loss: 0.6669\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 812us/step - loss: 0.6266 - val_loss: 0.5951\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 890us/step - loss: 0.5741 - val_loss: 0.5773\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 859us/step - loss: 0.5440 - val_loss: 0.5294\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 813us/step - loss: 0.5224 - val_loss: 0.5133\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 821us/step - loss: 0.5040 - val_loss: 0.4989\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 815us/step - loss: 0.4965 - val_loss: 0.4909\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 832us/step - loss: 0.4853 - val_loss: 0.4795\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 812us/step - loss: 0.4771 - val_loss: 0.4735\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 818us/step - loss: 0.4692 - val_loss: 0.4672\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 834us/step - loss: 0.4631 - val_loss: 0.4714\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 0s 815us/step - loss: 0.4613 - val_loss: 0.4616\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 837us/step - loss: 0.4525 - val_loss: 0.4529\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 862us/step - loss: 0.4485 - val_loss: 0.4502\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 804us/step - loss: 0.4437 - val_loss: 0.4452\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 810us/step - loss: 0.4402 - val_loss: 0.4421\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 0s 837us/step - loss: 0.4363 - val_loss: 0.4389\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 0s 804us/step - loss: 0.4328 - val_loss: 0.4356\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 873us/step - loss: 0.4297 - val_loss: 0.4344\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 785us/step - loss: 0.4260 - val_loss: 0.4327\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 0s 776us/step - loss: 0.4243 - val_loss: 0.4290\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 0s 776us/step - loss: 0.4209 - val_loss: 0.4270\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 0s 776us/step - loss: 0.4186 - val_loss: 0.4251\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 0s 785us/step - loss: 0.4164 - val_loss: 0.4222\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 0s 779us/step - loss: 0.4139 - val_loss: 0.4205\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 0s 771us/step - loss: 0.4119 - val_loss: 0.4196\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 0s 782us/step - loss: 0.4095 - val_loss: 0.4185\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 0s 795us/step - loss: 0.4081 - val_loss: 0.4152\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 0s 785us/step - loss: 0.4075 - val_loss: 0.4139\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 0s 774us/step - loss: 0.4040 - val_loss: 0.4153\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 0s 776us/step - loss: 0.4025 - val_loss: 0.4125\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 0s 768us/step - loss: 0.4007 - val_loss: 0.4106\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 0s 774us/step - loss: 0.3993 - val_loss: 0.4090\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 0s 785us/step - loss: 0.4005 - val_loss: 0.4100\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 0s 790us/step - loss: 0.3994 - val_loss: 0.4092\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 0s 895us/step - loss: 0.3954 - val_loss: 0.4057\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 0s 771us/step - loss: 0.3934 - val_loss: 0.4044\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 0s 790us/step - loss: 0.3934 - val_loss: 0.4044\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 0s 776us/step - loss: 0.3906 - val_loss: 0.4012\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 0s 782us/step - loss: 0.3894 - val_loss: 0.4015\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 0s 796us/step - loss: 0.3923 - val_loss: 0.4011\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 0s 796us/step - loss: 0.3873 - val_loss: 0.4000\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 0s 801us/step - loss: 0.3862 - val_loss: 0.4004\n",
      "Epoch 44/100\n",
      "363/363 [==============================] - 0s 779us/step - loss: 0.3847 - val_loss: 0.3970\n",
      "Epoch 45/100\n",
      "363/363 [==============================] - 0s 801us/step - loss: 0.3934 - val_loss: 0.3974\n",
      "Epoch 46/100\n",
      "363/363 [==============================] - 0s 772us/step - loss: 0.3876 - val_loss: 0.3965\n",
      "Epoch 47/100\n",
      "363/363 [==============================] - 0s 801us/step - loss: 0.3819 - val_loss: 0.3957\n",
      "Epoch 48/100\n",
      "363/363 [==============================] - 0s 877us/step - loss: 0.3811 - val_loss: 0.3940\n",
      "Epoch 49/100\n",
      "363/363 [==============================] - 0s 837us/step - loss: 0.3814 - val_loss: 0.3939\n",
      "Epoch 50/100\n",
      "363/363 [==============================] - 0s 829us/step - loss: 0.3793 - val_loss: 0.3910\n",
      "Epoch 51/100\n",
      "363/363 [==============================] - 0s 845us/step - loss: 0.3777 - val_loss: 0.3901\n",
      "Epoch 52/100\n",
      "363/363 [==============================] - 0s 848us/step - loss: 0.3772 - val_loss: 0.3904\n",
      "Epoch 53/100\n",
      "363/363 [==============================] - 0s 840us/step - loss: 0.3768 - val_loss: 0.3896\n",
      "Epoch 54/100\n",
      "363/363 [==============================] - 0s 829us/step - loss: 0.3746 - val_loss: 0.3922\n",
      "Epoch 55/100\n",
      "363/363 [==============================] - 0s 832us/step - loss: 0.3747 - val_loss: 0.3885\n",
      "Epoch 56/100\n",
      "363/363 [==============================] - 0s 830us/step - loss: 0.3736 - val_loss: 0.3926\n",
      "Epoch 57/100\n",
      "363/363 [==============================] - 0s 837us/step - loss: 0.3735 - val_loss: 0.3893\n",
      "Epoch 58/100\n",
      "363/363 [==============================] - 0s 851us/step - loss: 0.3830 - val_loss: 0.3881\n",
      "Epoch 59/100\n",
      "363/363 [==============================] - 0s 848us/step - loss: 0.3772 - val_loss: 0.3871\n",
      "Epoch 60/100\n",
      "363/363 [==============================] - 0s 823us/step - loss: 0.3693 - val_loss: 0.3854\n",
      "Epoch 61/100\n",
      "363/363 [==============================] - 0s 870us/step - loss: 0.3697 - val_loss: 0.3850\n",
      "Epoch 62/100\n",
      "363/363 [==============================] - 0s 843us/step - loss: 0.3698 - val_loss: 0.3838\n",
      "Epoch 63/100\n",
      "363/363 [==============================] - 0s 840us/step - loss: 0.3805 - val_loss: 0.3826\n",
      "Epoch 64/100\n",
      "363/363 [==============================] - 0s 832us/step - loss: 0.3676 - val_loss: 0.3816\n",
      "Epoch 65/100\n",
      "363/363 [==============================] - 0s 843us/step - loss: 0.3682 - val_loss: 0.3830\n",
      "Epoch 66/100\n",
      "363/363 [==============================] - 0s 845us/step - loss: 0.3668 - val_loss: 0.3818\n",
      "Epoch 67/100\n",
      "363/363 [==============================] - 0s 845us/step - loss: 0.3771 - val_loss: 0.3807\n",
      "Epoch 68/100\n",
      "363/363 [==============================] - 0s 840us/step - loss: 0.3638 - val_loss: 0.3809\n",
      "Epoch 69/100\n",
      "363/363 [==============================] - 0s 843us/step - loss: 0.3654 - val_loss: 0.3788\n",
      "Epoch 70/100\n",
      "363/363 [==============================] - 0s 845us/step - loss: 0.3792 - val_loss: 0.3786\n",
      "Epoch 71/100\n",
      "363/363 [==============================] - 0s 771us/step - loss: 0.3624 - val_loss: 0.3781\n",
      "Epoch 72/100\n",
      "363/363 [==============================] - 0s 776us/step - loss: 0.3686 - val_loss: 0.3783\n",
      "Epoch 73/100\n",
      "363/363 [==============================] - 0s 771us/step - loss: 0.3702 - val_loss: 0.3762\n",
      "Epoch 74/100\n",
      "363/363 [==============================] - 0s 779us/step - loss: 0.3636 - val_loss: 0.3761\n",
      "Epoch 75/100\n",
      "363/363 [==============================] - 0s 776us/step - loss: 0.3627 - val_loss: 0.3766\n",
      "Epoch 76/100\n",
      "363/363 [==============================] - 0s 776us/step - loss: 0.3838 - val_loss: 0.3766\n",
      "Epoch 77/100\n",
      "363/363 [==============================] - 0s 776us/step - loss: 0.3593 - val_loss: 0.3776\n",
      "Epoch 78/100\n",
      "363/363 [==============================] - 0s 782us/step - loss: 0.3785 - val_loss: 0.3784\n",
      "Epoch 79/100\n",
      "363/363 [==============================] - 0s 765us/step - loss: 0.3596 - val_loss: 0.3748\n",
      "Epoch 80/100\n",
      "363/363 [==============================] - 0s 779us/step - loss: 0.3757 - val_loss: 0.3759\n",
      "Epoch 81/100\n",
      "363/363 [==============================] - 0s 774us/step - loss: 0.3600 - val_loss: 0.3748\n",
      "Epoch 82/100\n",
      "363/363 [==============================] - 0s 771us/step - loss: 0.3736 - val_loss: 0.3785\n",
      "Epoch 83/100\n",
      "363/363 [==============================] - 0s 784us/step - loss: 0.3576 - val_loss: 0.3763\n",
      "Epoch 84/100\n",
      "363/363 [==============================] - 0s 774us/step - loss: 0.3770 - val_loss: 0.3763\n",
      "Epoch 85/100\n",
      "363/363 [==============================] - 0s 787us/step - loss: 0.3568 - val_loss: 0.3717\n",
      "Epoch 86/100\n",
      "363/363 [==============================] - 0s 782us/step - loss: 0.3755 - val_loss: 0.3729\n",
      "Epoch 87/100\n",
      "363/363 [==============================] - 0s 776us/step - loss: 0.3639 - val_loss: 0.3735\n",
      "Epoch 88/100\n",
      "363/363 [==============================] - 0s 787us/step - loss: 0.3558 - val_loss: 0.3729\n",
      "Epoch 89/100\n",
      "363/363 [==============================] - 0s 776us/step - loss: 0.3545 - val_loss: 0.3705\n",
      "Epoch 90/100\n",
      "363/363 [==============================] - 0s 782us/step - loss: 0.3662 - val_loss: 0.3718\n",
      "Epoch 91/100\n",
      "363/363 [==============================] - 0s 779us/step - loss: 0.3598 - val_loss: 0.3715\n",
      "Epoch 92/100\n",
      "363/363 [==============================] - 0s 785us/step - loss: 0.3565 - val_loss: 0.3720\n",
      "Epoch 93/100\n",
      "363/363 [==============================] - 0s 790us/step - loss: 0.3549 - val_loss: 0.3706\n",
      "Epoch 94/100\n",
      "363/363 [==============================] - 0s 776us/step - loss: 0.3584 - val_loss: 0.3698\n",
      "Epoch 95/100\n",
      "363/363 [==============================] - 0s 765us/step - loss: 0.3579 - val_loss: 0.3696\n",
      "Epoch 96/100\n",
      "363/363 [==============================] - 0s 774us/step - loss: 0.3571 - val_loss: 0.3686\n",
      "Epoch 97/100\n",
      "363/363 [==============================] - 0s 771us/step - loss: 0.3682 - val_loss: 0.3691\n",
      "Epoch 98/100\n",
      "363/363 [==============================] - 0s 779us/step - loss: 0.3523 - val_loss: 0.3687\n",
      "Epoch 99/100\n",
      "363/363 [==============================] - 0s 779us/step - loss: 0.3546 - val_loss: 0.3686\n",
      "Epoch 100/100\n",
      "363/363 [==============================] - 0s 787us/step - loss: 0.3523 - val_loss: 0.3671\n",
      "162/162 [==============================] - 0s 559us/step - loss: 0.3453\n"
     ]
    }
   ],
   "source": [
    "keras_reg.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
    "mse_test = keras_reg.score(X_test, y_test)\n",
    "y_pred = keras_reg.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ozing\\miniconda3\\envs\\handson-ml2\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3774 - val_loss: 0.6739\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: 0.9874 - val_loss: 0.5302\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 926us/step - loss: 0.5251 - val_loss: 0.4923\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.4834 - val_loss: 0.4743\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4686 - val_loss: 0.4647\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.4558 - val_loss: 0.4630\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.4664 - val_loss: 0.4657\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 930us/step - loss: 0.4488 - val_loss: 0.4544\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.4391 - val_loss: 0.4456\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4329 - val_loss: 0.4435\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: 0.4277 - val_loss: 0.4404\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: 0.4253 - val_loss: 0.4354\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.4213 - val_loss: 0.4369\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4215 - val_loss: 0.4294\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.4229 - val_loss: 0.4289\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.4150 - val_loss: 0.4252\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4124 - val_loss: 0.4278\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4091 - val_loss: 0.4204\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4091 - val_loss: 0.4184\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.4009 - val_loss: 0.4151\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.3976 - val_loss: 0.4119\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.3950 - val_loss: 0.4165\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 941us/step - loss: 0.3963 - val_loss: 0.4077\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3923 - val_loss: 0.4116\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3982 - val_loss: 0.4060\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: 0.3894 - val_loss: 0.4073\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 930us/step - loss: 0.3933 - val_loss: 0.4123\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3857 - val_loss: 0.4082\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.3820 - val_loss: 0.4020\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.3830 - val_loss: 0.4186\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.3811 - val_loss: 0.4007\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3757 - val_loss: 0.3961\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.3775 - val_loss: 0.3960\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.3808 - val_loss: 0.3966\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.3797 - val_loss: 0.3913\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3939 - val_loss: 0.3938\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3911 - val_loss: 0.3926\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3726 - val_loss: 0.3913\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.3699 - val_loss: 0.3879\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 930us/step - loss: 0.3726 - val_loss: 0.3867\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3660 - val_loss: 0.3852\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3653 - val_loss: 0.3871\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3642 - val_loss: 0.3939\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3605 - val_loss: 0.3944\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3604 - val_loss: 0.4477\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.3699 - val_loss: 0.3800\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.3705 - val_loss: 0.3814\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.3591 - val_loss: 0.3835\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3668 - val_loss: 0.3784\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 904us/step - loss: 0.3560 - val_loss: 0.3785\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.3535 - val_loss: 0.3822\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.3527 - val_loss: 0.3804\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.3604 - val_loss: 0.3779\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3510 - val_loss: 0.3738\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.3493 - val_loss: 0.3766\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.3639 - val_loss: 0.3760\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3492 - val_loss: 0.3745\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 930us/step - loss: 0.3507 - val_loss: 0.3734\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.3589 - val_loss: 0.3766\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.3544 - val_loss: 0.3745\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3485 - val_loss: 0.3735\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.3501 - val_loss: 0.3695\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3468 - val_loss: 0.3716\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.3466 - val_loss: 0.4271\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.3489 - val_loss: 0.3711\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3803 - val_loss: 0.3709\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.3496 - val_loss: 0.3691\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3489 - val_loss: 0.4145\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.3504 - val_loss: 0.3723\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3432 - val_loss: 0.3711\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3428 - val_loss: 0.3669\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3419 - val_loss: 0.3692\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 889us/step - loss: 0.3404 - val_loss: 0.3661\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.3395 - val_loss: 0.3648\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3390 - val_loss: 0.3678\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.3450 - val_loss: 0.3652\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3457 - val_loss: 0.3695\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3459 - val_loss: 0.3691\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3368 - val_loss: 0.3613\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3380 - val_loss: 0.3654\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.3408 - val_loss: 0.3641\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.3359 - val_loss: 0.3625\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.3352 - val_loss: 0.3582\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.3369 - val_loss: 0.3635\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 926us/step - loss: 0.3377 - val_loss: 0.3574\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 902us/step - loss: 0.3369 - val_loss: 0.3635\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 904us/step - loss: 0.3345 - val_loss: 0.3595\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.3322 - val_loss: 0.3767\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.3412 - val_loss: 0.3580\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3298 - val_loss: 0.3600\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.3311 - val_loss: 0.3587\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.3581 - val_loss: 0.3635\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 930us/step - loss: 0.3340 - val_loss: 0.3572\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3552 - val_loss: 0.3655\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.3388 - val_loss: 0.3604\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.3298 - val_loss: 0.3567\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3423 - val_loss: 0.4212\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3398 - val_loss: 0.3535\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.3409 - val_loss: 0.3651\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4615 - val_loss: 0.3598\n",
      "121/121 [==============================] - 0s 633us/step - loss: 0.3576\n",
      "[CV] END learning_rate=0.008147426718418045, n_hidden=1, n_neurons=50; total time=  22.7s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ozing\\miniconda3\\envs\\handson-ml2\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8417 - val_loss: 0.5803\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.5858 - val_loss: 0.5431\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: 0.5585 - val_loss: 0.4917\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.5573 - val_loss: 0.4861\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4951 - val_loss: 0.4638\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.4814 - val_loss: 0.4570\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4653 - val_loss: 0.4522\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4540 - val_loss: 0.4489\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4464 - val_loss: 0.4492\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.4429 - val_loss: 0.4381\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.4359 - val_loss: 0.4348\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4389 - val_loss: 0.4296\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.4478 - val_loss: 0.4327\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4237 - val_loss: 0.4248\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.4261 - val_loss: 0.4267\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 926us/step - loss: 0.4194 - val_loss: 0.4211\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4233 - val_loss: 0.4206\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4161 - val_loss: 0.4137\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4077 - val_loss: 0.4091\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4059 - val_loss: 0.4055\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: 0.4015 - val_loss: 0.4038\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4038 - val_loss: 0.4078\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4006 - val_loss: 0.4033\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.3981 - val_loss: 0.4025\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3919 - val_loss: 0.3950\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3890 - val_loss: 0.3955\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.3865 - val_loss: 0.3913\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3828 - val_loss: 0.3935\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3818 - val_loss: 0.3867\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.3814 - val_loss: 0.3921\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.3802 - val_loss: 0.3863\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3781 - val_loss: 0.3826\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3747 - val_loss: 0.3835\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.3736 - val_loss: 0.4017\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3764 - val_loss: 0.3803\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.3710 - val_loss: 0.3813\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3855 - val_loss: 0.3794\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.3707 - val_loss: 0.3759\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.3785 - val_loss: 0.3822\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3690 - val_loss: 0.3899\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3685 - val_loss: 0.3735\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3660 - val_loss: 0.4124\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3707 - val_loss: 0.3728\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.3618 - val_loss: 0.3728\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3608 - val_loss: 0.3698\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.3594 - val_loss: 0.3714\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: 0.3614 - val_loss: 0.3698\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 926us/step - loss: 0.3565 - val_loss: 0.3663\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 916us/step - loss: 0.3836 - val_loss: 0.3784\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.3596 - val_loss: 0.3706\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3568 - val_loss: 0.3664\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3538 - val_loss: 0.3678\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3556 - val_loss: 0.3641\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.3532 - val_loss: 0.3614\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3519 - val_loss: 0.3601\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 897us/step - loss: 0.3604 - val_loss: 0.3618\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3496 - val_loss: 0.3614\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3476 - val_loss: 0.3626\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3503 - val_loss: 0.3581\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 904us/step - loss: 0.3488 - val_loss: 0.3563\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.3468 - val_loss: 0.3562\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.3454 - val_loss: 0.3575\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.3452 - val_loss: 0.3564\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.3447 - val_loss: 0.3549\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.3478 - val_loss: 0.3566\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.3441 - val_loss: 0.3563\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3429 - val_loss: 0.3561\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.3415 - val_loss: 0.3678\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.3415 - val_loss: 0.3499\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 926us/step - loss: 0.3403 - val_loss: 0.3502\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3405 - val_loss: 0.3601\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3399 - val_loss: 0.3585\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.3388 - val_loss: 0.3532\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.3396 - val_loss: 0.3489\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 930us/step - loss: 0.3389 - val_loss: 0.3471\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.3443 - val_loss: 0.3501\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 908us/step - loss: 0.3399 - val_loss: 0.4455\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: 0.3470 - val_loss: 0.3604\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 902us/step - loss: 0.3416 - val_loss: 0.3504\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 910us/step - loss: 0.3363 - val_loss: 0.3530\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.3343 - val_loss: 0.3497\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3350 - val_loss: 0.3631\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.3329 - val_loss: 0.3652\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.3318 - val_loss: 0.3435\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 918us/step - loss: 0.3301 - val_loss: 0.3413\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.3291 - val_loss: 0.3437\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.3287 - val_loss: 0.3396\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3300 - val_loss: 0.3481\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.3336 - val_loss: 0.3412\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.3284 - val_loss: 0.3410\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3277 - val_loss: 0.3403\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3264 - val_loss: 0.3383\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3260 - val_loss: 0.3385\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3258 - val_loss: 0.3385\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.3257 - val_loss: 0.3396\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3259 - val_loss: 0.3356\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 893us/step - loss: 0.3242 - val_loss: 0.3414\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3253 - val_loss: 0.3417\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3292 - val_loss: 0.3365\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3231 - val_loss: 0.3385\n",
      "121/121 [==============================] - 0s 633us/step - loss: 0.3129\n",
      "[CV] END learning_rate=0.008147426718418045, n_hidden=1, n_neurons=50; total time=  22.5s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ozing\\miniconda3\\envs\\handson-ml2\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 1.2254 - val_loss: 0.6631\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 10.4776 - val_loss: 0.8203\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.5497 - val_loss: 0.4827\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 930us/step - loss: 0.4387 - val_loss: 0.4418\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 930us/step - loss: 0.4094 - val_loss: 0.4256\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 926us/step - loss: 0.3968 - val_loss: 0.4172\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.3892 - val_loss: 0.4119\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.3835 - val_loss: 0.4068\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.3803 - val_loss: 0.4047\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.3769 - val_loss: 0.4058\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3738 - val_loss: 0.4031\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3718 - val_loss: 0.3987\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.3702 - val_loss: 0.3943\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 930us/step - loss: 0.3679 - val_loss: 0.3922\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3654 - val_loss: 0.3884\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.3633 - val_loss: 0.3887\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3614 - val_loss: 0.3870\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.3598 - val_loss: 0.3830\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.3581 - val_loss: 0.3853\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3575 - val_loss: 0.3810\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3562 - val_loss: 0.3788\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.3534 - val_loss: 0.3855\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.3529 - val_loss: 0.3776\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.3518 - val_loss: 0.3755\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3512 - val_loss: 0.3752\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3500 - val_loss: 0.3789\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 0.3498 - val_loss: 0.3761\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.3487 - val_loss: 0.3740\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.3473 - val_loss: 0.3764\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3475 - val_loss: 0.3706\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3462 - val_loss: 0.3684\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3445 - val_loss: 0.3712\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3430 - val_loss: 0.3686\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3422 - val_loss: 0.3680\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.3412 - val_loss: 0.3671\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3405 - val_loss: 0.3637\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.3402 - val_loss: 0.3637\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3396 - val_loss: 0.3658\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3389 - val_loss: 0.3640\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3377 - val_loss: 0.3601\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.3376 - val_loss: 0.3603\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.3372 - val_loss: 0.3616\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3354 - val_loss: 0.3583\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3344 - val_loss: 0.3605\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3336 - val_loss: 0.3558\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.3331 - val_loss: 0.3553\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 906us/step - loss: 0.3320 - val_loss: 0.3563\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.3307 - val_loss: 0.3563\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3313 - val_loss: 0.3564\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.3301 - val_loss: 0.3551\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.3296 - val_loss: 0.3597\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.3281 - val_loss: 0.3618\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3298 - val_loss: 0.3532\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.3279 - val_loss: 0.3559\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.3278 - val_loss: 0.3510\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.3268 - val_loss: 0.3500\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3251 - val_loss: 0.3508\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3253 - val_loss: 0.3609\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3262 - val_loss: 0.3481\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 907us/step - loss: 0.3255 - val_loss: 0.3465\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3235 - val_loss: 0.3469\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3229 - val_loss: 0.3503\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 930us/step - loss: 0.3228 - val_loss: 0.3466\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.3238 - val_loss: 0.3473\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.3217 - val_loss: 0.3473\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3196 - val_loss: 0.3534\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 0.3198 - val_loss: 0.3497\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.3195 - val_loss: 0.3455\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 953us/step - loss: 0.3180 - val_loss: 0.3454\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3182 - val_loss: 0.3498\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.3191 - val_loss: 0.3426\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 943us/step - loss: 0.3203 - val_loss: 0.3398\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3193 - val_loss: 0.3417\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.3176 - val_loss: 0.3409\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.3177 - val_loss: 0.3383\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.3155 - val_loss: 0.3390\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.3165 - val_loss: 0.3388\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3156 - val_loss: 0.3432\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.3157 - val_loss: 0.3419\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.3144 - val_loss: 0.3373\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.3151 - val_loss: 0.3389\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.3162 - val_loss: 0.3427\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3134 - val_loss: 0.3437\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.3133 - val_loss: 0.3368\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 930us/step - loss: 0.3120 - val_loss: 0.3436\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.3114 - val_loss: 0.3425\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3133 - val_loss: 0.3383\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3119 - val_loss: 0.3363\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.3114 - val_loss: 0.3360\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3110 - val_loss: 0.3399\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3113 - val_loss: 0.3403\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3113 - val_loss: 0.3390\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.3126 - val_loss: 0.3361\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 928us/step - loss: 0.3101 - val_loss: 0.3352\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 899us/step - loss: 0.3079 - val_loss: 0.3419\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.3083 - val_loss: 0.3364\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3101 - val_loss: 0.3354\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3086 - val_loss: 0.3347\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.3072 - val_loss: 0.3346\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 898us/step - loss: 0.3064 - val_loss: 0.3407\n",
      "121/121 [==============================] - 0s 600us/step - loss: 0.3869\n",
      "[CV] END learning_rate=0.008147426718418045, n_hidden=1, n_neurons=50; total time=  22.6s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ozing\\miniconda3\\envs\\handson-ml2\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 1ms/step - loss: 2.2981 - val_loss: 1.2506\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: 1.1052 - val_loss: 0.8879\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: 0.8706 - val_loss: 0.7981\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.8044 - val_loss: 0.7583\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.7685 - val_loss: 0.7298\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.7404 - val_loss: 0.7063\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.7162 - val_loss: 0.6848\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.6944 - val_loss: 0.6656\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 930us/step - loss: 0.6748 - val_loss: 0.6482\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.6572 - val_loss: 0.6325\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.6410 - val_loss: 0.6184\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 914us/step - loss: 0.6262 - val_loss: 0.6054\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 930us/step - loss: 0.6127 - val_loss: 0.5936\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6006 - val_loss: 0.5829\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.5893 - val_loss: 0.5731\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.5793 - val_loss: 0.5648\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 930us/step - loss: 0.5699 - val_loss: 0.5568\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 914us/step - loss: 0.5614 - val_loss: 0.5493\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.5538 - val_loss: 0.5427\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.5467 - val_loss: 0.5363\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.5407 - val_loss: 0.5310\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 983us/step - loss: 0.5345 - val_loss: 0.5259\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.5293 - val_loss: 0.5213\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5242 - val_loss: 0.5168\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5197 - val_loss: 0.5134\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: 0.5154 - val_loss: 0.5098\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 930us/step - loss: 0.5113 - val_loss: 0.5059\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.5075 - val_loss: 0.5024\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.5042 - val_loss: 0.4997\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.5007 - val_loss: 0.4966\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4979 - val_loss: 0.4941\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4948 - val_loss: 0.4917\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4917 - val_loss: 0.4890\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.4892 - val_loss: 0.4865\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4870 - val_loss: 0.4845\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4846 - val_loss: 0.4827\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 899us/step - loss: 0.4821 - val_loss: 0.4807\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4797 - val_loss: 0.4785\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.4780 - val_loss: 0.4768\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.4758 - val_loss: 0.4752\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4739 - val_loss: 0.4735\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.4721 - val_loss: 0.4723\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4701 - val_loss: 0.4704\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4686 - val_loss: 0.4689\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: 0.4669 - val_loss: 0.4679\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4651 - val_loss: 0.4663\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 930us/step - loss: 0.4636 - val_loss: 0.4649\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.4624 - val_loss: 0.4643\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.4606 - val_loss: 0.4624\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.4593 - val_loss: 0.4618\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: 0.4580 - val_loss: 0.4601\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.4566 - val_loss: 0.4594\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 926us/step - loss: 0.4552 - val_loss: 0.4580\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.4540 - val_loss: 0.4574\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4526 - val_loss: 0.4560\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.4515 - val_loss: 0.4549\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4504 - val_loss: 0.4540\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4492 - val_loss: 0.4531\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: 0.4480 - val_loss: 0.4522\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.4470 - val_loss: 0.4512\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4457 - val_loss: 0.4505\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.4450 - val_loss: 0.4497\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.4438 - val_loss: 0.4486\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4429 - val_loss: 0.4481\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4420 - val_loss: 0.4474\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4410 - val_loss: 0.4463\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.4401 - val_loss: 0.4457\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.4391 - val_loss: 0.4451\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4383 - val_loss: 0.4445\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4375 - val_loss: 0.4435\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 907us/step - loss: 0.4367 - val_loss: 0.4430\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4359 - val_loss: 0.4426\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4351 - val_loss: 0.4419\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.4343 - val_loss: 0.4413\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4337 - val_loss: 0.4409\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4329 - val_loss: 0.4401\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.4323 - val_loss: 0.4396\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4314 - val_loss: 0.4391\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4309 - val_loss: 0.4384\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.4301 - val_loss: 0.4380\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.4295 - val_loss: 0.4373\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4287 - val_loss: 0.4371\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4283 - val_loss: 0.4363\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4273 - val_loss: 0.4361\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.4270 - val_loss: 0.4353\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4263 - val_loss: 0.4350\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.4258 - val_loss: 0.4346\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4251 - val_loss: 0.4339\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.4245 - val_loss: 0.4334\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 930us/step - loss: 0.4239 - val_loss: 0.4331\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4234 - val_loss: 0.4323\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4228 - val_loss: 0.4320\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4222 - val_loss: 0.4316\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.4217 - val_loss: 0.4313\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.4211 - val_loss: 0.4309\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4206 - val_loss: 0.4305\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4200 - val_loss: 0.4301\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.4195 - val_loss: 0.4294\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: 0.4189 - val_loss: 0.4292\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.4184 - val_loss: 0.4287\n",
      "121/121 [==============================] - 0s 575us/step - loss: 0.4231\n",
      "[CV] END learning_rate=0.0008173465957855727, n_hidden=1, n_neurons=94; total time=  23.0s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ozing\\miniconda3\\envs\\handson-ml2\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 2.8013 - val_loss: 1.1452\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.9109 - val_loss: 0.7803\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.7572 - val_loss: 0.7272\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.7212 - val_loss: 0.7024\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.6974 - val_loss: 0.6820\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: 0.6777 - val_loss: 0.6644\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 984us/step - loss: 0.6602 - val_loss: 0.6480\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.6444 - val_loss: 0.6332\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.6302 - val_loss: 0.6197\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.6172 - val_loss: 0.6075\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.6054 - val_loss: 0.5961\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.5946 - val_loss: 0.5855\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.5846 - val_loss: 0.5761\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.5756 - val_loss: 0.5671\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.5670 - val_loss: 0.5594\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: 0.5595 - val_loss: 0.5522\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 907us/step - loss: 0.5524 - val_loss: 0.5453\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 940us/step - loss: 0.5460 - val_loss: 0.5389\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.5400 - val_loss: 0.5337\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.5350 - val_loss: 0.5284\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.5301 - val_loss: 0.5236\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 943us/step - loss: 0.5254 - val_loss: 0.5193\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.5214 - val_loss: 0.5157\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.5177 - val_loss: 0.5122\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.5141 - val_loss: 0.5082\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5109 - val_loss: 0.5057\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.5079 - val_loss: 0.5025\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.5053 - val_loss: 0.4998\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.5027 - val_loss: 0.4972\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 936us/step - loss: 0.5006 - val_loss: 0.4949\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4984 - val_loss: 0.4928\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.4964 - val_loss: 0.4908\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4946 - val_loss: 0.4889\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.4926 - val_loss: 0.4872\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4910 - val_loss: 0.4861\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 910us/step - loss: 0.4893 - val_loss: 0.4841\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.4877 - val_loss: 0.4824\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.4862 - val_loss: 0.4810\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4849 - val_loss: 0.4797\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 926us/step - loss: 0.4835 - val_loss: 0.4784\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.4821 - val_loss: 0.4769\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.4809 - val_loss: 0.4754\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 930us/step - loss: 0.4796 - val_loss: 0.4746\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 907us/step - loss: 0.4786 - val_loss: 0.4735\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.4774 - val_loss: 0.4722\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4763 - val_loss: 0.4711\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4753 - val_loss: 0.4699\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.4742 - val_loss: 0.4688\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4731 - val_loss: 0.4682\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.4722 - val_loss: 0.4672\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 930us/step - loss: 0.4711 - val_loss: 0.4664\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.4702 - val_loss: 0.4650\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4693 - val_loss: 0.4642\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.4684 - val_loss: 0.4634\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4675 - val_loss: 0.4627\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.4667 - val_loss: 0.4620\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4658 - val_loss: 0.4609\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.4649 - val_loss: 0.4601\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4641 - val_loss: 0.4597\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.4632 - val_loss: 0.4590\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4625 - val_loss: 0.4580\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4616 - val_loss: 0.4572\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.4608 - val_loss: 0.4567\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.4600 - val_loss: 0.4558\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.4593 - val_loss: 0.4549\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.4584 - val_loss: 0.4540\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4579 - val_loss: 0.4535\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4569 - val_loss: 0.4532\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: 0.4563 - val_loss: 0.4524\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 907us/step - loss: 0.4555 - val_loss: 0.4519\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4549 - val_loss: 0.4509\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.4539 - val_loss: 0.4505\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.4534 - val_loss: 0.4496\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 0.4528 - val_loss: 0.4487\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 984us/step - loss: 0.4521 - val_loss: 0.4481\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4514 - val_loss: 0.4476\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.4508 - val_loss: 0.4470\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: 0.4500 - val_loss: 0.4468\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4495 - val_loss: 0.4458\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4488 - val_loss: 0.4457\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.4481 - val_loss: 0.4449\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 907us/step - loss: 0.4475 - val_loss: 0.4439\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.4469 - val_loss: 0.4434\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.4463 - val_loss: 0.4430\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.4456 - val_loss: 0.4425\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 910us/step - loss: 0.4450 - val_loss: 0.4423\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 904us/step - loss: 0.4444 - val_loss: 0.4413\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 991us/step - loss: 0.4439 - val_loss: 0.4411\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.4432 - val_loss: 0.4403\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4427 - val_loss: 0.4399\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4421 - val_loss: 0.4395\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4414 - val_loss: 0.4388\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.4408 - val_loss: 0.4385\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.4401 - val_loss: 0.4382\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.4400 - val_loss: 0.4374\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.4392 - val_loss: 0.4368\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4384 - val_loss: 0.4364\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 930us/step - loss: 0.4382 - val_loss: 0.4359\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4376 - val_loss: 0.4354\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4370 - val_loss: 0.4346\n",
      "121/121 [==============================] - 0s 600us/step - loss: 0.4102\n",
      "[CV] END learning_rate=0.0008173465957855727, n_hidden=1, n_neurons=94; total time=  22.9s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ozing\\miniconda3\\envs\\handson-ml2\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 2.3578 - val_loss: 1.3118\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 1.0214 - val_loss: 0.8666\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: 0.7867 - val_loss: 0.7574\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.7140 - val_loss: 0.7114\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.6788 - val_loss: 0.6850\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.6547 - val_loss: 0.6648\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 944us/step - loss: 0.6350 - val_loss: 0.6464\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 902us/step - loss: 0.6185 - val_loss: 0.6312\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 907us/step - loss: 0.6031 - val_loss: 0.6168\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.5892 - val_loss: 0.6039\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.5773 - val_loss: 0.5922\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 897us/step - loss: 0.5656 - val_loss: 0.5814\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 911us/step - loss: 0.5557 - val_loss: 0.5712\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.5465 - val_loss: 0.5623\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 937us/step - loss: 0.5381 - val_loss: 0.5541\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.5304 - val_loss: 0.5463\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.5237 - val_loss: 0.5401\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 904us/step - loss: 0.5173 - val_loss: 0.5341\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 922us/step - loss: 0.5114 - val_loss: 0.5277\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.5064 - val_loss: 0.5227\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.5015 - val_loss: 0.5180\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4969 - val_loss: 0.5137\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.4930 - val_loss: 0.5096\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4892 - val_loss: 0.5061\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4857 - val_loss: 0.5024\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 908us/step - loss: 0.4825 - val_loss: 0.4993\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 0.4797 - val_loss: 0.4962\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.4767 - val_loss: 0.4934\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 914us/step - loss: 0.4742 - val_loss: 0.4906\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.4718 - val_loss: 0.4885\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.4695 - val_loss: 0.4860\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4672 - val_loss: 0.4839\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 984us/step - loss: 0.4653 - val_loss: 0.4821\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4632 - val_loss: 0.4800\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 0.4614 - val_loss: 0.4786\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.4598 - val_loss: 0.4769\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.4581 - val_loss: 0.4752\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 980us/step - loss: 0.4566 - val_loss: 0.4735\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.4550 - val_loss: 0.4724\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4537 - val_loss: 0.4708\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.4522 - val_loss: 0.4692\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4509 - val_loss: 0.4680\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4495 - val_loss: 0.4670\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4482 - val_loss: 0.4652\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4471 - val_loss: 0.4641\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.4458 - val_loss: 0.4630\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4447 - val_loss: 0.4620\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.4436 - val_loss: 0.4608\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.4425 - val_loss: 0.4598\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4415 - val_loss: 0.4590\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.4404 - val_loss: 0.4580\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4394 - val_loss: 0.4571\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4384 - val_loss: 0.4559\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4375 - val_loss: 0.4549\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.4365 - val_loss: 0.4541\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4354 - val_loss: 0.4533\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4346 - val_loss: 0.4525\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.4336 - val_loss: 0.4519\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.4326 - val_loss: 0.4510\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4319 - val_loss: 0.4500\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4309 - val_loss: 0.4492\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.4301 - val_loss: 0.4483\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4294 - val_loss: 0.4476\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 897us/step - loss: 0.4285 - val_loss: 0.4469\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4277 - val_loss: 0.4462\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4270 - val_loss: 0.4452\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.4262 - val_loss: 0.4446\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.4253 - val_loss: 0.4444\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4248 - val_loss: 0.4433\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4240 - val_loss: 0.4424\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4232 - val_loss: 0.4421\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.4225 - val_loss: 0.4410\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 930us/step - loss: 0.4217 - val_loss: 0.4403\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4210 - val_loss: 0.4400\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4202 - val_loss: 0.4393\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 914us/step - loss: 0.4195 - val_loss: 0.4383\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4189 - val_loss: 0.4381\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4181 - val_loss: 0.4374\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4175 - val_loss: 0.4368\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.4169 - val_loss: 0.4362\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4162 - val_loss: 0.4356\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: 0.4155 - val_loss: 0.4351\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4149 - val_loss: 0.4343\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4142 - val_loss: 0.4341\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.4135 - val_loss: 0.4332\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.4130 - val_loss: 0.4328\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 930us/step - loss: 0.4123 - val_loss: 0.4326\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.4119 - val_loss: 0.4320\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 926us/step - loss: 0.4111 - val_loss: 0.4313\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.4107 - val_loss: 0.4311\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4099 - val_loss: 0.4303\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.4095 - val_loss: 0.4296\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.4089 - val_loss: 0.4292\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4083 - val_loss: 0.4287\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.4078 - val_loss: 0.4282\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: 0.4072 - val_loss: 0.4281\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4067 - val_loss: 0.4279\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: 0.4061 - val_loss: 0.4273\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.4057 - val_loss: 0.4267\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4051 - val_loss: 0.4266\n",
      "121/121 [==============================] - 0s 633us/step - loss: 0.4456\n",
      "[CV] END learning_rate=0.0008173465957855727, n_hidden=1, n_neurons=94; total time=  22.8s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ozing\\miniconda3\\envs\\handson-ml2\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 1.5168 - val_loss: 0.7026\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.6848 - val_loss: 0.6357\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.6294 - val_loss: 0.5952\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 930us/step - loss: 0.5875 - val_loss: 0.5655\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.5636 - val_loss: 0.5443\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.5439 - val_loss: 0.5293\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.5278 - val_loss: 0.5180\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.5159 - val_loss: 0.5065\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.5075 - val_loss: 0.5001\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4989 - val_loss: 0.4935\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4927 - val_loss: 0.4859\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4847 - val_loss: 0.4801\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4804 - val_loss: 0.4765\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4732 - val_loss: 0.4722\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4697 - val_loss: 0.4690\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4660 - val_loss: 0.4646\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4638 - val_loss: 0.4633\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4610 - val_loss: 0.4623\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4572 - val_loss: 0.4567\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.4556 - val_loss: 0.4564\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4529 - val_loss: 0.4542\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4505 - val_loss: 0.4509\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4477 - val_loss: 0.4503\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4468 - val_loss: 0.4479\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4460 - val_loss: 0.4463\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4425 - val_loss: 0.4456\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.4407 - val_loss: 0.4437\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 926us/step - loss: 0.4388 - val_loss: 0.4416\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4370 - val_loss: 0.4409\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4368 - val_loss: 0.4394\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.4341 - val_loss: 0.4373\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4332 - val_loss: 0.4366\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 903us/step - loss: 0.4322 - val_loss: 0.4372\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 890us/step - loss: 0.4295 - val_loss: 0.4335\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4277 - val_loss: 0.4324\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 912us/step - loss: 0.4281 - val_loss: 0.4331\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4261 - val_loss: 0.4312\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4230 - val_loss: 0.4301\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4224 - val_loss: 0.4281\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4201 - val_loss: 0.4267\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4208 - val_loss: 0.4265\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4177 - val_loss: 0.4268\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: 0.4210 - val_loss: 0.4240\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 926us/step - loss: 0.4181 - val_loss: 0.4236\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 911us/step - loss: 0.4145 - val_loss: 0.4222\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 895us/step - loss: 0.4128 - val_loss: 0.4221\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4121 - val_loss: 0.4201\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4105 - val_loss: 0.4211\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.4098 - val_loss: 0.4179\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4090 - val_loss: 0.4169\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4069 - val_loss: 0.4173\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4069 - val_loss: 0.4155\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4056 - val_loss: 0.4140\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4040 - val_loss: 0.4134\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: 0.4029 - val_loss: 0.4135\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 906us/step - loss: 0.4023 - val_loss: 0.4119\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4009 - val_loss: 0.4106\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3997 - val_loss: 0.4100\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3999 - val_loss: 0.4087\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.3981 - val_loss: 0.4075\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3973 - val_loss: 0.4069\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 910us/step - loss: 0.3966 - val_loss: 0.4066\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3953 - val_loss: 0.4051\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3948 - val_loss: 0.4050\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.3927 - val_loss: 0.4046\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.3930 - val_loss: 0.4029\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 912us/step - loss: 0.3923 - val_loss: 0.4029\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.3902 - val_loss: 0.4034\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 930us/step - loss: 0.3912 - val_loss: 0.4006\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.3891 - val_loss: 0.4004\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3923 - val_loss: 0.3983\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 904us/step - loss: 0.3883 - val_loss: 0.3983\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.3917 - val_loss: 0.3986\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3868 - val_loss: 0.3972\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.3886 - val_loss: 0.3980\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.3884 - val_loss: 0.3962\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: 0.3838 - val_loss: 0.3958\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3839 - val_loss: 0.3941\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.3841 - val_loss: 0.3941\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3814 - val_loss: 0.3934\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3812 - val_loss: 0.3946\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3808 - val_loss: 0.3922\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 930us/step - loss: 0.3799 - val_loss: 0.3919\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.3799 - val_loss: 0.3925\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.3790 - val_loss: 0.3911\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3788 - val_loss: 0.3902\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3772 - val_loss: 0.3915\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3778 - val_loss: 0.3896\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3763 - val_loss: 0.3902\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3761 - val_loss: 0.3890\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.3762 - val_loss: 0.3885\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.3751 - val_loss: 0.3879\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3759 - val_loss: 0.3877\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3738 - val_loss: 0.3876\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.3739 - val_loss: 0.3877\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 904us/step - loss: 0.3730 - val_loss: 0.3857\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 912us/step - loss: 0.3736 - val_loss: 0.3880\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3718 - val_loss: 0.3856\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: 0.3751 - val_loss: 0.3857\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.3710 - val_loss: 0.3855\n",
      "121/121 [==============================] - 0s 592us/step - loss: 0.3812\n",
      "[CV] END learning_rate=0.0030576336442206574, n_hidden=1, n_neurons=28; total time=  22.5s\n",
      "Epoch 1/100\n",
      "  1/242 [..............................] - ETA: 40s - loss: 2.8551"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ozing\\miniconda3\\envs\\handson-ml2\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3372 - val_loss: 0.7499\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.7427 - val_loss: 0.6764\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.6688 - val_loss: 0.6257\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.6199 - val_loss: 0.5878\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.5860 - val_loss: 0.5625\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.5595 - val_loss: 0.5341\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.5377 - val_loss: 0.5185\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.5223 - val_loss: 0.5103\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.5101 - val_loss: 0.4945\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.5031 - val_loss: 0.4890\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.4957 - val_loss: 0.4802\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.4879 - val_loss: 0.4765\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.4842 - val_loss: 0.4773\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.4794 - val_loss: 0.4665\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.4780 - val_loss: 0.4630\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.4720 - val_loss: 0.4612\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.4676 - val_loss: 0.4585\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.4626 - val_loss: 0.4561\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.4597 - val_loss: 0.4534\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.4570 - val_loss: 0.4517\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: 0.4564 - val_loss: 0.4502\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 952us/step - loss: 0.4529 - val_loss: 0.4506\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 914us/step - loss: 0.4498 - val_loss: 0.4477\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 966us/step - loss: 0.4482 - val_loss: 0.4462\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.4458 - val_loss: 0.4451\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.4433 - val_loss: 0.4517\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.4429 - val_loss: 0.4423\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.4415 - val_loss: 0.4405\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.4386 - val_loss: 0.4389\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4375 - val_loss: 0.4385\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.4364 - val_loss: 0.4370\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.4342 - val_loss: 0.4388\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.4333 - val_loss: 0.4342\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.4308 - val_loss: 0.4353\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.4315 - val_loss: 0.4335\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.4285 - val_loss: 0.4328\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.4272 - val_loss: 0.4311\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.4274 - val_loss: 0.4317\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.4249 - val_loss: 0.4306\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.4245 - val_loss: 0.4277\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.4226 - val_loss: 0.4281\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.4210 - val_loss: 0.4275\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.4198 - val_loss: 0.4270\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.4193 - val_loss: 0.4245\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4174 - val_loss: 0.4237\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.4165 - val_loss: 0.4225\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 984us/step - loss: 0.4156 - val_loss: 0.4236\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.4144 - val_loss: 0.4234\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.4134 - val_loss: 0.4230\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.4139 - val_loss: 0.4196\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.4119 - val_loss: 0.4187\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.4103 - val_loss: 0.4180\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.4108 - val_loss: 0.4178\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.4084 - val_loss: 0.4177\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.4089 - val_loss: 0.4161\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.4065 - val_loss: 0.4156\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 984us/step - loss: 0.4064 - val_loss: 0.4151\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.4057 - val_loss: 0.4128\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.4046 - val_loss: 0.4145\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.4034 - val_loss: 0.4119\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.4047 - val_loss: 0.4103\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.4011 - val_loss: 0.4107\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.4010 - val_loss: 0.4131\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.4007 - val_loss: 0.4087\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.3987 - val_loss: 0.4118\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.3989 - val_loss: 0.4082\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.3994 - val_loss: 0.4066\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.3966 - val_loss: 0.4072\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.3967 - val_loss: 0.4086\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.3954 - val_loss: 0.4046\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 984us/step - loss: 0.3947 - val_loss: 0.4069\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.3947 - val_loss: 0.4040\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3924 - val_loss: 0.4052\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.3922 - val_loss: 0.4018\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.3917 - val_loss: 0.4028\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.3908 - val_loss: 0.4032\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.3910 - val_loss: 0.4048\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.3894 - val_loss: 0.4001\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.3875 - val_loss: 0.4015\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.3881 - val_loss: 0.3981\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.3884 - val_loss: 0.3973\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: 0.3866 - val_loss: 0.3989\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3859 - val_loss: 0.3972\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3854 - val_loss: 0.4077\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.3880 - val_loss: 0.3960\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3860 - val_loss: 0.3956\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 947us/step - loss: 0.3834 - val_loss: 0.3943\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3823 - val_loss: 0.3945\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 990us/step - loss: 0.3817 - val_loss: 0.3931\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.3820 - val_loss: 0.3945\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 995us/step - loss: 0.3815 - val_loss: 0.3949\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3815 - val_loss: 0.3924\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.3804 - val_loss: 0.3950\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.3811 - val_loss: 0.3918\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.3815 - val_loss: 0.3905\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.3795 - val_loss: 0.3904\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.3787 - val_loss: 0.3946\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.3779 - val_loss: 0.3891\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.3779 - val_loss: 0.3885\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.3771 - val_loss: 0.3877\n",
      "121/121 [==============================] - 0s 600us/step - loss: 0.3576\n",
      "[CV] END learning_rate=0.0030576336442206574, n_hidden=1, n_neurons=28; total time=  23.6s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ozing\\miniconda3\\envs\\handson-ml2\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 1.7925 - val_loss: 0.7608\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.7257 - val_loss: 0.6974\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.8026 - val_loss: 0.6284\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 1.6115 - val_loss: 0.7044\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.7240 - val_loss: 0.6330\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.5941 - val_loss: 0.6049\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.5671 - val_loss: 0.5794\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 974us/step - loss: 0.5469 - val_loss: 0.5613\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.5311 - val_loss: 0.5455\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 943us/step - loss: 0.5191 - val_loss: 0.5354\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 981us/step - loss: 0.5091 - val_loss: 0.5255\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 958us/step - loss: 0.5008 - val_loss: 0.5167\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.4936 - val_loss: 0.5099\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 984us/step - loss: 0.4873 - val_loss: 0.5037\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.4815 - val_loss: 0.5000\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.4769 - val_loss: 0.4942\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.4720 - val_loss: 0.4902\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4678 - val_loss: 0.4864\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.4631 - val_loss: 0.4834\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.4596 - val_loss: 0.4798\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.4562 - val_loss: 0.4771\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.4526 - val_loss: 0.4731\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.4494 - val_loss: 0.4710\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.4464 - val_loss: 0.4677\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.4432 - val_loss: 0.4659\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.4407 - val_loss: 0.4639\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.4381 - val_loss: 0.4603\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.4351 - val_loss: 0.4600\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4325 - val_loss: 0.4569\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.4307 - val_loss: 0.4546\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4283 - val_loss: 0.4533\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.4264 - val_loss: 0.4512\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.4241 - val_loss: 0.4503\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.4226 - val_loss: 0.4488\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.4212 - val_loss: 0.4468\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.4195 - val_loss: 0.4459\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.4177 - val_loss: 0.4453\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.4167 - val_loss: 0.4439\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.4147 - val_loss: 0.4437\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.4141 - val_loss: 0.4425\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.4123 - val_loss: 0.4402\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.4113 - val_loss: 0.4397\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.4100 - val_loss: 0.4397\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.4094 - val_loss: 0.4374\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.4081 - val_loss: 0.4360\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.4066 - val_loss: 0.4362\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.4057 - val_loss: 0.4356\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.4043 - val_loss: 0.4343\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.4036 - val_loss: 0.4329\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4026 - val_loss: 0.4323\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.4014 - val_loss: 0.4322\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.4007 - val_loss: 0.4313\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.3995 - val_loss: 0.4303\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.3983 - val_loss: 0.4302\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.3979 - val_loss: 0.4296\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.3969 - val_loss: 0.4294\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.3963 - val_loss: 0.4278\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.3950 - val_loss: 0.4269\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: 0.3946 - val_loss: 0.4262\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.3931 - val_loss: 0.4271\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.3926 - val_loss: 0.4247\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.3917 - val_loss: 0.4244\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.3910 - val_loss: 0.4232\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.3903 - val_loss: 0.4230\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3887 - val_loss: 0.4216\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.3885 - val_loss: 0.4214\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.3876 - val_loss: 0.4211\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.3869 - val_loss: 0.4196\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.3864 - val_loss: 0.4191\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.3855 - val_loss: 0.4192\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.3848 - val_loss: 0.4198\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3842 - val_loss: 0.4189\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: 0.3835 - val_loss: 0.4172\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.3827 - val_loss: 0.4166\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.3818 - val_loss: 0.4159\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.3815 - val_loss: 0.4168\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3804 - val_loss: 0.4148\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.3798 - val_loss: 0.4140\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 976us/step - loss: 0.3795 - val_loss: 0.4136\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.3788 - val_loss: 0.4126\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.3784 - val_loss: 0.4127\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.3774 - val_loss: 0.4122\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.3763 - val_loss: 0.4104\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.3762 - val_loss: 0.4104\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.3754 - val_loss: 0.4101\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.3749 - val_loss: 0.4104\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.3741 - val_loss: 0.4112\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.3736 - val_loss: 0.4084\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 954us/step - loss: 0.3732 - val_loss: 0.4079\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 962us/step - loss: 0.3727 - val_loss: 0.4073\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.3720 - val_loss: 0.4065\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.3715 - val_loss: 0.4057\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.3712 - val_loss: 0.4062\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.3703 - val_loss: 0.4045\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.3697 - val_loss: 0.4052\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.3691 - val_loss: 0.4039\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3684 - val_loss: 0.4052\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.3684 - val_loss: 0.4028\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3677 - val_loss: 0.4020\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3669 - val_loss: 0.4017\n",
      "121/121 [==============================] - 0s 725us/step - loss: 0.4063\n",
      "[CV] END learning_rate=0.0030576336442206574, n_hidden=1, n_neurons=28; total time=  23.7s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ozing\\miniconda3\\envs\\handson-ml2\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 3.4470 - val_loss: 0.7204\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.7379 - val_loss: 0.5948\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: 0.6063 - val_loss: 0.5673\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.5998 - val_loss: 0.5612\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.5627 - val_loss: 0.5530\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.5554 - val_loss: 0.5474\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.5502 - val_loss: 0.5423\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.5484 - val_loss: 0.5400\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.5484 - val_loss: 0.5396\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.5472 - val_loss: 0.5384\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.5442 - val_loss: 0.5364\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.5410 - val_loss: 0.5329\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.5443 - val_loss: 0.5328\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.5430 - val_loss: 0.5321\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 936us/step - loss: 0.5444 - val_loss: 0.5365\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.5361 - val_loss: 0.5304\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.5421 - val_loss: 0.5301\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.5452 - val_loss: 0.5303\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.5417 - val_loss: 0.5313\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.5361 - val_loss: 0.5289\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.5476 - val_loss: 0.5351\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.5410 - val_loss: 0.5305\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 916us/step - loss: 0.5421 - val_loss: 0.5321\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.5393 - val_loss: 0.5295\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.5403 - val_loss: 0.5332\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.5377 - val_loss: 0.5296\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.5426 - val_loss: 0.5305\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.5425 - val_loss: 0.5318\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.5365 - val_loss: 0.5296\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 962us/step - loss: 0.5394 - val_loss: 0.5545\n",
      "121/121 [==============================] - 0s 567us/step - loss: 0.5575\n",
      "[CV] END learning_rate=0.0039002518911171397, n_hidden=0, n_neurons=16; total time=   7.0s\n",
      "Epoch 1/100\n",
      "  1/242 [..............................] - ETA: 38s - loss: 6.1282"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ozing\\miniconda3\\envs\\handson-ml2\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 2.0732 - val_loss: 0.8571\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.7658 - val_loss: 0.6989\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.6776 - val_loss: 0.6534\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.6415 - val_loss: 0.6240\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 984us/step - loss: 0.6178 - val_loss: 0.6020\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.6002 - val_loss: 0.5851\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.5883 - val_loss: 0.5727\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 980us/step - loss: 0.5770 - val_loss: 0.5628\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.5725 - val_loss: 0.5562\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.5659 - val_loss: 0.5524\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.5596 - val_loss: 0.5459\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.5593 - val_loss: 0.5480\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.5564 - val_loss: 0.5516\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.5537 - val_loss: 0.5374\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.5544 - val_loss: 0.5407\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.5522 - val_loss: 0.5356\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.5514 - val_loss: 0.5359\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.5517 - val_loss: 0.5376\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.5495 - val_loss: 0.5349\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.5490 - val_loss: 0.5330\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.5479 - val_loss: 0.5313\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 958us/step - loss: 0.5512 - val_loss: 0.5321\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: 0.5503 - val_loss: 0.5337\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.5474 - val_loss: 0.5321\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.5481 - val_loss: 0.5325\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.5478 - val_loss: 0.5505\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.5511 - val_loss: 0.5336\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.5465 - val_loss: 0.5381\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.5444 - val_loss: 0.5300\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.5480 - val_loss: 0.5452\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.5490 - val_loss: 0.5410\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.5440 - val_loss: 0.5303\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.5487 - val_loss: 0.5299\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.5482 - val_loss: 0.5329\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.5482 - val_loss: 0.5319\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.5442 - val_loss: 0.5453\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.5480 - val_loss: 0.5311\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.5441 - val_loss: 0.5291\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.5496 - val_loss: 0.5297\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.5510 - val_loss: 0.5311\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.5488 - val_loss: 0.5310\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.5444 - val_loss: 0.5307\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.5480 - val_loss: 0.5294\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.5474 - val_loss: 0.5472\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.5489 - val_loss: 0.5320\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.5474 - val_loss: 0.5326\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.5466 - val_loss: 0.5337\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.5469 - val_loss: 0.5317\n",
      "121/121 [==============================] - 0s 600us/step - loss: 0.5126\n",
      "[CV] END learning_rate=0.0039002518911171397, n_hidden=0, n_neurons=16; total time=  11.4s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ozing\\miniconda3\\envs\\handson-ml2\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 2.5289 - val_loss: 0.7591\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 929us/step - loss: 1.6609 - val_loss: 0.6073\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 2.3431 - val_loss: 0.6819\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 930us/step - loss: 3.2871 - val_loss: 0.5996\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 5.1562 - val_loss: 0.8293\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 7.9515 - val_loss: 0.7623\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 12.9131 - val_loss: 1.0722\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 20.3232 - val_loss: 1.0452\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 32.9640 - val_loss: 2.1559\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 52.9708 - val_loss: 2.8107\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 87.7070 - val_loss: 3.7911\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 141.1941 - val_loss: 6.0107\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 231.6083 - val_loss: 9.1357\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 374.8872 - val_loss: 15.0171\n",
      "121/121 [==============================] - 0s 583us/step - loss: 292.5040\n",
      "[CV] END learning_rate=0.0039002518911171397, n_hidden=0, n_neurons=16; total time=   3.5s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ozing\\miniconda3\\envs\\handson-ml2\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 1ms/step - loss: 2.2112 - val_loss: 0.9839\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8327 - val_loss: 0.6984\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 983us/step - loss: 0.6772 - val_loss: 0.6270\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6149 - val_loss: 0.5866\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5756 - val_loss: 0.5600\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 984us/step - loss: 0.5485 - val_loss: 0.5393\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.5292 - val_loss: 0.5255\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.5157 - val_loss: 0.5176\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.5059 - val_loss: 0.5066\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.4972 - val_loss: 0.4996\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.4901 - val_loss: 0.4940\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 984us/step - loss: 0.4841 - val_loss: 0.4886\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.4786 - val_loss: 0.4841\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.4742 - val_loss: 0.4808\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.4695 - val_loss: 0.4770\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4657 - val_loss: 0.4741\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4626 - val_loss: 0.4698\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.4584 - val_loss: 0.4678\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4554 - val_loss: 0.4669\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4524 - val_loss: 0.4614\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.4497 - val_loss: 0.4595\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4470 - val_loss: 0.4562\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.4462 - val_loss: 0.4556\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.4423 - val_loss: 0.4521\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4409 - val_loss: 0.4509\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4373 - val_loss: 0.4483\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 984us/step - loss: 0.4348 - val_loss: 0.4469\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4324 - val_loss: 0.4444\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4301 - val_loss: 0.4435\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4288 - val_loss: 0.4418\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4266 - val_loss: 0.4396\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4246 - val_loss: 0.4411\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4226 - val_loss: 0.4376\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4215 - val_loss: 0.4351\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4191 - val_loss: 0.4341\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.4177 - val_loss: 0.4345\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4167 - val_loss: 0.4315\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4153 - val_loss: 0.4338\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.4136 - val_loss: 0.4306\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4120 - val_loss: 0.4278\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4108 - val_loss: 0.4295\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.4091 - val_loss: 0.4270\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.4085 - val_loss: 0.4253\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.4074 - val_loss: 0.4238\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.4054 - val_loss: 0.4231\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4045 - val_loss: 0.4223\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.4038 - val_loss: 0.4211\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4026 - val_loss: 0.4199\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4018 - val_loss: 0.4190\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3999 - val_loss: 0.4180\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3996 - val_loss: 0.4166\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3979 - val_loss: 0.4167\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3969 - val_loss: 0.4171\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3960 - val_loss: 0.4159\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3949 - val_loss: 0.4133\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.3942 - val_loss: 0.4149\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3934 - val_loss: 0.4121\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3921 - val_loss: 0.4119\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.3911 - val_loss: 0.4108\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3892 - val_loss: 0.4106\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3897 - val_loss: 0.4083\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.3882 - val_loss: 0.4079\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 984us/step - loss: 0.3873 - val_loss: 0.4079\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.3858 - val_loss: 0.4059\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.3854 - val_loss: 0.4060\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3850 - val_loss: 0.4061\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.3841 - val_loss: 0.4041\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.3829 - val_loss: 0.4025\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3817 - val_loss: 0.4027\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.3814 - val_loss: 0.4021\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3806 - val_loss: 0.4011\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 997us/step - loss: 0.3798 - val_loss: 0.3991\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.3786 - val_loss: 0.3984\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3777 - val_loss: 0.3982\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.3770 - val_loss: 0.3971\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3761 - val_loss: 0.3960\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.3752 - val_loss: 0.3965\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3746 - val_loss: 0.3951\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3747 - val_loss: 0.3939\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3731 - val_loss: 0.3961\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3727 - val_loss: 0.3928\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3711 - val_loss: 0.3925\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.3708 - val_loss: 0.3919\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.3698 - val_loss: 0.3908\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.3709 - val_loss: 0.3904\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3697 - val_loss: 0.3923\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3681 - val_loss: 0.3898\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 984us/step - loss: 0.3680 - val_loss: 0.3895\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3665 - val_loss: 0.3892\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3665 - val_loss: 0.3881\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 984us/step - loss: 0.3662 - val_loss: 0.3881\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3651 - val_loss: 0.3885\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 972us/step - loss: 0.3648 - val_loss: 0.3879\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 969us/step - loss: 0.3636 - val_loss: 0.3869\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3635 - val_loss: 0.3855\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.3639 - val_loss: 0.3866\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.3627 - val_loss: 0.3841\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3638 - val_loss: 0.3845\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3634 - val_loss: 0.3827\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3613 - val_loss: 0.3827\n",
      "121/121 [==============================] - 0s 625us/step - loss: 0.3743\n",
      "[CV] END learning_rate=0.0019515497708098523, n_hidden=2, n_neurons=18; total time=  24.8s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ozing\\miniconda3\\envs\\handson-ml2\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 1ms/step - loss: 1.8124 - val_loss: 0.8633\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8246 - val_loss: 0.7642\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7039 - val_loss: 0.6668\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6574 - val_loss: 0.6339\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6244 - val_loss: 0.6043\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 984us/step - loss: 0.5979 - val_loss: 0.5783\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.5765 - val_loss: 0.5584\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.5567 - val_loss: 0.5410\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5417 - val_loss: 0.5296\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5299 - val_loss: 0.5152\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5193 - val_loss: 0.5072\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5114 - val_loss: 0.4981\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5041 - val_loss: 0.4936\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.4974 - val_loss: 0.4871\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4926 - val_loss: 0.4814\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.4871 - val_loss: 0.4789\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4838 - val_loss: 0.4732\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4795 - val_loss: 0.4722\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4760 - val_loss: 0.4673\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4740 - val_loss: 0.4662\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4708 - val_loss: 0.4662\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4677 - val_loss: 0.4603\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4652 - val_loss: 0.4596\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4625 - val_loss: 0.4562\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4607 - val_loss: 0.4548\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 984us/step - loss: 0.4592 - val_loss: 0.4545\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4568 - val_loss: 0.4519\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.4544 - val_loss: 0.4499\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4524 - val_loss: 0.4493\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.4507 - val_loss: 0.4463\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4496 - val_loss: 0.4474\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4475 - val_loss: 0.4446\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4453 - val_loss: 0.4435\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4442 - val_loss: 0.4429\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4418 - val_loss: 0.4449\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4411 - val_loss: 0.4413\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.4394 - val_loss: 0.4397\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4376 - val_loss: 0.4381\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.4362 - val_loss: 0.4366\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4359 - val_loss: 0.4366\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 984us/step - loss: 0.4334 - val_loss: 0.4360\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4322 - val_loss: 0.4331\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4309 - val_loss: 0.4328\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.4291 - val_loss: 0.4354\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.4285 - val_loss: 0.4319\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4265 - val_loss: 0.4299\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4251 - val_loss: 0.4304\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4238 - val_loss: 0.4279\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4224 - val_loss: 0.4264\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4226 - val_loss: 0.4244\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4201 - val_loss: 0.4249\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4188 - val_loss: 0.4289\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4181 - val_loss: 0.4211\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.4164 - val_loss: 0.4205\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.4159 - val_loss: 0.4197\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4132 - val_loss: 0.4186\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4129 - val_loss: 0.4170\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 984us/step - loss: 0.4113 - val_loss: 0.4159\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4108 - val_loss: 0.4147\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4095 - val_loss: 0.4189\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4095 - val_loss: 0.4128\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4077 - val_loss: 0.4129\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4068 - val_loss: 0.4107\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4054 - val_loss: 0.4100\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.4047 - val_loss: 0.4099\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4035 - val_loss: 0.4088\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.4022 - val_loss: 0.4077\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4022 - val_loss: 0.4067\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3999 - val_loss: 0.4075\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.3995 - val_loss: 0.4046\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4003 - val_loss: 0.4045\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4003 - val_loss: 0.4076\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3993 - val_loss: 0.4048\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4011 - val_loss: 0.4021\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3950 - val_loss: 0.4022\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.3934 - val_loss: 0.3995\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3928 - val_loss: 0.3981\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3921 - val_loss: 0.3974\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3922 - val_loss: 0.3968\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3903 - val_loss: 0.3966\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3892 - val_loss: 0.3958\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3931 - val_loss: 0.3944\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.3874 - val_loss: 0.4006\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3872 - val_loss: 0.3951\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3866 - val_loss: 0.3947\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3852 - val_loss: 0.3910\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3852 - val_loss: 0.3915\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 894us/step - loss: 0.3848 - val_loss: 0.3913\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3849 - val_loss: 0.3899\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 886us/step - loss: 0.3809 - val_loss: 0.3884\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.3810 - val_loss: 0.3858\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.3804 - val_loss: 0.3893\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3791 - val_loss: 0.3853\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 908us/step - loss: 0.3789 - val_loss: 0.3838\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3764 - val_loss: 0.3826\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.3766 - val_loss: 0.3846\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.3765 - val_loss: 0.3819\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.3761 - val_loss: 0.3855\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.3743 - val_loss: 0.3819\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.3729 - val_loss: 0.3792\n",
      "121/121 [==============================] - 0s 567us/step - loss: 0.3506\n",
      "[CV] END learning_rate=0.0019515497708098523, n_hidden=2, n_neurons=18; total time=  24.1s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ozing\\miniconda3\\envs\\handson-ml2\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 1.4903 - val_loss: 0.8713\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.7244 - val_loss: 0.6795\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.6212 - val_loss: 0.6255\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.5804 - val_loss: 0.5962\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.5583 - val_loss: 0.5758\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 0.5403 - val_loss: 0.5591\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.5271 - val_loss: 0.5461\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.5152 - val_loss: 0.5359\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.5051 - val_loss: 0.5249\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4966 - val_loss: 0.5167\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4892 - val_loss: 0.5093\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4825 - val_loss: 0.5025\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4776 - val_loss: 0.4975\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4722 - val_loss: 0.4924\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4680 - val_loss: 0.4877\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4640 - val_loss: 0.4853\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4600 - val_loss: 0.4816\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4562 - val_loss: 0.4775\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4533 - val_loss: 0.4739\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4499 - val_loss: 0.4721\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4466 - val_loss: 0.4686\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4440 - val_loss: 0.4652\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4411 - val_loss: 0.4621\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 912us/step - loss: 0.4387 - val_loss: 0.4596\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4360 - val_loss: 0.4570\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4337 - val_loss: 0.4554\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 910us/step - loss: 0.4313 - val_loss: 0.4527\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4291 - val_loss: 0.4507\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 0.4273 - val_loss: 0.4487\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4245 - val_loss: 0.4473\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4235 - val_loss: 0.4463\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4217 - val_loss: 0.4431\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4199 - val_loss: 0.4415\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4176 - val_loss: 0.4413\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4159 - val_loss: 0.4395\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4136 - val_loss: 0.4395\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4125 - val_loss: 0.4366\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4111 - val_loss: 0.4360\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4098 - val_loss: 0.4334\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4085 - val_loss: 0.4325\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4068 - val_loss: 0.4305\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.4059 - val_loss: 0.4299\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 916us/step - loss: 0.4047 - val_loss: 0.4289\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4037 - val_loss: 0.4288\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4022 - val_loss: 0.4263\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4006 - val_loss: 0.4256\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.3998 - val_loss: 0.4257\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3985 - val_loss: 0.4233\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3973 - val_loss: 0.4221\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.3965 - val_loss: 0.4217\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3955 - val_loss: 0.4214\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3940 - val_loss: 0.4201\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3931 - val_loss: 0.4211\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 908us/step - loss: 0.3924 - val_loss: 0.4175\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3915 - val_loss: 0.4178\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.3905 - val_loss: 0.4158\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3891 - val_loss: 0.4155\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3882 - val_loss: 0.4140\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3869 - val_loss: 0.4144\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.3862 - val_loss: 0.4150\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3855 - val_loss: 0.4141\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3846 - val_loss: 0.4106\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.3831 - val_loss: 0.4105\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3828 - val_loss: 0.4092\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3817 - val_loss: 0.4084\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3810 - val_loss: 0.4071\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3800 - val_loss: 0.4061\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3793 - val_loss: 0.4073\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.3782 - val_loss: 0.4078\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.3773 - val_loss: 0.4053\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.3764 - val_loss: 0.4033\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3757 - val_loss: 0.4044\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.3749 - val_loss: 0.4021\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 899us/step - loss: 0.3740 - val_loss: 0.4027\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3737 - val_loss: 0.4008\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 908us/step - loss: 0.3719 - val_loss: 0.4012\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3718 - val_loss: 0.4019\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.3706 - val_loss: 0.3992\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.3705 - val_loss: 0.3989\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3693 - val_loss: 0.3971\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3690 - val_loss: 0.3972\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.3683 - val_loss: 0.3964\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 926us/step - loss: 0.3674 - val_loss: 0.3962\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3671 - val_loss: 0.3944\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.3661 - val_loss: 0.3958\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3655 - val_loss: 0.3939\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 906us/step - loss: 0.3652 - val_loss: 0.3921\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.3644 - val_loss: 0.3919\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3638 - val_loss: 0.3913\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.3628 - val_loss: 0.3924\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3620 - val_loss: 0.3907\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3615 - val_loss: 0.3893\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3612 - val_loss: 0.3905\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.3609 - val_loss: 0.3891\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3603 - val_loss: 0.3873\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3594 - val_loss: 0.3873\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3589 - val_loss: 0.3871\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3585 - val_loss: 0.3874\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.3580 - val_loss: 0.3860\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3575 - val_loss: 0.3860\n",
      "121/121 [==============================] - 0s 583us/step - loss: 0.4008\n",
      "[CV] END learning_rate=0.0019515497708098523, n_hidden=2, n_neurons=18; total time=  22.4s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ozing\\miniconda3\\envs\\handson-ml2\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 2.7659 - val_loss: 1.3093\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 1.1226 - val_loss: 0.8490\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.7990 - val_loss: 0.7421\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.7275 - val_loss: 0.6994\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.6939 - val_loss: 0.6721\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 882us/step - loss: 0.6694 - val_loss: 0.6507\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.6484 - val_loss: 0.6316\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.6305 - val_loss: 0.6156\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.6148 - val_loss: 0.6018\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.6006 - val_loss: 0.5888\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 879us/step - loss: 0.5886 - val_loss: 0.5781\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 886us/step - loss: 0.5773 - val_loss: 0.5682\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 895us/step - loss: 0.5675 - val_loss: 0.5593\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.5587 - val_loss: 0.5515\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.5507 - val_loss: 0.5442\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.5435 - val_loss: 0.5375\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.5371 - val_loss: 0.5316\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.5312 - val_loss: 0.5262\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.5259 - val_loss: 0.5215\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.5210 - val_loss: 0.5173\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.5167 - val_loss: 0.5133\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.5126 - val_loss: 0.5096\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 879us/step - loss: 0.5090 - val_loss: 0.5061\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.5056 - val_loss: 0.5032\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.5026 - val_loss: 0.5004\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.4997 - val_loss: 0.4978\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4970 - val_loss: 0.4954\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4946 - val_loss: 0.4931\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4921 - val_loss: 0.4909\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4901 - val_loss: 0.4893\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4879 - val_loss: 0.4872\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4861 - val_loss: 0.4854\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4842 - val_loss: 0.4839\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4824 - val_loss: 0.4821\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4809 - val_loss: 0.4808\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4793 - val_loss: 0.4794\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4778 - val_loss: 0.4777\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4765 - val_loss: 0.4765\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4750 - val_loss: 0.4754\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4738 - val_loss: 0.4741\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4725 - val_loss: 0.4730\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4712 - val_loss: 0.4720\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 883us/step - loss: 0.4699 - val_loss: 0.4708\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.4688 - val_loss: 0.4697\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4677 - val_loss: 0.4690\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.4667 - val_loss: 0.4678\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 890us/step - loss: 0.4656 - val_loss: 0.4667\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4644 - val_loss: 0.4656\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 883us/step - loss: 0.4635 - val_loss: 0.4649\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 883us/step - loss: 0.4623 - val_loss: 0.4639\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4614 - val_loss: 0.4629\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4605 - val_loss: 0.4621\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 879us/step - loss: 0.4595 - val_loss: 0.4612\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4586 - val_loss: 0.4607\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 877us/step - loss: 0.4576 - val_loss: 0.4595\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 883us/step - loss: 0.4569 - val_loss: 0.4588\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 885us/step - loss: 0.4560 - val_loss: 0.4581\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 881us/step - loss: 0.4551 - val_loss: 0.4574\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4544 - val_loss: 0.4567\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 867us/step - loss: 0.4535 - val_loss: 0.4560\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.4527 - val_loss: 0.4555\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4520 - val_loss: 0.4547\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4513 - val_loss: 0.4539\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.4505 - val_loss: 0.4535\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4498 - val_loss: 0.4525\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4490 - val_loss: 0.4518\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4483 - val_loss: 0.4513\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4477 - val_loss: 0.4505\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 891us/step - loss: 0.4469 - val_loss: 0.4501\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4462 - val_loss: 0.4492\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4457 - val_loss: 0.4487\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4449 - val_loss: 0.4482\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4442 - val_loss: 0.4475\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4435 - val_loss: 0.4470\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4429 - val_loss: 0.4465\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4423 - val_loss: 0.4459\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4416 - val_loss: 0.4454\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4409 - val_loss: 0.4449\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4403 - val_loss: 0.4442\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.4396 - val_loss: 0.4436\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4390 - val_loss: 0.4434\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4384 - val_loss: 0.4425\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4378 - val_loss: 0.4423\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 864us/step - loss: 0.4372 - val_loss: 0.4416\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4367 - val_loss: 0.4414\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 870us/step - loss: 0.4360 - val_loss: 0.4411\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4354 - val_loss: 0.4402\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4349 - val_loss: 0.4397\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4344 - val_loss: 0.4395\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4339 - val_loss: 0.4392\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4332 - val_loss: 0.4386\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4327 - val_loss: 0.4381\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.4321 - val_loss: 0.4378\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.4317 - val_loss: 0.4372\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4312 - val_loss: 0.4371\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4307 - val_loss: 0.4364\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4302 - val_loss: 0.4360\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4297 - val_loss: 0.4355\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4292 - val_loss: 0.4352\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4288 - val_loss: 0.4347\n",
      "121/121 [==============================] - 0s 583us/step - loss: 0.4326\n",
      "[CV] END learning_rate=0.0007442955289020042, n_hidden=1, n_neurons=66; total time=  21.9s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ozing\\miniconda3\\envs\\handson-ml2\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 3.2852 - val_loss: 1.5561\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 1.2149 - val_loss: 0.8902\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.8515 - val_loss: 0.7543\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 867us/step - loss: 0.7569 - val_loss: 0.7126\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.7186 - val_loss: 0.6883\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.6933 - val_loss: 0.6702\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.6739 - val_loss: 0.6539\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.6572 - val_loss: 0.6397\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.6427 - val_loss: 0.6270\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.6298 - val_loss: 0.6154\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.6184 - val_loss: 0.6046\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.6082 - val_loss: 0.5953\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.5988 - val_loss: 0.5865\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.5903 - val_loss: 0.5786\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.5825 - val_loss: 0.5711\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.5754 - val_loss: 0.5641\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.5688 - val_loss: 0.5580\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.5627 - val_loss: 0.5522\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 903us/step - loss: 0.5573 - val_loss: 0.5468\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.5518 - val_loss: 0.5414\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.5473 - val_loss: 0.5370\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 877us/step - loss: 0.5428 - val_loss: 0.5325\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.5388 - val_loss: 0.5288\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.5348 - val_loss: 0.5250\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.5312 - val_loss: 0.5215\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.5280 - val_loss: 0.5183\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.5246 - val_loss: 0.5152\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 878us/step - loss: 0.5218 - val_loss: 0.5125\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.5188 - val_loss: 0.5096\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.5166 - val_loss: 0.5071\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.5142 - val_loss: 0.5053\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.5114 - val_loss: 0.5022\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.5096 - val_loss: 0.5017\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 883us/step - loss: 0.5076 - val_loss: 0.4995\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.5050 - val_loss: 0.4963\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.5037 - val_loss: 0.4953\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.5016 - val_loss: 0.4928\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.5004 - val_loss: 0.4914\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4986 - val_loss: 0.4905\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4971 - val_loss: 0.4888\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4956 - val_loss: 0.4870\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4940 - val_loss: 0.4853\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 891us/step - loss: 0.4927 - val_loss: 0.4841\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 886us/step - loss: 0.4915 - val_loss: 0.4832\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 891us/step - loss: 0.4900 - val_loss: 0.4814\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.4887 - val_loss: 0.4802\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4878 - val_loss: 0.4795\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4864 - val_loss: 0.4786\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.4849 - val_loss: 0.4765\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4840 - val_loss: 0.4764\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4831 - val_loss: 0.4759\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4817 - val_loss: 0.4744\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.4809 - val_loss: 0.4733\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4794 - val_loss: 0.4718\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4788 - val_loss: 0.4708\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 881us/step - loss: 0.4776 - val_loss: 0.4702\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4770 - val_loss: 0.4692\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4761 - val_loss: 0.4685\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4744 - val_loss: 0.4676\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4741 - val_loss: 0.4674\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4734 - val_loss: 0.4659\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4723 - val_loss: 0.4657\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.4717 - val_loss: 0.4649\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4707 - val_loss: 0.4636\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 887us/step - loss: 0.4702 - val_loss: 0.4631\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.4691 - val_loss: 0.4626\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 882us/step - loss: 0.4683 - val_loss: 0.4614\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.4679 - val_loss: 0.4608\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.4669 - val_loss: 0.4602\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4662 - val_loss: 0.4596\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4656 - val_loss: 0.4589\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4650 - val_loss: 0.4582\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4641 - val_loss: 0.4579\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4633 - val_loss: 0.4572\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4627 - val_loss: 0.4568\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4622 - val_loss: 0.4561\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4613 - val_loss: 0.4552\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 893us/step - loss: 0.4608 - val_loss: 0.4546\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.4602 - val_loss: 0.4542\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4594 - val_loss: 0.4534\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.4589 - val_loss: 0.4530\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4582 - val_loss: 0.4525\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4576 - val_loss: 0.4520\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4570 - val_loss: 0.4513\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4564 - val_loss: 0.4511\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4557 - val_loss: 0.4501\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4552 - val_loss: 0.4498\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 877us/step - loss: 0.4545 - val_loss: 0.4492\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4540 - val_loss: 0.4491\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4533 - val_loss: 0.4482\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4530 - val_loss: 0.4481\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.4524 - val_loss: 0.4474\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4518 - val_loss: 0.4472\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4514 - val_loss: 0.4467\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 882us/step - loss: 0.4506 - val_loss: 0.4466\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4501 - val_loss: 0.4458\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4497 - val_loss: 0.4451\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4491 - val_loss: 0.4449\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4487 - val_loss: 0.4445\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4481 - val_loss: 0.4442\n",
      "121/121 [==============================] - 0s 583us/step - loss: 0.4179\n",
      "[CV] END learning_rate=0.0007442955289020042, n_hidden=1, n_neurons=66; total time=  21.9s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ozing\\miniconda3\\envs\\handson-ml2\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 2.2985 - val_loss: 1.2520\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.9969 - val_loss: 0.8556\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7677 - val_loss: 0.7601\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.7060 - val_loss: 0.7194\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.6747 - val_loss: 0.6941\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.6533 - val_loss: 0.6750\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.6361 - val_loss: 0.6592\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.6216 - val_loss: 0.6449\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.6084 - val_loss: 0.6321\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.5970 - val_loss: 0.6203\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.5863 - val_loss: 0.6095\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.5765 - val_loss: 0.5996\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.5673 - val_loss: 0.5899\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.5588 - val_loss: 0.5812\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.5511 - val_loss: 0.5735\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.5438 - val_loss: 0.5662\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.5372 - val_loss: 0.5589\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.5311 - val_loss: 0.5526\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.5253 - val_loss: 0.5467\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.5201 - val_loss: 0.5414\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.5152 - val_loss: 0.5364\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.5110 - val_loss: 0.5320\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.5066 - val_loss: 0.5281\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.5026 - val_loss: 0.5238\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4990 - val_loss: 0.5203\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4957 - val_loss: 0.5169\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4924 - val_loss: 0.5138\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4895 - val_loss: 0.5107\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4868 - val_loss: 0.5079\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4840 - val_loss: 0.5055\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4815 - val_loss: 0.5030\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4793 - val_loss: 0.5008\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.4769 - val_loss: 0.4984\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4749 - val_loss: 0.4966\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4729 - val_loss: 0.4943\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4712 - val_loss: 0.4926\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4693 - val_loss: 0.4910\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4675 - val_loss: 0.4892\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4661 - val_loss: 0.4879\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4644 - val_loss: 0.4863\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4631 - val_loss: 0.4851\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4615 - val_loss: 0.4836\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4604 - val_loss: 0.4822\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 883us/step - loss: 0.4589 - val_loss: 0.4811\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4578 - val_loss: 0.4800\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 878us/step - loss: 0.4565 - val_loss: 0.4790\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 878us/step - loss: 0.4553 - val_loss: 0.4778\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.4542 - val_loss: 0.4765\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 874us/step - loss: 0.4530 - val_loss: 0.4757\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4520 - val_loss: 0.4748\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4508 - val_loss: 0.4740\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4500 - val_loss: 0.4726\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4490 - val_loss: 0.4717\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4480 - val_loss: 0.4708\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4470 - val_loss: 0.4701\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 898us/step - loss: 0.4462 - val_loss: 0.4694\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4452 - val_loss: 0.4684\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 878us/step - loss: 0.4444 - val_loss: 0.4673\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4435 - val_loss: 0.4667\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4426 - val_loss: 0.4657\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4418 - val_loss: 0.4650\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4408 - val_loss: 0.4642\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4402 - val_loss: 0.4636\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4394 - val_loss: 0.4627\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.4387 - val_loss: 0.4621\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4379 - val_loss: 0.4615\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 889us/step - loss: 0.4371 - val_loss: 0.4607\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4365 - val_loss: 0.4598\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4358 - val_loss: 0.4592\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4350 - val_loss: 0.4585\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 886us/step - loss: 0.4344 - val_loss: 0.4580\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4337 - val_loss: 0.4573\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 893us/step - loss: 0.4331 - val_loss: 0.4568\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4324 - val_loss: 0.4562\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 889us/step - loss: 0.4317 - val_loss: 0.4555\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4312 - val_loss: 0.4549\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4304 - val_loss: 0.4542\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4299 - val_loss: 0.4538\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4293 - val_loss: 0.4531\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 891us/step - loss: 0.4287 - val_loss: 0.4527\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4280 - val_loss: 0.4521\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4274 - val_loss: 0.4514\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4269 - val_loss: 0.4508\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.4263 - val_loss: 0.4505\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4257 - val_loss: 0.4502\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4250 - val_loss: 0.4498\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4247 - val_loss: 0.4490\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4240 - val_loss: 0.4485\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4236 - val_loss: 0.4480\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4231 - val_loss: 0.4476\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4223 - val_loss: 0.4472\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4219 - val_loss: 0.4466\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4215 - val_loss: 0.4461\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4210 - val_loss: 0.4457\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4204 - val_loss: 0.4454\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4199 - val_loss: 0.4448\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.4195 - val_loss: 0.4446\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4190 - val_loss: 0.4442\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4185 - val_loss: 0.4435\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4181 - val_loss: 0.4432\n",
      "121/121 [==============================] - 0s 642us/step - loss: 0.4646\n",
      "[CV] END learning_rate=0.0007442955289020042, n_hidden=1, n_neurons=66; total time=  22.0s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ozing\\miniconda3\\envs\\handson-ml2\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 1ms/step - loss: 2.0944 - val_loss: 0.9751\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.8500 - val_loss: 0.7531\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.7414 - val_loss: 0.7040\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.6991 - val_loss: 0.6692\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.6659 - val_loss: 0.6408\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 958us/step - loss: 0.6384 - val_loss: 0.6178\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.6141 - val_loss: 0.5980\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.5940 - val_loss: 0.5805\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.5765 - val_loss: 0.5653\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.5616 - val_loss: 0.5526\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.5484 - val_loss: 0.5411\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.5358 - val_loss: 0.5305\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.5267 - val_loss: 0.5214\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.5171 - val_loss: 0.5144\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.5092 - val_loss: 0.5060\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.5014 - val_loss: 0.4988\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.4950 - val_loss: 0.4933\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 998us/step - loss: 0.4887 - val_loss: 0.4876\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.4829 - val_loss: 0.4824\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.4775 - val_loss: 0.4778\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.4727 - val_loss: 0.4744\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.4679 - val_loss: 0.4696\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.4641 - val_loss: 0.4656\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.4603 - val_loss: 0.4624\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.4563 - val_loss: 0.4590\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.4521 - val_loss: 0.4572\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.4497 - val_loss: 0.4531\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.4465 - val_loss: 0.4505\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.4433 - val_loss: 0.4479\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.4408 - val_loss: 0.4458\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.4384 - val_loss: 0.4434\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.4359 - val_loss: 0.4417\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.4335 - val_loss: 0.4400\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4314 - val_loss: 0.4381\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4292 - val_loss: 0.4366\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4273 - val_loss: 0.4349\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.4257 - val_loss: 0.4333\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.4237 - val_loss: 0.4319\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.4222 - val_loss: 0.4303\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.4205 - val_loss: 0.4296\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.4190 - val_loss: 0.4284\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.4174 - val_loss: 0.4267\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.4161 - val_loss: 0.4255\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.4146 - val_loss: 0.4250\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.4132 - val_loss: 0.4238\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.4121 - val_loss: 0.4234\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.4109 - val_loss: 0.4215\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.4096 - val_loss: 0.4207\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 965us/step - loss: 0.4084 - val_loss: 0.4202\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.4072 - val_loss: 0.4190\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.4059 - val_loss: 0.4179\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 957us/step - loss: 0.4051 - val_loss: 0.4174\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.4040 - val_loss: 0.4163\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.4029 - val_loss: 0.4156\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.4016 - val_loss: 0.4146\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.4008 - val_loss: 0.4142\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 957us/step - loss: 0.3997 - val_loss: 0.4132\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.3987 - val_loss: 0.4125\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.3975 - val_loss: 0.4116\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.3968 - val_loss: 0.4110\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.3955 - val_loss: 0.4103\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.3947 - val_loss: 0.4089\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.3940 - val_loss: 0.4085\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.3930 - val_loss: 0.4076\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 960us/step - loss: 0.3921 - val_loss: 0.4073\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.3911 - val_loss: 0.4062\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.3899 - val_loss: 0.4073\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.3893 - val_loss: 0.4056\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.3886 - val_loss: 0.4051\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.3877 - val_loss: 0.4040\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.3870 - val_loss: 0.4037\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.3862 - val_loss: 0.4023\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.3852 - val_loss: 0.4017\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.3845 - val_loss: 0.4012\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 974us/step - loss: 0.3833 - val_loss: 0.4008\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.3826 - val_loss: 0.4005\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.3821 - val_loss: 0.3992\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.3812 - val_loss: 0.3998\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.3805 - val_loss: 0.3984\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.3798 - val_loss: 0.3977\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.3792 - val_loss: 0.3966\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.3782 - val_loss: 0.3967\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.3774 - val_loss: 0.3957\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.3768 - val_loss: 0.3956\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.3762 - val_loss: 0.3943\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.3754 - val_loss: 0.3941\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.3748 - val_loss: 0.3932\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.3740 - val_loss: 0.3928\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.3734 - val_loss: 0.3929\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 982us/step - loss: 0.3728 - val_loss: 0.3916\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.3718 - val_loss: 0.3919\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.3716 - val_loss: 0.3904\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.3708 - val_loss: 0.3905\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.3702 - val_loss: 0.3893\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.3697 - val_loss: 0.3892\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.3687 - val_loss: 0.3883\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.3683 - val_loss: 0.3876\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.3677 - val_loss: 0.3872\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.3671 - val_loss: 0.3871\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.3664 - val_loss: 0.3868\n",
      "121/121 [==============================] - 0s 558us/step - loss: 0.3796\n",
      "[CV] END learning_rate=0.0009710617211943913, n_hidden=2, n_neurons=82; total time=  23.9s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ozing\\miniconda3\\envs\\handson-ml2\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 2.2389 - val_loss: 0.8824\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.9305 - val_loss: 0.7503\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.7597 - val_loss: 0.7016\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.6980 - val_loss: 0.6697\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.6614 - val_loss: 0.6427\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 984us/step - loss: 0.6344 - val_loss: 0.6206\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.6126 - val_loss: 0.6017\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.5948 - val_loss: 0.5850\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.5790 - val_loss: 0.5716\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.5662 - val_loss: 0.5579\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.5545 - val_loss: 0.5464\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.5443 - val_loss: 0.5368\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.5352 - val_loss: 0.5281\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.5273 - val_loss: 0.5196\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.5203 - val_loss: 0.5125\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.5137 - val_loss: 0.5060\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.5078 - val_loss: 0.5000\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.5026 - val_loss: 0.4954\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.4978 - val_loss: 0.4906\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.4934 - val_loss: 0.4865\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.4897 - val_loss: 0.4819\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.4857 - val_loss: 0.4783\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.4821 - val_loss: 0.4756\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.4791 - val_loss: 0.4720\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.4761 - val_loss: 0.4693\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.4734 - val_loss: 0.4665\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.4706 - val_loss: 0.4643\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.4683 - val_loss: 0.4624\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.4660 - val_loss: 0.4592\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.4638 - val_loss: 0.4572\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.4617 - val_loss: 0.4552\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 956us/step - loss: 0.4597 - val_loss: 0.4538\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.4577 - val_loss: 0.4522\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.4559 - val_loss: 0.4507\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.4541 - val_loss: 0.4486\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.4522 - val_loss: 0.4479\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.4505 - val_loss: 0.4454\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.4489 - val_loss: 0.4450\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.4476 - val_loss: 0.4434\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.4461 - val_loss: 0.4417\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.4445 - val_loss: 0.4400\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.4433 - val_loss: 0.4385\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.4416 - val_loss: 0.4381\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.4406 - val_loss: 0.4360\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.4392 - val_loss: 0.4350\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.4378 - val_loss: 0.4339\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.4365 - val_loss: 0.4330\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 973us/step - loss: 0.4354 - val_loss: 0.4318\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.4341 - val_loss: 0.4306\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.4327 - val_loss: 0.4297\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.4315 - val_loss: 0.4286\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.4306 - val_loss: 0.4278\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.4294 - val_loss: 0.4271\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.4282 - val_loss: 0.4259\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.4271 - val_loss: 0.4251\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.4260 - val_loss: 0.4236\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.4251 - val_loss: 0.4238\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.4243 - val_loss: 0.4221\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.4230 - val_loss: 0.4218\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.4218 - val_loss: 0.4204\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 990us/step - loss: 0.4210 - val_loss: 0.4208\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.4203 - val_loss: 0.4192\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 948us/step - loss: 0.4191 - val_loss: 0.4179\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.4180 - val_loss: 0.4169\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.4174 - val_loss: 0.4172\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 980us/step - loss: 0.4164 - val_loss: 0.4166\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.4155 - val_loss: 0.4147\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.4147 - val_loss: 0.4146\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.4136 - val_loss: 0.4146\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.4131 - val_loss: 0.4128\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 957us/step - loss: 0.4119 - val_loss: 0.4121\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.4113 - val_loss: 0.4111\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4103 - val_loss: 0.4109\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.4095 - val_loss: 0.4099\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.4086 - val_loss: 0.4094\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.4081 - val_loss: 0.4092\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.4071 - val_loss: 0.4083\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.4064 - val_loss: 0.4074\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.4058 - val_loss: 0.4078\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.4048 - val_loss: 0.4060\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.4043 - val_loss: 0.4050\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 962us/step - loss: 0.4030 - val_loss: 0.4041\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 954us/step - loss: 0.4026 - val_loss: 0.4037\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 956us/step - loss: 0.4019 - val_loss: 0.4034\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.4010 - val_loss: 0.4028\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 968us/step - loss: 0.4005 - val_loss: 0.4020\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.3995 - val_loss: 0.4015\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.3991 - val_loss: 0.4005\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.3983 - val_loss: 0.4001\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.3978 - val_loss: 0.3992\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.3968 - val_loss: 0.3983\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.3961 - val_loss: 0.3983\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.3955 - val_loss: 0.3975\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 954us/step - loss: 0.3948 - val_loss: 0.3968\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.3939 - val_loss: 0.3973\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 951us/step - loss: 0.3934 - val_loss: 0.3970\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 958us/step - loss: 0.3925 - val_loss: 0.3950\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.3917 - val_loss: 0.3946\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.3911 - val_loss: 0.3950\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 962us/step - loss: 0.3903 - val_loss: 0.3937\n",
      "121/121 [==============================] - 0s 609us/step - loss: 0.3750\n",
      "[CV] END learning_rate=0.0009710617211943913, n_hidden=2, n_neurons=82; total time=  23.8s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ozing\\miniconda3\\envs\\handson-ml2\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 2.2448 - val_loss: 1.0148\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.8485 - val_loss: 0.7886\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 947us/step - loss: 0.7375 - val_loss: 0.7305\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.6912 - val_loss: 0.6970\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.6599 - val_loss: 0.6703\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.6358 - val_loss: 0.6487\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.6152 - val_loss: 0.6296\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 930us/step - loss: 0.5969 - val_loss: 0.6129\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.5805 - val_loss: 0.5964\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.5661 - val_loss: 0.5821\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.5527 - val_loss: 0.5695\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.5408 - val_loss: 0.5582\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 930us/step - loss: 0.5302 - val_loss: 0.5473\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.5206 - val_loss: 0.5378\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.5120 - val_loss: 0.5293\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.5039 - val_loss: 0.5213\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.4970 - val_loss: 0.5145\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.4904 - val_loss: 0.5083\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: 0.4847 - val_loss: 0.5024\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.4793 - val_loss: 0.4970\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.4744 - val_loss: 0.4924\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.4701 - val_loss: 0.4881\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.4660 - val_loss: 0.4840\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.4623 - val_loss: 0.4807\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4586 - val_loss: 0.4776\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.4557 - val_loss: 0.4743\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.4524 - val_loss: 0.4718\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.4498 - val_loss: 0.4690\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.4470 - val_loss: 0.4662\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.4446 - val_loss: 0.4638\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.4422 - val_loss: 0.4616\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.4398 - val_loss: 0.4598\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.4377 - val_loss: 0.4573\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.4356 - val_loss: 0.4553\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.4336 - val_loss: 0.4534\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.4320 - val_loss: 0.4519\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.4297 - val_loss: 0.4497\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.4281 - val_loss: 0.4482\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.4262 - val_loss: 0.4465\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.4245 - val_loss: 0.4459\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.4231 - val_loss: 0.4440\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.4214 - val_loss: 0.4421\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.4200 - val_loss: 0.4412\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.4187 - val_loss: 0.4394\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.4168 - val_loss: 0.4386\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.4158 - val_loss: 0.4372\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.4144 - val_loss: 0.4356\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.4130 - val_loss: 0.4351\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.4116 - val_loss: 0.4336\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.4101 - val_loss: 0.4319\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.4091 - val_loss: 0.4315\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.4077 - val_loss: 0.4298\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.4066 - val_loss: 0.4289\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 936us/step - loss: 0.4053 - val_loss: 0.4279\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.4043 - val_loss: 0.4265\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.4031 - val_loss: 0.4253\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.4018 - val_loss: 0.4244\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.4005 - val_loss: 0.4238\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.3996 - val_loss: 0.4226\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.3983 - val_loss: 0.4218\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.3971 - val_loss: 0.4205\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.3960 - val_loss: 0.4202\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.3952 - val_loss: 0.4192\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.3942 - val_loss: 0.4173\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.3932 - val_loss: 0.4167\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.3921 - val_loss: 0.4158\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.3909 - val_loss: 0.4151\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.3901 - val_loss: 0.4138\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.3890 - val_loss: 0.4127\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.3883 - val_loss: 0.4121\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.3872 - val_loss: 0.4118\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.3859 - val_loss: 0.4104\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.3852 - val_loss: 0.4096\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.3842 - val_loss: 0.4086\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.3834 - val_loss: 0.4083\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.3827 - val_loss: 0.4072\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.3817 - val_loss: 0.4067\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.3808 - val_loss: 0.4064\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.3800 - val_loss: 0.4056\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.3792 - val_loss: 0.4046\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.3783 - val_loss: 0.4042\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.3777 - val_loss: 0.4033\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.3768 - val_loss: 0.4020\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.3761 - val_loss: 0.4014\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.3752 - val_loss: 0.4005\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 984us/step - loss: 0.3743 - val_loss: 0.4006\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.3739 - val_loss: 0.3998\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.3731 - val_loss: 0.3995\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3723 - val_loss: 0.3981\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.3716 - val_loss: 0.3973\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.3708 - val_loss: 0.3977\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.3703 - val_loss: 0.3962\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.3696 - val_loss: 0.3961\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.3686 - val_loss: 0.3945\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.3680 - val_loss: 0.3944\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.3675 - val_loss: 0.3934\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.3667 - val_loss: 0.3929\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.3661 - val_loss: 0.3924\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.3652 - val_loss: 0.3921\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.3649 - val_loss: 0.3909\n",
      "121/121 [==============================] - 0s 592us/step - loss: 0.4062\n",
      "[CV] END learning_rate=0.0009710617211943913, n_hidden=2, n_neurons=82; total time=  23.6s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ozing\\miniconda3\\envs\\handson-ml2\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 1ms/step - loss: 3.5063 - val_loss: 1.7367\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 1.2869 - val_loss: 0.8950\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.8264 - val_loss: 0.7411\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.7406 - val_loss: 0.7089\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.7098 - val_loss: 0.6843\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 894us/step - loss: 0.6856 - val_loss: 0.6638\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.6657 - val_loss: 0.6453\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 897us/step - loss: 0.6475 - val_loss: 0.6291\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.6314 - val_loss: 0.6150\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.6169 - val_loss: 0.6020\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.6039 - val_loss: 0.5905\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.5922 - val_loss: 0.5803\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.5817 - val_loss: 0.5710\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.5723 - val_loss: 0.5630\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.5635 - val_loss: 0.5558\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.5560 - val_loss: 0.5486\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.5488 - val_loss: 0.5425\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.5422 - val_loss: 0.5370\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.5363 - val_loss: 0.5329\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.5311 - val_loss: 0.5276\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.5258 - val_loss: 0.5234\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.5220 - val_loss: 0.5196\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.5183 - val_loss: 0.5162\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 885us/step - loss: 0.5145 - val_loss: 0.5134\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 886us/step - loss: 0.5112 - val_loss: 0.5105\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 908us/step - loss: 0.5083 - val_loss: 0.5079\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.5055 - val_loss: 0.5053\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.5027 - val_loss: 0.5041\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.5005 - val_loss: 0.5021\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 0.4982 - val_loss: 0.4994\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 926us/step - loss: 0.4962 - val_loss: 0.4972\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4936 - val_loss: 0.4949\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4922 - val_loss: 0.4949\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 893us/step - loss: 0.4903 - val_loss: 0.4920\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4885 - val_loss: 0.4925\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4867 - val_loss: 0.4886\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4849 - val_loss: 0.4889\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4836 - val_loss: 0.4874\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4813 - val_loss: 0.4839\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 0.4809 - val_loss: 0.4827\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4796 - val_loss: 0.4823\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4774 - val_loss: 0.4797\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4765 - val_loss: 0.4788\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 0.4754 - val_loss: 0.4784\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 914us/step - loss: 0.4741 - val_loss: 0.4771\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 903us/step - loss: 0.4727 - val_loss: 0.4759\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4711 - val_loss: 0.4740\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4698 - val_loss: 0.4732\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4689 - val_loss: 0.4718\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 902us/step - loss: 0.4676 - val_loss: 0.4709\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4665 - val_loss: 0.4694\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4652 - val_loss: 0.4684\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4637 - val_loss: 0.4676\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4632 - val_loss: 0.4667\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4616 - val_loss: 0.4662\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4609 - val_loss: 0.4647\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4592 - val_loss: 0.4632\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4586 - val_loss: 0.4626\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4575 - val_loss: 0.4618\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4562 - val_loss: 0.4605\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4553 - val_loss: 0.4596\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4546 - val_loss: 0.4589\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4534 - val_loss: 0.4579\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4522 - val_loss: 0.4571\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4516 - val_loss: 0.4570\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4500 - val_loss: 0.4553\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4494 - val_loss: 0.4546\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.4481 - val_loss: 0.4537\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4477 - val_loss: 0.4526\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4464 - val_loss: 0.4519\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4453 - val_loss: 0.4517\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4450 - val_loss: 0.4502\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4434 - val_loss: 0.4493\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4431 - val_loss: 0.4487\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4417 - val_loss: 0.4479\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4414 - val_loss: 0.4475\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4401 - val_loss: 0.4474\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4393 - val_loss: 0.4467\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4385 - val_loss: 0.4451\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4378 - val_loss: 0.4445\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4369 - val_loss: 0.4439\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4365 - val_loss: 0.4432\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 891us/step - loss: 0.4354 - val_loss: 0.4431\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4342 - val_loss: 0.4420\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4344 - val_loss: 0.4414\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4332 - val_loss: 0.4405\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4323 - val_loss: 0.4399\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.4318 - val_loss: 0.4394\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4310 - val_loss: 0.4386\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4302 - val_loss: 0.4386\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4295 - val_loss: 0.4376\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 0.4289 - val_loss: 0.4371\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4283 - val_loss: 0.4365\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4276 - val_loss: 0.4358\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4267 - val_loss: 0.4354\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4261 - val_loss: 0.4347\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4254 - val_loss: 0.4346\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4248 - val_loss: 0.4336\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4241 - val_loss: 0.4332\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4232 - val_loss: 0.4324\n",
      "121/121 [==============================] - 0s 575us/step - loss: 0.4294\n",
      "[CV] END learning_rate=0.0007563660607219671, n_hidden=2, n_neurons=23; total time=  22.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ozing\\miniconda3\\envs\\handson-ml2\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 1ms/step - loss: 3.0596 - val_loss: 1.7344\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 1.2696 - val_loss: 0.9756\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.8935 - val_loss: 0.8179\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.8067 - val_loss: 0.7669\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.7692 - val_loss: 0.7379\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.7438 - val_loss: 0.7167\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.7234 - val_loss: 0.6986\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.7063 - val_loss: 0.6831\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.6910 - val_loss: 0.6695\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.6774 - val_loss: 0.6574\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.6649 - val_loss: 0.6459\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.6532 - val_loss: 0.6351\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.6424 - val_loss: 0.6249\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.6323 - val_loss: 0.6153\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.6222 - val_loss: 0.6059\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.6134 - val_loss: 0.5969\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.6042 - val_loss: 0.5888\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.5957 - val_loss: 0.5801\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.5875 - val_loss: 0.5723\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.5795 - val_loss: 0.5646\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.5719 - val_loss: 0.5574\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.5645 - val_loss: 0.5502\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.5577 - val_loss: 0.5435\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.5508 - val_loss: 0.5372\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.5443 - val_loss: 0.5313\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.5384 - val_loss: 0.5249\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.5324 - val_loss: 0.5193\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.5268 - val_loss: 0.5140\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.5215 - val_loss: 0.5093\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.5167 - val_loss: 0.5049\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.5121 - val_loss: 0.5005\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.5078 - val_loss: 0.4974\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 893us/step - loss: 0.5036 - val_loss: 0.4929\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.5005 - val_loss: 0.4899\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4971 - val_loss: 0.4869\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4941 - val_loss: 0.4841\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4912 - val_loss: 0.4817\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.4885 - val_loss: 0.4789\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4864 - val_loss: 0.4774\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4840 - val_loss: 0.4752\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4818 - val_loss: 0.4728\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4800 - val_loss: 0.4714\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 878us/step - loss: 0.4781 - val_loss: 0.4701\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 894us/step - loss: 0.4761 - val_loss: 0.4682\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4747 - val_loss: 0.4669\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4726 - val_loss: 0.4654\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4717 - val_loss: 0.4643\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4700 - val_loss: 0.4632\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4686 - val_loss: 0.4622\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4670 - val_loss: 0.4606\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4652 - val_loss: 0.4592\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4652 - val_loss: 0.4587\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4628 - val_loss: 0.4576\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4625 - val_loss: 0.4570\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 0.4609 - val_loss: 0.4562\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 894us/step - loss: 0.4599 - val_loss: 0.4549\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4583 - val_loss: 0.4543\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4581 - val_loss: 0.4532\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4565 - val_loss: 0.4520\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4557 - val_loss: 0.4515\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4548 - val_loss: 0.4508\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4534 - val_loss: 0.4500\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4532 - val_loss: 0.4489\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4519 - val_loss: 0.4481\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4510 - val_loss: 0.4478\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4499 - val_loss: 0.4468\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 897us/step - loss: 0.4496 - val_loss: 0.4461\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4482 - val_loss: 0.4454\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4476 - val_loss: 0.4449\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4468 - val_loss: 0.4440\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4458 - val_loss: 0.4434\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4453 - val_loss: 0.4428\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4439 - val_loss: 0.4428\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4435 - val_loss: 0.4415\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4424 - val_loss: 0.4411\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4418 - val_loss: 0.4407\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4408 - val_loss: 0.4400\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 898us/step - loss: 0.4401 - val_loss: 0.4397\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4393 - val_loss: 0.4389\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4383 - val_loss: 0.4380\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4378 - val_loss: 0.4376\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4370 - val_loss: 0.4368\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4365 - val_loss: 0.4362\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4356 - val_loss: 0.4361\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4348 - val_loss: 0.4348\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4341 - val_loss: 0.4341\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.4333 - val_loss: 0.4338\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4328 - val_loss: 0.4331\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.4319 - val_loss: 0.4322\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4311 - val_loss: 0.4321\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4304 - val_loss: 0.4318\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4298 - val_loss: 0.4304\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.4292 - val_loss: 0.4301\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4282 - val_loss: 0.4305\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4276 - val_loss: 0.4297\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 889us/step - loss: 0.4270 - val_loss: 0.4285\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4257 - val_loss: 0.4285\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4258 - val_loss: 0.4280\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4248 - val_loss: 0.4270\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4242 - val_loss: 0.4265\n",
      "121/121 [==============================] - 0s 592us/step - loss: 0.3992\n",
      "[CV] END learning_rate=0.0007563660607219671, n_hidden=2, n_neurons=23; total time=  22.5s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ozing\\miniconda3\\envs\\handson-ml2\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 2.9196 - val_loss: 1.7966\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 1.3992 - val_loss: 1.1348\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.9742 - val_loss: 0.8738\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.7837 - val_loss: 0.7569\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.6980 - val_loss: 0.7024\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 898us/step - loss: 0.6561 - val_loss: 0.6719\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.6301 - val_loss: 0.6495\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.6098 - val_loss: 0.6308\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.5928 - val_loss: 0.6142\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.5781 - val_loss: 0.5996\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.5651 - val_loss: 0.5866\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.5534 - val_loss: 0.5749\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.5430 - val_loss: 0.5643\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 903us/step - loss: 0.5337 - val_loss: 0.5545\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 897us/step - loss: 0.5253 - val_loss: 0.5458\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 910us/step - loss: 0.5176 - val_loss: 0.5375\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.5106 - val_loss: 0.5301\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 904us/step - loss: 0.5043 - val_loss: 0.5236\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 0.4985 - val_loss: 0.5173\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4937 - val_loss: 0.5118\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4886 - val_loss: 0.5072\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4845 - val_loss: 0.5028\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4805 - val_loss: 0.4989\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4768 - val_loss: 0.4948\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4737 - val_loss: 0.4916\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4703 - val_loss: 0.4889\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4677 - val_loss: 0.4859\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4647 - val_loss: 0.4831\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4624 - val_loss: 0.4810\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4600 - val_loss: 0.4788\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4576 - val_loss: 0.4763\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.4562 - val_loss: 0.4749\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4537 - val_loss: 0.4726\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4525 - val_loss: 0.4717\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4503 - val_loss: 0.4694\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4489 - val_loss: 0.4687\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 932us/step - loss: 0.4471 - val_loss: 0.4669\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4462 - val_loss: 0.4661\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4446 - val_loss: 0.4643\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 903us/step - loss: 0.4437 - val_loss: 0.4640\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 912us/step - loss: 0.4421 - val_loss: 0.4620\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 891us/step - loss: 0.4411 - val_loss: 0.4622\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4398 - val_loss: 0.4601\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4387 - val_loss: 0.4594\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 910us/step - loss: 0.4378 - val_loss: 0.4585\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 908us/step - loss: 0.4368 - val_loss: 0.4576\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 894us/step - loss: 0.4357 - val_loss: 0.4569\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4349 - val_loss: 0.4556\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 914us/step - loss: 0.4341 - val_loss: 0.4550\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 0.4330 - val_loss: 0.4547\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4320 - val_loss: 0.4535\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4313 - val_loss: 0.4525\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.4305 - val_loss: 0.4518\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 907us/step - loss: 0.4296 - val_loss: 0.4513\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4287 - val_loss: 0.4503\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4279 - val_loss: 0.4494\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4273 - val_loss: 0.4491\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 897us/step - loss: 0.4262 - val_loss: 0.4488\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 922us/step - loss: 0.4256 - val_loss: 0.4479\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4247 - val_loss: 0.4467\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 906us/step - loss: 0.4237 - val_loss: 0.4459\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4232 - val_loss: 0.4454\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4225 - val_loss: 0.4447\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 930us/step - loss: 0.4215 - val_loss: 0.4437\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4208 - val_loss: 0.4434\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4199 - val_loss: 0.4425\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4194 - val_loss: 0.4420\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4185 - val_loss: 0.4414\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4178 - val_loss: 0.4406\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4172 - val_loss: 0.4405\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4165 - val_loss: 0.4395\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4157 - val_loss: 0.4389\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4151 - val_loss: 0.4383\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4145 - val_loss: 0.4381\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 907us/step - loss: 0.4138 - val_loss: 0.4371\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4130 - val_loss: 0.4359\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4125 - val_loss: 0.4358\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4118 - val_loss: 0.4350\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4111 - val_loss: 0.4346\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.4104 - val_loss: 0.4343\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.4099 - val_loss: 0.4337\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4092 - val_loss: 0.4327\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4085 - val_loss: 0.4320\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.4078 - val_loss: 0.4319\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4075 - val_loss: 0.4313\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4067 - val_loss: 0.4306\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4061 - val_loss: 0.4300\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 904us/step - loss: 0.4055 - val_loss: 0.4298\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4050 - val_loss: 0.4290\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.4044 - val_loss: 0.4290\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4038 - val_loss: 0.4284\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 907us/step - loss: 0.4033 - val_loss: 0.4273\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4027 - val_loss: 0.4268\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4021 - val_loss: 0.4266\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4016 - val_loss: 0.4255\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4011 - val_loss: 0.4252\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4005 - val_loss: 0.4248\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.3998 - val_loss: 0.4241\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3994 - val_loss: 0.4242\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.3988 - val_loss: 0.4234\n",
      "121/121 [==============================] - 0s 558us/step - loss: 0.4472\n",
      "[CV] END learning_rate=0.0007563660607219671, n_hidden=2, n_neurons=23; total time=  22.4s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ozing\\miniconda3\\envs\\handson-ml2\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 2.0396 - val_loss: 0.9006\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.8710 - val_loss: 0.7185\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.7226 - val_loss: 0.6658\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.6841 - val_loss: 0.6398\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.6409 - val_loss: 0.6132\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.6158 - val_loss: 0.5929\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.5945 - val_loss: 0.5768\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.5777 - val_loss: 0.5622\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.5626 - val_loss: 0.5517\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.5501 - val_loss: 0.5399\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.5392 - val_loss: 0.5309\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.5299 - val_loss: 0.5237\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.5227 - val_loss: 0.5171\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.5155 - val_loss: 0.5112\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 887us/step - loss: 0.5097 - val_loss: 0.5069\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.5046 - val_loss: 0.5027\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.4994 - val_loss: 0.4971\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.4954 - val_loss: 0.4945\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4913 - val_loss: 0.4902\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4876 - val_loss: 0.4884\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 871us/step - loss: 0.4845 - val_loss: 0.4858\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 878us/step - loss: 0.4807 - val_loss: 0.4835\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 867us/step - loss: 0.4784 - val_loss: 0.4804\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4759 - val_loss: 0.4784\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4732 - val_loss: 0.4770\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4707 - val_loss: 0.4740\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4687 - val_loss: 0.4727\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4666 - val_loss: 0.4711\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.4650 - val_loss: 0.4695\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 867us/step - loss: 0.4632 - val_loss: 0.4686\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.4611 - val_loss: 0.4673\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4604 - val_loss: 0.4654\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4580 - val_loss: 0.4642\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4567 - val_loss: 0.4631\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 867us/step - loss: 0.4559 - val_loss: 0.4621\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4542 - val_loss: 0.4611\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.4523 - val_loss: 0.4602\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4516 - val_loss: 0.4595\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.4504 - val_loss: 0.4574\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4502 - val_loss: 0.4569\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 885us/step - loss: 0.4479 - val_loss: 0.4568\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.4479 - val_loss: 0.4555\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4467 - val_loss: 0.4540\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4446 - val_loss: 0.4534\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.4446 - val_loss: 0.4529\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 879us/step - loss: 0.4444 - val_loss: 0.4515\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4417 - val_loss: 0.4511\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.4407 - val_loss: 0.4498\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 871us/step - loss: 0.4415 - val_loss: 0.4499\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4411 - val_loss: 0.4488\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4378 - val_loss: 0.4484\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 867us/step - loss: 0.4369 - val_loss: 0.4472\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4373 - val_loss: 0.4466\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4356 - val_loss: 0.4466\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 879us/step - loss: 0.4345 - val_loss: 0.4453\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4339 - val_loss: 0.4444\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.4337 - val_loss: 0.4434\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4321 - val_loss: 0.4434\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.4318 - val_loss: 0.4427\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4317 - val_loss: 0.4419\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4295 - val_loss: 0.4413\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.4313 - val_loss: 0.4408\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4286 - val_loss: 0.4395\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 875us/step - loss: 0.4310 - val_loss: 0.4398\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4299 - val_loss: 0.4391\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4265 - val_loss: 0.4384\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4259 - val_loss: 0.4380\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 889us/step - loss: 0.4248 - val_loss: 0.4369\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4245 - val_loss: 0.4370\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4242 - val_loss: 0.4360\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 891us/step - loss: 0.4232 - val_loss: 0.4353\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4247 - val_loss: 0.4352\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4238 - val_loss: 0.4344\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4209 - val_loss: 0.4336\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.4211 - val_loss: 0.4335\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.4200 - val_loss: 0.4325\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.4194 - val_loss: 0.4335\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 867us/step - loss: 0.4187 - val_loss: 0.4316\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4180 - val_loss: 0.4317\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 878us/step - loss: 0.4171 - val_loss: 0.4311\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.4162 - val_loss: 0.4301\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4155 - val_loss: 0.4298\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 863us/step - loss: 0.4154 - val_loss: 0.4297\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4142 - val_loss: 0.4291\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4142 - val_loss: 0.4282\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4134 - val_loss: 0.4274\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4134 - val_loss: 0.4288\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4127 - val_loss: 0.4268\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.4126 - val_loss: 0.4271\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 871us/step - loss: 0.4120 - val_loss: 0.4255\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4103 - val_loss: 0.4258\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4093 - val_loss: 0.4248\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 874us/step - loss: 0.4099 - val_loss: 0.4241\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4087 - val_loss: 0.4235\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4079 - val_loss: 0.4246\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.4085 - val_loss: 0.4229\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.4086 - val_loss: 0.4225\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4064 - val_loss: 0.4230\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4059 - val_loss: 0.4211\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.4052 - val_loss: 0.4217\n",
      "121/121 [==============================] - 0s 550us/step - loss: 0.4134\n",
      "[CV] END learning_rate=0.0014109707233345545, n_hidden=1, n_neurons=53; total time=  21.8s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ozing\\miniconda3\\envs\\handson-ml2\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 1ms/step - loss: 2.4952 - val_loss: 0.9147\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.7916 - val_loss: 0.7149\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.6943 - val_loss: 0.6696\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.6601 - val_loss: 0.6416\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.6377 - val_loss: 0.6226\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.6179 - val_loss: 0.6046\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.6010 - val_loss: 0.5876\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.5890 - val_loss: 0.5747\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.5760 - val_loss: 0.5639\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.5641 - val_loss: 0.5529\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.5561 - val_loss: 0.5443\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.5464 - val_loss: 0.5351\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.5409 - val_loss: 0.5304\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.5318 - val_loss: 0.5199\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.5293 - val_loss: 0.5184\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 883us/step - loss: 0.5208 - val_loss: 0.5095\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.5182 - val_loss: 0.5089\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.5116 - val_loss: 0.4995\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.5090 - val_loss: 0.4966\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 863us/step - loss: 0.5065 - val_loss: 0.4924\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.5034 - val_loss: 0.4897\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.4996 - val_loss: 0.4869\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4980 - val_loss: 0.4872\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.4942 - val_loss: 0.4831\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.4913 - val_loss: 0.4799\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.4864 - val_loss: 0.4874\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4868 - val_loss: 0.4775\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 867us/step - loss: 0.4842 - val_loss: 0.4747\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4834 - val_loss: 0.4721\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 883us/step - loss: 0.4797 - val_loss: 0.4719\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4791 - val_loss: 0.4676\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.4768 - val_loss: 0.4654\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4758 - val_loss: 0.4650\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4728 - val_loss: 0.4635\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 885us/step - loss: 0.4694 - val_loss: 0.4615\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 871us/step - loss: 0.4689 - val_loss: 0.4698\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.4684 - val_loss: 0.4587\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.4664 - val_loss: 0.4575\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4664 - val_loss: 0.4570\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4643 - val_loss: 0.4563\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 867us/step - loss: 0.4623 - val_loss: 0.4545\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4603 - val_loss: 0.4531\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.4609 - val_loss: 0.4533\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4574 - val_loss: 0.4507\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.4556 - val_loss: 0.4557\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 871us/step - loss: 0.4560 - val_loss: 0.4498\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 885us/step - loss: 0.4558 - val_loss: 0.4491\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4525 - val_loss: 0.4510\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4522 - val_loss: 0.4465\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 867us/step - loss: 0.4520 - val_loss: 0.4458\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4507 - val_loss: 0.4449\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4495 - val_loss: 0.4447\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.4476 - val_loss: 0.4441\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.4466 - val_loss: 0.4441\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4449 - val_loss: 0.4414\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 863us/step - loss: 0.4452 - val_loss: 0.4452\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.4449 - val_loss: 0.4432\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 867us/step - loss: 0.4439 - val_loss: 0.4412\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4409 - val_loss: 0.4385\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4420 - val_loss: 0.4388\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.4399 - val_loss: 0.4378\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4399 - val_loss: 0.4378\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4390 - val_loss: 0.4364\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 870us/step - loss: 0.4374 - val_loss: 0.4359\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4373 - val_loss: 0.4354\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 877us/step - loss: 0.4357 - val_loss: 0.4341\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4358 - val_loss: 0.4336\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4341 - val_loss: 0.4325\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4342 - val_loss: 0.4327\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.4324 - val_loss: 0.4316\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.4319 - val_loss: 0.4309\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4307 - val_loss: 0.4298\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4299 - val_loss: 0.4326\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.4296 - val_loss: 0.4303\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 867us/step - loss: 0.4283 - val_loss: 0.4299\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4283 - val_loss: 0.4279\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.4271 - val_loss: 0.4270\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4260 - val_loss: 0.4273\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4258 - val_loss: 0.4264\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4249 - val_loss: 0.4260\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4239 - val_loss: 0.4250\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.4237 - val_loss: 0.4244\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 863us/step - loss: 0.4224 - val_loss: 0.4237\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4219 - val_loss: 0.4227\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.4209 - val_loss: 0.4225\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.4203 - val_loss: 0.4225\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 867us/step - loss: 0.4197 - val_loss: 0.4213\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.4197 - val_loss: 0.4205\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4185 - val_loss: 0.4204\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.4181 - val_loss: 0.4189\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.4170 - val_loss: 0.4189\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 867us/step - loss: 0.4160 - val_loss: 0.4186\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.4157 - val_loss: 0.4176\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4145 - val_loss: 0.4184\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.4147 - val_loss: 0.4163\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4133 - val_loss: 0.4158\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 877us/step - loss: 0.4128 - val_loss: 0.4152\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.4117 - val_loss: 0.4160\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.4118 - val_loss: 0.4134\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.4109 - val_loss: 0.4137\n",
      "121/121 [==============================] - 0s 600us/step - loss: 0.3839\n",
      "[CV] END learning_rate=0.0014109707233345545, n_hidden=1, n_neurons=53; total time=  21.8s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ozing\\miniconda3\\envs\\handson-ml2\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 1.9819 - val_loss: 0.9726\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.7676 - val_loss: 0.7049\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.6551 - val_loss: 0.6580\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.6216 - val_loss: 0.6310\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.5930 - val_loss: 0.6074\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.5718 - val_loss: 0.5865\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.5537 - val_loss: 0.5693\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.5371 - val_loss: 0.5522\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.5259 - val_loss: 0.5408\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.5124 - val_loss: 0.5281\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.5038 - val_loss: 0.5190\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4944 - val_loss: 0.5110\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.4879 - val_loss: 0.5048\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4816 - val_loss: 0.4992\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4779 - val_loss: 0.4948\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4730 - val_loss: 0.4918\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4695 - val_loss: 0.4873\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.4664 - val_loss: 0.4847\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.4638 - val_loss: 0.4819\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 867us/step - loss: 0.4609 - val_loss: 0.4797\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4591 - val_loss: 0.4775\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4571 - val_loss: 0.4749\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4553 - val_loss: 0.4737\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4532 - val_loss: 0.4716\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4511 - val_loss: 0.4703\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4501 - val_loss: 0.4689\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4484 - val_loss: 0.4672\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.4470 - val_loss: 0.4660\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4458 - val_loss: 0.4640\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.4441 - val_loss: 0.4627\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4432 - val_loss: 0.4615\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4417 - val_loss: 0.4605\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.4407 - val_loss: 0.4592\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4393 - val_loss: 0.4579\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4382 - val_loss: 0.4571\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.4371 - val_loss: 0.4560\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4358 - val_loss: 0.4551\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4351 - val_loss: 0.4540\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4338 - val_loss: 0.4528\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4334 - val_loss: 0.4527\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4319 - val_loss: 0.4514\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4313 - val_loss: 0.4503\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.4302 - val_loss: 0.4500\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4292 - val_loss: 0.4486\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4282 - val_loss: 0.4485\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 867us/step - loss: 0.4277 - val_loss: 0.4470\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4268 - val_loss: 0.4465\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4258 - val_loss: 0.4469\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4252 - val_loss: 0.4448\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 866us/step - loss: 0.4244 - val_loss: 0.4446\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 877us/step - loss: 0.4236 - val_loss: 0.4436\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 893us/step - loss: 0.4228 - val_loss: 0.4430\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4225 - val_loss: 0.4426\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4214 - val_loss: 0.4413\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 883us/step - loss: 0.4206 - val_loss: 0.4409\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4198 - val_loss: 0.4406\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4190 - val_loss: 0.4393\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4181 - val_loss: 0.4410\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 863us/step - loss: 0.4180 - val_loss: 0.4381\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4172 - val_loss: 0.4381\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4167 - val_loss: 0.4372\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4155 - val_loss: 0.4372\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4153 - val_loss: 0.4363\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4143 - val_loss: 0.4357\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4139 - val_loss: 0.4350\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4131 - val_loss: 0.4346\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 883us/step - loss: 0.4124 - val_loss: 0.4343\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4120 - val_loss: 0.4329\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4113 - val_loss: 0.4320\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 886us/step - loss: 0.4110 - val_loss: 0.4320\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 867us/step - loss: 0.4100 - val_loss: 0.4315\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4095 - val_loss: 0.4307\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4088 - val_loss: 0.4302\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4082 - val_loss: 0.4295\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4072 - val_loss: 0.4300\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.4071 - val_loss: 0.4282\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4061 - val_loss: 0.4287\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4056 - val_loss: 0.4274\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.4051 - val_loss: 0.4273\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 867us/step - loss: 0.4047 - val_loss: 0.4269\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 885us/step - loss: 0.4039 - val_loss: 0.4262\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4032 - val_loss: 0.4263\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.4030 - val_loss: 0.4246\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 879us/step - loss: 0.4020 - val_loss: 0.4242\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.4017 - val_loss: 0.4236\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4011 - val_loss: 0.4235\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4008 - val_loss: 0.4230\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4000 - val_loss: 0.4225\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.3995 - val_loss: 0.4219\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.3989 - val_loss: 0.4215\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.3979 - val_loss: 0.4208\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.3980 - val_loss: 0.4204\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.3974 - val_loss: 0.4196\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.3969 - val_loss: 0.4197\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.3964 - val_loss: 0.4191\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 867us/step - loss: 0.3956 - val_loss: 0.4184\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.3954 - val_loss: 0.4184\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.3948 - val_loss: 0.4180\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.3944 - val_loss: 0.4173\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.3938 - val_loss: 0.4164\n",
      "121/121 [==============================] - 0s 592us/step - loss: 0.4309\n",
      "[CV] END learning_rate=0.0014109707233345545, n_hidden=1, n_neurons=53; total time=  21.9s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ozing\\miniconda3\\envs\\handson-ml2\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 1ms/step - loss: 1.3135 - val_loss: 0.6660\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0332 - val_loss: 0.5614\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5128 - val_loss: 0.4725\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4434 - val_loss: 0.4255\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4144 - val_loss: 0.3983\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3871 - val_loss: 0.4121\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3745 - val_loss: 0.3847\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3654 - val_loss: 0.3964\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3577 - val_loss: 0.3619\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3497 - val_loss: 0.3666\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.3440 - val_loss: 0.3789\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3395 - val_loss: 0.3711\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3347 - val_loss: 0.3391\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3310 - val_loss: 0.3463\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3274 - val_loss: 0.3337\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3218 - val_loss: 0.3276\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.3206 - val_loss: 0.3283\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3140 - val_loss: 0.3289\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3127 - val_loss: 0.3254\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3122 - val_loss: 0.3305\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3104 - val_loss: 0.3419\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3074 - val_loss: 0.3188\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3052 - val_loss: 0.3680\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3064 - val_loss: 0.3267\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3014 - val_loss: 0.3558\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3026 - val_loss: 0.3556\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2996 - val_loss: 0.3149\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2977 - val_loss: 0.3294\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2976 - val_loss: 0.3304\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2960 - val_loss: 0.3287\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2953 - val_loss: 0.3079\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2941 - val_loss: 0.3107\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2908 - val_loss: 0.3131\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2905 - val_loss: 0.3149\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2907 - val_loss: 0.3230\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.2902 - val_loss: 0.3277\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2882 - val_loss: 0.3122\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2851 - val_loss: 0.3316\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2887 - val_loss: 0.3069\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2861 - val_loss: 0.3081\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2828 - val_loss: 0.3041\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2847 - val_loss: 0.3062\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2833 - val_loss: 0.3089\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2808 - val_loss: 0.3244\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2807 - val_loss: 0.3345\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2808 - val_loss: 0.3035\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2806 - val_loss: 0.3081\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.2777 - val_loss: 0.3022\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2777 - val_loss: 0.3037\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2767 - val_loss: 0.3012\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2745 - val_loss: 0.3077\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2749 - val_loss: 0.3051\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2765 - val_loss: 0.2968\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2740 - val_loss: 0.3089\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2736 - val_loss: 0.3045\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2734 - val_loss: 0.2978\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2716 - val_loss: 0.3260\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.2729 - val_loss: 0.3019\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.2718 - val_loss: 0.3048\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2712 - val_loss: 0.3021\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2696 - val_loss: 0.3029\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2692 - val_loss: 0.3127\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2672 - val_loss: 0.3093\n",
      "121/121 [==============================] - 0s 583us/step - loss: 0.3126\n",
      "[CV] END learning_rate=0.009930511533267052, n_hidden=3, n_neurons=68; total time=  15.8s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ozing\\miniconda3\\envs\\handson-ml2\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 2ms/step - loss: 0.8210 - val_loss: 0.5185\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5189 - val_loss: 0.4738\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4576 - val_loss: 0.4321\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4326 - val_loss: 0.4195\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.4182 - val_loss: 0.4030\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4052 - val_loss: 0.4021\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3956 - val_loss: 0.3947\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3878 - val_loss: 0.4086\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3794 - val_loss: 0.3976\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3775 - val_loss: 0.3896\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3774 - val_loss: 0.3667\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.3632 - val_loss: 0.3625\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3548 - val_loss: 0.3664\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3481 - val_loss: 0.3575\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3453 - val_loss: 0.3843\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3466 - val_loss: 0.3613\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.3410 - val_loss: 0.3547\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3374 - val_loss: 0.3586\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3341 - val_loss: 0.3383\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3345 - val_loss: 0.3338\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3275 - val_loss: 0.3385\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3285 - val_loss: 0.3413\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.3246 - val_loss: 0.3362\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.3194 - val_loss: 0.3382\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3163 - val_loss: 0.3304\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3174 - val_loss: 0.3298\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3120 - val_loss: 0.3222\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3150 - val_loss: 0.3220\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3129 - val_loss: 0.3355\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.3117 - val_loss: 0.3280\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3081 - val_loss: 0.3365\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3044 - val_loss: 0.3204\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3038 - val_loss: 0.3448\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3012 - val_loss: 0.3430\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3043 - val_loss: 0.3232\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3012 - val_loss: 0.3346\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2975 - val_loss: 0.3335\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2991 - val_loss: 0.3148\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2934 - val_loss: 0.3159\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2960 - val_loss: 0.3161\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.2939 - val_loss: 0.3137\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2913 - val_loss: 0.4169\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2897 - val_loss: 0.3270\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2913 - val_loss: 0.3394\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2899 - val_loss: 0.3310\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2897 - val_loss: 0.3120\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2857 - val_loss: 0.3302\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 998us/step - loss: 0.2860 - val_loss: 0.3068\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2822 - val_loss: 0.3145\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.2916 - val_loss: 0.3322\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2868 - val_loss: 0.3116\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2871 - val_loss: 0.3186\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2838 - val_loss: 0.3151\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2808 - val_loss: 0.3134\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2799 - val_loss: 0.3064\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2805 - val_loss: 0.3191\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.2799 - val_loss: 0.3104\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2823 - val_loss: 0.3137\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2800 - val_loss: 0.3216\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.2803 - val_loss: 0.3082\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2789 - val_loss: 0.3107\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2807 - val_loss: 0.3078\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.2850 - val_loss: 0.3132\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2982 - val_loss: 0.3224\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2820 - val_loss: 0.3507\n",
      "121/121 [==============================] - 0s 583us/step - loss: 0.3319\n",
      "[CV] END learning_rate=0.009930511533267052, n_hidden=3, n_neurons=68; total time=  16.3s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ozing\\miniconda3\\envs\\handson-ml2\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 1ms/step - loss: 1.1041 - val_loss: 0.5759\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 991us/step - loss: 0.5189 - val_loss: 0.4972\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7002 - val_loss: 0.5254\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.3897 - val_loss: 6.7200\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5524 - val_loss: 0.4475\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4211 - val_loss: 0.4140\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3834 - val_loss: 0.3830\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.3679 - val_loss: 0.4079\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.3515 - val_loss: 0.3893\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3412 - val_loss: 0.3595\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3339 - val_loss: 0.3506\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3278 - val_loss: 0.3556\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3240 - val_loss: 0.3412\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3196 - val_loss: 0.3402\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3136 - val_loss: 0.3484\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3110 - val_loss: 0.3351\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3059 - val_loss: 0.3334\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3048 - val_loss: 0.3864\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3005 - val_loss: 0.3238\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2962 - val_loss: 0.3263\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2952 - val_loss: 0.3187\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2935 - val_loss: 0.3227\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2909 - val_loss: 0.3222\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2880 - val_loss: 0.3722\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2829 - val_loss: 0.3217\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2823 - val_loss: 0.3204\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2817 - val_loss: 0.3169\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2763 - val_loss: 0.3115\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2761 - val_loss: 0.3072\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.2750 - val_loss: 0.3072\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.2720 - val_loss: 0.3127\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2714 - val_loss: 0.3076\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2693 - val_loss: 0.3116\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2669 - val_loss: 0.3201\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.2680 - val_loss: 0.3141\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2665 - val_loss: 0.3072\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.2638 - val_loss: 0.3111\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.2629 - val_loss: 0.3022\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2615 - val_loss: 0.2999\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.2589 - val_loss: 0.3075\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2594 - val_loss: 0.3032\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2570 - val_loss: 0.3043\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.2588 - val_loss: 0.3423\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.2574 - val_loss: 0.3064\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2541 - val_loss: 0.3049\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2544 - val_loss: 0.2953\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2536 - val_loss: 0.3089\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2512 - val_loss: 0.3007\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2498 - val_loss: 0.2981\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2490 - val_loss: 0.3123\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2493 - val_loss: 0.2955\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2482 - val_loss: 0.2933\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2460 - val_loss: 0.3012\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2448 - val_loss: 0.2982\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.2461 - val_loss: 0.2992\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2471 - val_loss: 0.2949\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.2443 - val_loss: 0.2974\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.2459 - val_loss: 0.3091\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2423 - val_loss: 0.2944\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2395 - val_loss: 0.2949\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2406 - val_loss: 0.2974\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2405 - val_loss: 0.2944\n",
      "121/121 [==============================] - 0s 592us/step - loss: 0.3137\n",
      "[CV] END learning_rate=0.009930511533267052, n_hidden=3, n_neurons=68; total time=  15.8s\n",
      "Epoch 1/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.7248 - val_loss: 0.4742\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 909us/step - loss: 0.4494 - val_loss: 0.4434\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 901us/step - loss: 0.4122 - val_loss: 0.4061\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 909us/step - loss: 0.3984 - val_loss: 0.4079\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 904us/step - loss: 0.3850 - val_loss: 0.3799\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3725 - val_loss: 0.3742\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 898us/step - loss: 0.3635 - val_loss: 0.3672\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 901us/step - loss: 0.3577 - val_loss: 0.3806\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 904us/step - loss: 0.3509 - val_loss: 0.3615\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 898us/step - loss: 0.3461 - val_loss: 0.3496\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 887us/step - loss: 0.3401 - val_loss: 0.3565\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 0s 905us/step - loss: 0.3356 - val_loss: 0.3613\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 906us/step - loss: 0.3295 - val_loss: 0.3632\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 915us/step - loss: 0.3268 - val_loss: 0.3494\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 904us/step - loss: 0.3238 - val_loss: 0.3513\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 887us/step - loss: 0.3238 - val_loss: 0.3318\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 0s 912us/step - loss: 0.3142 - val_loss: 0.3406\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 0s 909us/step - loss: 0.3142 - val_loss: 0.3298\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 901us/step - loss: 0.3121 - val_loss: 0.3461\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 906us/step - loss: 0.3095 - val_loss: 0.3238\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 0s 909us/step - loss: 0.3128 - val_loss: 0.3410\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 0s 909us/step - loss: 0.3060 - val_loss: 0.3471\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 0s 906us/step - loss: 0.3038 - val_loss: 0.3421\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 0s 884us/step - loss: 0.3021 - val_loss: 0.3160\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 0s 892us/step - loss: 0.2998 - val_loss: 0.3145\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 0s 913us/step - loss: 0.2980 - val_loss: 0.3196\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 0s 904us/step - loss: 0.2969 - val_loss: 0.3356\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 0s 901us/step - loss: 0.2942 - val_loss: 0.3266\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 0s 917us/step - loss: 0.2930 - val_loss: 0.3165\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 0s 909us/step - loss: 0.2906 - val_loss: 0.3134\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 0s 904us/step - loss: 0.2912 - val_loss: 0.3235\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 0s 901us/step - loss: 0.2891 - val_loss: 0.3084\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 0s 920us/step - loss: 0.2845 - val_loss: 0.3001\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 0s 884us/step - loss: 0.2866 - val_loss: 0.3072\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 0s 909us/step - loss: 0.2833 - val_loss: 0.3026\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 0s 898us/step - loss: 0.2808 - val_loss: 0.3201\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 0s 912us/step - loss: 0.2836 - val_loss: 0.3794\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 0s 890us/step - loss: 0.3030 - val_loss: 0.3037\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 0s 904us/step - loss: 0.2845 - val_loss: 0.3064\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 0s 915us/step - loss: 0.2800 - val_loss: 0.3547\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 0s 906us/step - loss: 0.2778 - val_loss: 0.3067\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 0s 904us/step - loss: 0.2758 - val_loss: 0.3205\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 0s 892us/step - loss: 0.2769 - val_loss: 0.3017\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=<keras.wrappers.scikit_learn.KerasRegressor object at 0x000001C82B11D088>,\n",
       "                   param_distributions={'learning_rate': [0.027691457623632842,\n",
       "                                                          0.007539355530202844,\n",
       "                                                          0.0021066278932862785,\n",
       "                                                          0.0028037914077228437,\n",
       "                                                          0.0013940419903195136,\n",
       "                                                          0.010827748068018995,\n",
       "                                                          0.0019805802614542693,\n",
       "                                                          0.011112004034319285,\n",
       "                                                          0.0013046412240822812,\n",
       "                                                          0.00195154977...\n",
       "                                                          0.00982568655219795,\n",
       "                                                          0.0007728221229238563,\n",
       "                                                          0.0009464552869694759,\n",
       "                                                          0.0003292760055804616,\n",
       "                                                          0.00037433836905597365,\n",
       "                                                          0.0011454953744525888,\n",
       "                                                          0.0007908712840176638,\n",
       "                                                          0.000766437332996236,\n",
       "                                                          0.012388215226631291,\n",
       "                                                          0.028353847632330967, ...],\n",
       "                                        'n_hidden': [0, 1, 2, 3],\n",
       "                                        'n_neurons': [1, 2, 3, 4, 5, 6, 7, 8, 9,\n",
       "                                                      10, 11, 12, 13, 14, 15,\n",
       "                                                      16, 17, 18, 19, 20, 21,\n",
       "                                                      22, 23, 24, 25, 26, 27,\n",
       "                                                      28, 29, 30, ...]},\n",
       "                   verbose=2)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distribs = {\n",
    "    \"n_hidden\": [0, 1, 2, 3],\n",
    "    \"n_neurons\": np.arange(1, 100).tolist(),\n",
    "    \"learning_rate\": reciprocal(3e-4, 3e-2).rvs(1000).tolist(),\n",
    "}\n",
    "\n",
    "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3, verbose=2)\n",
    "rnd_search_cv.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neurons': 68, 'n_hidden': 3, 'learning_rate': 0.009930511533267052}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.31939783692359924"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rnd_search_cv.best_estimator_.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 631us/step - loss: 0.2880\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.28802862763404846"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c4e1c02ba9a4315c9c9b9f3ccdc568bf0028a114bbf7c4447cf8df78c88a2f71"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('handson-ml2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
