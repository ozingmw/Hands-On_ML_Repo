{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_zip = tf.keras.utils.get_file('spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip', extract=True)\n",
    "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "    w = w.strip()\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> may i borrow this book ? <end>\n",
      "b'<start> ? \\xc2\\xbf puedo tomar prestado este libro ? <end>'\n"
     ]
    }
   ],
   "source": [
    "en_sentence = u\"May I borrow this book?\"\n",
    "sp_sentence = u\"?¿Puedo tomar prestado este libro?\"\n",
    "print(preprocess_sentence(en_sentence))\n",
    "print(preprocess_sentence(sp_sentence).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(path, num_examples):\n",
    "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
    "    return zip(*word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> if you want to sound like a native speaker , you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo . <end>\n",
      "<start> si quieres sonar como un hablante nativo , debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un musico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado . <end>\n"
     ]
    }
   ],
   "source": [
    "en, sp = create_dataset(path_to_file, None)\n",
    "print(en[-1])\n",
    "print(sp[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "    lang_tokenize = keras.preprocessing.text.Tokenizer(filters=\" \")\n",
    "    lang_tokenize.fit_on_texts(lang)\n",
    "    tensor = lang_tokenize.texts_to_sequences(lang)\n",
    "    tensor = keras.preprocessing.sequence.pad_sequences(tensor, padding=\"post\")\n",
    "    return tensor, lang_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path, num_examples=None):\n",
    "  targ_lang, inp_lang = create_dataset(path, num_examples)\n",
    "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_examples = 100000\n",
    "target_tensor, input_tensor, targ_lang, inp_lang = load_dataset(path_to_file)\n",
    "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(lang, tensor):\n",
    "    for t in tensor:\n",
    "        if t != 0:\n",
    "            print(\"%d =====> %s\" % (t, lang.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "1 =====> <start>\n",
      "4 =====> i\n",
      "21 =====> was\n",
      "64 =====> about\n",
      "7 =====> to\n",
      "2096 =====> suggest\n",
      "6 =====> the\n",
      "295 =====> same\n",
      "289 =====> thing\n",
      "3 =====> .\n",
      "2 =====> <end>\n",
      "\n",
      "Target Language; index to word mapping\n",
      "1 =====> <start>\n",
      "631 =====> estuve\n",
      "9 =====> a\n",
      "461 =====> punto\n",
      "8 =====> de\n",
      "8877 =====> sugerir\n",
      "20 =====> lo\n",
      "163 =====> mismo\n",
      "3 =====> .\n",
      "2 =====> <end>\n"
     ]
    }
   ],
   "source": [
    "print (\"Input Language; index to word mapping\")\n",
    "convert(inp_lang, input_tensor_train[0])\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert(targ_lang, target_tensor_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = len(input_tensor_train)\n",
    "batch_size = 512\n",
    "steps_per_epoch = len(input_tensor_train) // batch_size\n",
    "embedding_dim = 512\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word_index) + 1\n",
    "vocab_tar_size = len(targ_lang.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(buffer_size)\n",
    "dataset = dataset.batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([256, 17]), TensorShape([256, 20]))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, encoding_units, batch_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.encoding_units = encoding_units\n",
    "        self.embedding = keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = keras.layers.GRU(self.encoding_units, return_sequences=True, return_state=True, recurrent_initializer=\"glorot_uniform\")\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size, self.encoding_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([256, 17, 1024]), TensorShape([256, 1024]))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, batch_size)\n",
    "\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "sample_output.shape, sample_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanuAttention(keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanuAttention, self).__init__()\n",
    "        self.W1 = keras.layers.Dense(units)\n",
    "        self.W2 = keras.layers.Dense(units)\n",
    "        self.V = keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(query_with_time_axis) + self.W2(values)))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([256, 1024]), TensorShape([256, 17, 1]))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_layer = BahdanuAttention(20)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "attention_result.shape, attention_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, decoding_units, batch_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.decoding_units = decoding_units\n",
    "        self.embedding = keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = keras.layers.GRU(self.decoding_units, return_sequences=True, return_state=True, recurrent_initializer=\"glorot_uniform\")\n",
    "        self.fc = keras.layers.Dense(vocab_size)\n",
    "        self.attention = BahdanuAttention(self.decoding_units)\n",
    "\n",
    "    def call(self, x, hidden, encoding_output):\n",
    "        context_vector, attention_weights = self.attention(hidden, encoding_output)\n",
    "        x = self.embedding(x)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        output, state = self.gru(x)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        x = self.fc(output)\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([256, 20708])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, batch_size)\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((batch_size, 1)), sample_hidden, sample_output)\n",
    "sample_decoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(input, target, encoding_hidden):\n",
    "    loss = 0\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        encoding_output, encoding_hidden = encoder(input, encoding_hidden)\n",
    "        decoding_hidden = encoding_hidden\n",
    "        decoding_input = tf.expand_dims([targ_lang.word_index[\"<start>\"]] * batch_size, 1)\n",
    "        \n",
    "        for t in range(1, target.shape[1]):\n",
    "            predictions, decoding_hidden, _ = decoder(decoding_input, decoding_hidden, encoding_output)\n",
    "            loss += loss_function(target[:, t], predictions)\n",
    "            decoding_input = tf.expand_dims(target[:, t], 1)\n",
    "        \n",
    "        batch_loss = (loss / int(target.shape[1]))\n",
    "        variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "        return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1    Batch : 0    Loss : 3.674436569213867\n",
      "Epoch : 1    Batch : 100    Loss : 1.9611549377441406\n",
      "Epoch : 1    Batch : 200    Loss : 1.8098548650741577\n",
      "Epoch : 1    Batch : 300    Loss : 1.7883329391479492\n",
      "Time taken for 1 epoch 136.57321643829346 sec\n",
      "\n",
      "Epoch : 2    Batch : 0    Loss : 1.7404159307479858\n",
      "Epoch : 2    Batch : 100    Loss : 1.7029027938842773\n",
      "Epoch : 2    Batch : 200    Loss : 1.5274641513824463\n",
      "Epoch : 2    Batch : 300    Loss : 1.5194475650787354\n",
      "Time taken for 1 epoch 114.26237964630127 sec\n",
      "\n",
      "Epoch : 3    Batch : 0    Loss : 1.451350450515747\n",
      "Epoch : 3    Batch : 100    Loss : 1.35951828956604\n",
      "Epoch : 3    Batch : 200    Loss : 1.3536344766616821\n",
      "Epoch : 3    Batch : 300    Loss : 1.2660406827926636\n",
      "Time taken for 1 epoch 114.7772045135498 sec\n",
      "\n",
      "Epoch : 4    Batch : 0    Loss : 1.2290763854980469\n",
      "Epoch : 4    Batch : 100    Loss : 1.1705677509307861\n",
      "Epoch : 4    Batch : 200    Loss : 1.1622554063796997\n",
      "Epoch : 4    Batch : 300    Loss : 1.0734319686889648\n",
      "Time taken for 1 epoch 145.80862092971802 sec\n",
      "\n",
      "Epoch : 5    Batch : 0    Loss : 1.0790013074874878\n",
      "Epoch : 5    Batch : 100    Loss : 1.0176972150802612\n",
      "Epoch : 5    Batch : 200    Loss : 0.9326139688491821\n",
      "Epoch : 5    Batch : 300    Loss : 1.0082340240478516\n",
      "Time taken for 1 epoch 148.51922225952148 sec\n",
      "\n",
      "Epoch : 6    Batch : 0    Loss : 0.9121909141540527\n",
      "Epoch : 6    Batch : 100    Loss : 0.846522331237793\n",
      "Epoch : 6    Batch : 200    Loss : 0.7771244049072266\n",
      "Epoch : 6    Batch : 300    Loss : 0.7535572052001953\n",
      "Time taken for 1 epoch 148.83745908737183 sec\n",
      "\n",
      "Epoch : 7    Batch : 0    Loss : 0.7064842581748962\n",
      "Epoch : 7    Batch : 100    Loss : 0.7018699049949646\n",
      "Epoch : 7    Batch : 200    Loss : 0.6587724089622498\n",
      "Epoch : 7    Batch : 300    Loss : 0.6670627593994141\n",
      "Time taken for 1 epoch 148.5724642276764 sec\n",
      "\n",
      "Epoch : 8    Batch : 0    Loss : 0.576840341091156\n",
      "Epoch : 8    Batch : 100    Loss : 0.5577153563499451\n",
      "Epoch : 8    Batch : 200    Loss : 0.5911847949028015\n",
      "Epoch : 8    Batch : 300    Loss : 0.4971819519996643\n",
      "Time taken for 1 epoch 148.7336483001709 sec\n",
      "\n",
      "Epoch : 9    Batch : 0    Loss : 0.3999605178833008\n",
      "Epoch : 9    Batch : 100    Loss : 0.45000991225242615\n",
      "Epoch : 9    Batch : 200    Loss : 0.43441858887672424\n",
      "Epoch : 9    Batch : 300    Loss : 0.42762717604637146\n",
      "Time taken for 1 epoch 148.57527327537537 sec\n",
      "\n",
      "Epoch : 10    Batch : 0    Loss : 0.4049557149410248\n",
      "Epoch : 10    Batch : 100    Loss : 0.3688127398490906\n",
      "Epoch : 10    Batch : 200    Loss : 0.4112032949924469\n",
      "Epoch : 10    Batch : 300    Loss : 0.3966383635997772\n",
      "Time taken for 1 epoch 149.3630268573761 sec\n",
      "\n",
      "Epoch : 11    Batch : 0    Loss : 0.28837910294532776\n",
      "Epoch : 11    Batch : 100    Loss : 0.29527243971824646\n",
      "Epoch : 11    Batch : 200    Loss : 0.2968848645687103\n",
      "Epoch : 11    Batch : 300    Loss : 0.3354301452636719\n",
      "Time taken for 1 epoch 148.88410878181458 sec\n",
      "\n",
      "Epoch : 12    Batch : 0    Loss : 0.2719673812389374\n",
      "Epoch : 12    Batch : 100    Loss : 0.28007620573043823\n",
      "Epoch : 12    Batch : 200    Loss : 0.2776610553264618\n",
      "Epoch : 12    Batch : 300    Loss : 0.27258455753326416\n",
      "Time taken for 1 epoch 147.81697845458984 sec\n",
      "\n",
      "Epoch : 13    Batch : 0    Loss : 0.2158287614583969\n",
      "Epoch : 13    Batch : 100    Loss : 0.2045060694217682\n",
      "Epoch : 13    Batch : 200    Loss : 0.2518991529941559\n",
      "Epoch : 13    Batch : 300    Loss : 0.24234285950660706\n",
      "Time taken for 1 epoch 148.05844736099243 sec\n",
      "\n",
      "Epoch : 14    Batch : 0    Loss : 0.20922212302684784\n",
      "Epoch : 14    Batch : 100    Loss : 0.21481171250343323\n",
      "Epoch : 14    Batch : 200    Loss : 0.21121858060359955\n",
      "Epoch : 14    Batch : 300    Loss : 0.25295940041542053\n",
      "Time taken for 1 epoch 147.30175828933716 sec\n",
      "\n",
      "Epoch : 15    Batch : 0    Loss : 0.182522252202034\n",
      "Epoch : 15    Batch : 100    Loss : 0.1764688938856125\n",
      "Epoch : 15    Batch : 200    Loss : 0.19281844794750214\n",
      "Epoch : 15    Batch : 300    Loss : 0.1965891718864441\n",
      "Time taken for 1 epoch 147.52343559265137 sec\n",
      "\n",
      "Epoch : 16    Batch : 0    Loss : 0.1436782032251358\n",
      "Epoch : 16    Batch : 100    Loss : 0.1585654765367508\n",
      "Epoch : 16    Batch : 200    Loss : 0.17140084505081177\n",
      "Epoch : 16    Batch : 300    Loss : 0.17826801538467407\n",
      "Time taken for 1 epoch 147.5183870792389 sec\n",
      "\n",
      "Epoch : 17    Batch : 0    Loss : 0.12085514515638351\n",
      "Epoch : 17    Batch : 100    Loss : 0.14301587641239166\n",
      "Epoch : 17    Batch : 200    Loss : 0.16151650249958038\n",
      "Epoch : 17    Batch : 300    Loss : 0.15518787503242493\n",
      "Time taken for 1 epoch 148.0848879814148 sec\n",
      "\n",
      "Epoch : 18    Batch : 0    Loss : 0.11397325247526169\n",
      "Epoch : 18    Batch : 100    Loss : 0.1221669465303421\n",
      "Epoch : 18    Batch : 200    Loss : 0.14765293896198273\n",
      "Epoch : 18    Batch : 300    Loss : 0.14712099730968475\n",
      "Time taken for 1 epoch 147.83398413658142 sec\n",
      "\n",
      "Epoch : 19    Batch : 0    Loss : 0.10987269878387451\n",
      "Epoch : 19    Batch : 100    Loss : 0.09970545023679733\n",
      "Epoch : 19    Batch : 200    Loss : 0.13029558956623077\n",
      "Epoch : 19    Batch : 300    Loss : 0.13455913960933685\n",
      "Time taken for 1 epoch 147.63093447685242 sec\n",
      "\n",
      "Epoch : 20    Batch : 0    Loss : 0.11102873086929321\n",
      "Epoch : 20    Batch : 100    Loss : 0.09563911706209183\n",
      "Epoch : 20    Batch : 200    Loss : 0.10428764671087265\n",
      "Epoch : 20    Batch : 300    Loss : 0.116563580930233\n",
      "Time taken for 1 epoch 147.57408380508423 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    encoding_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (input, target)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(input, target, encoding_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print(f\"Epoch : {epoch+1}    Batch : {batch}    Loss : {batch_loss.numpy()}\")\n",
    "\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    \n",
    "    inputs = [inp_lang.word_index[i] for i in sentence.split(\" \")]\n",
    "    inputs = keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_inp, padding=\"post\")\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = \"\"\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    encoding_output, encoding_hidden = encoder(inputs, hidden)\n",
    "    decoding_hidden = encoding_hidden\n",
    "    decoding_input = tf.expand_dims([targ_lang.word_index[\"<start>\"]], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        prediction, decoding_hidden, attention_weights = decoder(decoding_input, decoding_hidden, encoding_output)\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "        prediction_id = tf.argmax(prediction[0]).numpy()\n",
    "        result += targ_lang.index_word[prediction_id] + \" \"\n",
    "        \n",
    "        if targ_lang.index_word[prediction_id] == \"<end>\":\n",
    "            return result, sentence, attention_plot\n",
    "        \n",
    "        decoding_input = tf.expand_dims([prediction_id], 0)\n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "  result, sentence, attention_plot = evaluate(sentence)\n",
    "\n",
    "  print('Input: %s' % (sentence))\n",
    "  print('Predicted translation: {}'.format(result))\n",
    "\n",
    "  attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "  plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> moon sun <end>\n",
      "Predicted translation: la luna enemigo . <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ozing\\miniconda3\\envs\\handson-ml2\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  \n",
      "C:\\Users\\ozing\\miniconda3\\envs\\handson-ml2\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate(u\"moon sun\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c4e1c02ba9a4315c9c9b9f3ccdc568bf0028a114bbf7c4447cf8df78c88a2f71"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 ('handson-ml2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
