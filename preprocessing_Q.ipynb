{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, test_size=5000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(len(X_train))\n",
    "test_set = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "valid_set = tf.data.Dataset.from_tensor_slices((X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.train import Feature, Features, Example, BytesList, Int64List\n",
    "\n",
    "def create_example(image, label):\n",
    "    image_data = tf.io.serialize_tensor(image)\n",
    "    return Example(\n",
    "        features=Features(\n",
    "            feature={\n",
    "                \"image\": Feature(bytes_list=BytesList(value=[image_data.numpy()])),\n",
    "                \"label\": Feature(int64_list=Int64List(value=[label]))\n",
    "            }\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features {\n",
      "  feature {\n",
      "    key: \"image\"\n",
      "    value {\n",
      "      bytes_list {\n",
      "        value: \"\\010\\004\\022\\010\\022\\002\\010\\034\\022\\002\\010\\034\\\"\\220\\006\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\002\\004\\000\\000\\223}jq\\033\\000\\001\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\001\\001\\000\\000\\000\\003\\000\\033\\306;\\000\\000\\020|(\\000\\002\\001\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\001\\002\\000\\002\\000\\2552\\000\\000\\000\\000\\000\\211\\000\\000\\000\\001\\001\\000\\000\\001\\000\\000\\000\\000\\000\\000\\000\\000\\001\\000\\000\\003\\214\\000\\000\\000\\000\\n\\000e-\\000\\004\\000\\000\\001\\001\\000\\000\\000\\000\\000\\000\\000\\001\\000\\000\\004\\000\\035\\221\\000\\000\\002\\002\\003\\000+e\\000\\007\\000\\000\\000\\001\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\002\\000i\\213\\000\\000\\000\\000\\001\\000\\007\\233\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\001\\003\\005\\000\\001\\002\\000\\000\\270}\\000\\003\\001\\002\\004\\000\\000\\220\\010\\000\\000\\000\\000\\000\\003\\001\\000\\001\\000\\000\\000\\000\\000\\000\\000\\000\\347,\\000\\000\\000\\000\\000\\000\\000\\230$\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000ccRTS;K\\317D%==?=;6\\267p6el\\\\JhC\\004\\000\\n\\264\\273\\263\\260\\302\\267\\241\\225\\226\\251\\260\\265\\270\\266\\254\\274\\247\\243\\250\\272\\271\\251\\234\\2409\\000\\000\\002R\\256\\242\\242\\207\\225\\234\\211\\212\\200\\202\\206\\211\\212\\210\\220\\227\\210\\224\\217\\247\\233\\217\\236\\000\\000)\\247\\017\\331\\243\\251\\225\\231\\223\\203\\231\\223\\212\\206\\205\\215\\221\\227\\231s\\237\\226\\262\\241\\223\\251\\000\\000\\311\\377\\000\\335\\241\\243\\234\\231\\246\\243\\216\\217\\212\\201\\177\\212\\222\\231\\246\\236\\221\\225\\260\\242\\220\\235\\000\\000\\331x\\000\\341\\244\\235\\234\\232\\243\\253\\222\\226\\227\\226\\230\\230\\232\\226\\235\\252\\240\\241\\265\\253\\224\\225\\0005\\352\\010\\000\\342\\257\\226\\227\\240\\237\\240\\240\\235\\235\\233\\232\\232\\235\\240\\245\\243\\251\\240\\261\\256\\224\\207\\000u\\366\\000\\000\\323\\267\\224\\223\\243\\241\\242\\240\\237\\242\\235\\231\\235\\242\\241\\241\\245\\247\\242\\261\\264\\235n\\000\\223\\255\\000\\000\\311\\274\\224\\217\\243\\243\\243\\242\\241\\242\\243\\244\\243\\241\\241\\242\\244\\245\\243\\261\\261\\255L\\000\\226\\235\\000\\000\\277\\276\\215\\227\\242\\242\\243\\244\\242\\242\\244\\245\\245\\243\\244\\245\\245\\245\\247\\253\\263\\265,\\000\\207\\226\\000\\000\\232\\303\\205\\230\\244\\243\\246\\247\\245\\250\\247\\246\\247\\250\\246\\247\\247\\250\\247\\247\\270\\251\\027\\000D\\274\\000\\000~\\350\\203\\240\\246\\247\\244\\246\\247\\246\\246\\242\\246\\247\\247\\247\\251\\251\\245\\245\\272\\246\\000\\000\\000\\235N\\000\\031\\344\\205\\237\\257\\255\\252\\261\\254\\247\\253\\246\\251\\244\\250\\247\\250\\247\\257\\235\\301\\204\\000\\000\\000\\000p?\\201\\346\\222\\247\\252\\247\\256\\251\\255\\254\\235\\243\\235\\244\\241\\242\\243\\242\\244\\240\\305p\\000\\000\\000\\000Y\\267\\272\\311\\275\\324\\333\\347\\351\\343\\332\\303\\264\\254\\255\\252\\247\\250\\252\\252\\255\\262\\331O\\000\\001\\002\\000p\\233xu_UPIVeF\\237\\367\\350\\342\\341\\334\\322\\304\\271\\270\\313\\217\\000\\000\\001\\002\\000\\025enz\\224\\250\\270\\301\\272\\267\\341+\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\001\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"label\"\n",
      "    value {\n",
      "      int64_list {\n",
      "        value: 8\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for image, label in valid_set.take(1):\n",
    "    print(create_example(image, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import ExitStack\n",
    "\n",
    "def to_tfrecords(name, dataset, n_shards=5):\n",
    "    paths = [\"{}.tfrecord-{:02d}-of-{:02d}\".format(name, index, n_shards) for index in range(n_shards)]\n",
    "    with ExitStack() as stack:\n",
    "        writers = [stack.enter_context(tf.io.TFRecordWriter(path)) for path in paths]\n",
    "        for index, (image, label) in dataset.enumerate():\n",
    "            shard = index % n_shards\n",
    "            example = create_example(image, label)\n",
    "            writers[shard].write(example.SerializeToString())\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filepaths = to_tfrecords(\"./tfrecords/my_fashion_mnist.train\", train_set)\n",
    "valid_filepaths = to_tfrecords(\"./tfrecords/my_fashion_mnist.valid\", valid_set)\n",
    "test_filepaths = to_tfrecords(\"./tfrecords/my_fashion_mnist.test\", test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(tfrecord):\n",
    "    feature_descriptions={\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n",
    "        \"label\": tf.io.FixedLenFeature([], tf.int64, default_value=-1)\n",
    "    }\n",
    "    example = tf.io.parse_single_example(tfrecord, feature_descriptions)\n",
    "    image = tf.io.parse_tensor(example[\"image\"], out_type=tf.uint8)\n",
    "    image = tf.reshape(image, shape=[28, 28])\n",
    "    return image, example[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_dataset(filepaths, n_read_threads=5, shuffle_buffer_size=None, n_parse_threads=5, batch_size=32, cache=True):\n",
    "    dataset = tf.data.TFRecordDataset(filepaths, num_parallel_reads=n_read_threads)\n",
    "    if cache:\n",
    "        dataset = dataset.cache()\n",
    "    if shuffle_buffer_size:\n",
    "        dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "    dataset = dataset.map(preprocessing, num_parallel_calls=n_parse_threads)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = mnist_dataset(train_filepaths, shuffle_buffer_size=60000)\n",
    "valid_set = mnist_dataset(valid_filepaths)\n",
    "test_set = mnist_dataset(test_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for X, y in valid_set.take(2):\n",
    "    for i in range(5):\n",
    "        plt.subplot(1, 5, i+1)\n",
    "        plt.imshow(X[i].numpy(), cmap=\"binary\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(str(y[i].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Standardization(keras.layers.Layer):\n",
    "    def adapt(self, data_sample):\n",
    "        self.means_ = np.mean(data_sample, axis=0, keepdims=True)\n",
    "        self.stds_ = np.std(data_sample, axis=0, keepdims=True)\n",
    "    def call(self, inputs):\n",
    "        return (inputs - self.means_) / (self.stds_ + keras.backend.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardization = Standardization(input_shape=[28, 28])\n",
    "standardization = keras.layers.Normalization(input_shape=[28, 28])\n",
    "\n",
    "sample_image_batches = train_set.take(420).map(lambda image, label: image)\n",
    "sample_images = np.concatenate(list(sample_image_batches.as_numpy_iterator()), axis=0).astype(np.float32)\n",
    "standardization.adapt(sample_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    standardization,\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4435 - accuracy: 0.8417 - val_loss: 0.3685 - val_accuracy: 0.8674\n",
      "Epoch 2/100\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3333 - accuracy: 0.8784 - val_loss: 0.3484 - val_accuracy: 0.8728\n",
      "Epoch 3/100\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2948 - accuracy: 0.8918 - val_loss: 0.3558 - val_accuracy: 0.8710\n",
      "Epoch 4/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2703 - accuracy: 0.9001 - val_loss: 0.3443 - val_accuracy: 0.8804\n",
      "Epoch 5/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2522 - accuracy: 0.9079 - val_loss: 0.3607 - val_accuracy: 0.8694\n",
      "Epoch 6/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2340 - accuracy: 0.9131 - val_loss: 0.3436 - val_accuracy: 0.8790\n",
      "Epoch 7/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2198 - accuracy: 0.9189 - val_loss: 0.3505 - val_accuracy: 0.8810\n",
      "Epoch 8/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2085 - accuracy: 0.9218 - val_loss: 0.3686 - val_accuracy: 0.8800\n",
      "Epoch 9/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1992 - accuracy: 0.9267 - val_loss: 0.3629 - val_accuracy: 0.8862\n",
      "Epoch 10/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1868 - accuracy: 0.9308 - val_loss: 0.3826 - val_accuracy: 0.8790\n",
      "Epoch 11/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1777 - accuracy: 0.9347 - val_loss: 0.3596 - val_accuracy: 0.8864\n",
      "Epoch 12/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1702 - accuracy: 0.9360 - val_loss: 0.3852 - val_accuracy: 0.8830\n",
      "Epoch 13/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1627 - accuracy: 0.9396 - val_loss: 0.3841 - val_accuracy: 0.8860\n",
      "Epoch 14/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1532 - accuracy: 0.9427 - val_loss: 0.3996 - val_accuracy: 0.8820\n",
      "Epoch 15/100\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.1479 - accuracy: 0.9447 - val_loss: 0.4113 - val_accuracy: 0.8812\n",
      "Epoch 16/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1434 - accuracy: 0.9470 - val_loss: 0.4161 - val_accuracy: 0.8824\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28c559d4108>"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "logs = os.path.join(os.curdir, \"my_logs\", \"run_\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
    "\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(log_dir=logs, histogram_freq=1, profile_batch=10)\n",
    "earlystopping_cb = keras.callbacks.EarlyStopping(patience=10)\n",
    "\n",
    "model.fit(train_set, epochs=100, validation_data=valid_set, callbacks=[tensorboard_cb, earlystopping_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/ozing/.keras/datasets/aclImdb')"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DOWNLOAD_ROOT = \"http://ai.stanford.edu/~amaas/data/sentiment/\"\n",
    "FILENAME = \"aclImdb_v1.tar.gz\"\n",
    "filepath = keras.utils.get_file(FILENAME, DOWNLOAD_ROOT + FILENAME, extract=True)\n",
    "path = Path(filepath).parent / \"aclImdb\"\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aclImdb\\\n",
      "    README\n",
      "    imdb.vocab\n",
      "    imdbEr.txt\n",
      "    test\\\n",
      "        labeledBow.feat\n",
      "        urls_neg.txt\n",
      "        urls_pos.txt\n",
      "        neg\\\n",
      "            0_2.txt\n",
      "            10000_4.txt\n",
      "            10001_1.txt\n",
      "            ...\n",
      "        pos\\\n",
      "            0_10.txt\n",
      "            10000_7.txt\n",
      "            10001_9.txt\n",
      "            ...\n",
      "    train\\\n",
      "        labeledBow.feat\n",
      "        unsupBow.feat\n",
      "        urls_neg.txt\n",
      "        ...\n",
      "        neg\\\n",
      "            0_3.txt\n",
      "            10000_4.txt\n",
      "            10001_4.txt\n",
      "            ...\n",
      "        pos\\\n",
      "            0_9.txt\n",
      "            10000_8.txt\n",
      "            10001_10.txt\n",
      "            ...\n",
      "        unsup\\\n",
      "            0_0.txt\n",
      "            10000_0.txt\n",
      "            10001_0.txt\n",
      "            ...\n"
     ]
    }
   ],
   "source": [
    "for name, subdirs, files in os.walk(path):\n",
    "    indent = len(Path(name).parts) - len(path.parts)\n",
    "    print(\"    \" * indent + Path(name).parts[-1] + os.sep)\n",
    "    for index, filename in enumerate(sorted(files)):\n",
    "        if index == 3:\n",
    "            print(\"    \" * (indent + 1) + \"...\")\n",
    "            break\n",
    "        print(\"    \" * (indent + 1) + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12500, 12500, 12500, 12500)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def review_paths(dirpath):\n",
    "    return [str(path) for path in dirpath.glob(\"*.txt\")]\n",
    "\n",
    "train_pos = review_paths(path / \"train\" / \"pos\")\n",
    "train_neg = review_paths(path / \"train\" / \"neg\")\n",
    "test_valid_pos = review_paths(path / \"test\" / \"pos\")\n",
    "test_valid_neg = review_paths(path / \"test\" / \"neg\")\n",
    "\n",
    "len(train_pos), len(train_neg), len(test_valid_pos), len(test_valid_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(test_valid_pos)\n",
    "\n",
    "test_pos = test_valid_pos[:5000]\n",
    "test_neg = test_valid_neg[:5000]\n",
    "valid_pos = test_valid_pos[5000:]\n",
    "valid_neg = test_valid_neg[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋이 메모리에 적당할 때, 정확도 100%?\n",
    "def imdb(filepath_pos, filepath_neg):\n",
    "    reviews = []\n",
    "    labels = []\n",
    "    for filepaths, label in ((filepath_pos, 0), (filepath_neg, 1)):\n",
    "        for filepath in filepaths:\n",
    "            with open(filepath) as review_file:\n",
    "                reviews.append(review_file.read())\n",
    "            labels.append(label)\n",
    "        return tf.data.Dataset.from_tensor_slices((tf.constant(reviews), tf.constant(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High\\'s satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers\\' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I\\'m here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn\\'t!', shape=(), dtype=string)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for X, y in imdb(train_pos, train_neg).take(1):\n",
    "    print(X)\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.3 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -r1 for X, y in imdb(train_pos, train_neg).repeat(10): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋이 메모리에 적당하지 않을 때, 정확도:77%\n",
    "def imdb_dataset(filepaths_positive, filepaths_negative, n_read_threads=5):\n",
    "    dataset_neg = tf.data.TextLineDataset(filepaths_negative, num_parallel_reads=n_read_threads)\n",
    "    dataset_neg = dataset_neg.map(lambda review: (review, 0))\n",
    "    dataset_pos = tf.data.TextLineDataset(filepaths_positive, num_parallel_reads=n_read_threads)\n",
    "    dataset_pos = dataset_pos.map(lambda review: (review, 1))\n",
    "    return tf.data.Dataset.concatenate(dataset_pos, dataset_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_set = imdb_dataset(train_pos, train_neg).shuffle(25000).batch(batch_size).prefetch(1)\n",
    "valid_set = imdb_dataset(valid_pos, valid_neg).batch(batch_size).prefetch(1)\n",
    "test_set = imdb_dataset(test_pos, test_neg).batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X_batch, n_words=100):\n",
    "    shape = tf.shape(X_batch) * tf.constant([1, 0]) + tf.constant([0, n_words])\n",
    "    Z = tf.strings.substr(X_batch, 0, 300)\n",
    "    Z = tf.strings.lower(Z)\n",
    "    Z = tf.strings.regex_replace(Z, b\"<br\\\\s*/?>\", b\" \")\n",
    "    Z = tf.strings.regex_replace(Z, b\"[^a-z]\", b\" \")\n",
    "    Z = tf.strings.split(Z)\n",
    "    return Z.to_tensor(shape=shape, default_value=b\"<pad>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 50), dtype=string, numpy=\n",
       "array([[b'it', b's', b'a', b'great', b'great', b'movie', b'i', b'loved',\n",
       "        b'it', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>'],\n",
       "       [b'it', b'was', b'terrible', b'run', b'away', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>']], dtype=object)>"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_example = tf.constant([\"It's a great, great movie! I loved it.\", \"It was terrible, run away!!!\"])\n",
    "preprocess(X_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_vocabulary(data_sample, max_size=1000):\n",
    "    preprocessed_reviews = preprocess(data_sample).numpy()\n",
    "    counter = Counter()\n",
    "    for words in preprocessed_reviews:\n",
    "        for word in words:\n",
    "            if word != b\"<pad>\":\n",
    "                counter[word] += 1\n",
    "    return [b\"<pad>\"] + [word for word, count in counter.most_common(max_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'<pad>',\n",
       " b'it',\n",
       " b'great',\n",
       " b's',\n",
       " b'a',\n",
       " b'movie',\n",
       " b'i',\n",
       " b'loved',\n",
       " b'was',\n",
       " b'terrible',\n",
       " b'run',\n",
       " b'away']"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_vocabulary(X_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextVectorization(keras.layers.Layer):\n",
    "    def __init__(self, max_vocabulary_size=1000, n_oov_buckets=100, dtype=tf.string, **kwargs):\n",
    "        super().__init__(dtype=dtype, **kwargs)\n",
    "        self.max_vocabulary_size = max_vocabulary_size\n",
    "        self.n_oov_buckets = n_oov_buckets\n",
    "\n",
    "    def adapt(self, data_sample):\n",
    "        self.vocab = get_vocabulary(data_sample, self.max_vocabulary_size)\n",
    "        words = tf.constant(self.vocab)\n",
    "        word_ids = tf.range(len(self.vocab), dtype=tf.int64)\n",
    "        vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
    "        self.table = tf.lookup.StaticVocabularyTable(vocab_init, self.n_oov_buckets)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        preprocessed_inputs = preprocess(inputs)\n",
    "        return self.table.lookup(preprocessed_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 50), dtype=int64, numpy=\n",
       "array([[ 1,  3,  4,  2,  2,  5,  6,  7,  1,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0],\n",
       "       [ 1,  8,  9, 10, 11,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0]], dtype=int64)>"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorization = TextVectorization()\n",
    "text_vectorization.adapt(X_example)\n",
    "text_vectorization(X_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 100), dtype=int64, numpy=\n",
       "array([[  7,  14,   2,  64,  64,  12,   5, 265,   7,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  7,  13, 271, 526, 336,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0]], dtype=int64)>"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_vocabulary_size = 1000\n",
    "n_oov_buckets = 100\n",
    "sample_review_batches = train_set.map(lambda review, label: review)\n",
    "sample_reviews = np.concatenate(list(sample_review_batches.as_numpy_iterator()), axis=0)\n",
    "text_vectorization = TextVectorization(max_vocabulary_size, n_oov_buckets, input_shape=[])\n",
    "text_vectorization.adapt(sample_reviews)\n",
    "text_vectorization(X_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'<pad>', b'the', b'a', b'and', b'of', b'i', b'to', b'it', b'is', b'this']"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorization.vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 8), dtype=int64, numpy=\n",
       "array([[ 30,   4,  85,  85,  18,  10, 434,   9],\n",
       "       [  9,  14, 384, 530, 242,   0,   0,   0]], dtype=int64)>"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorization = keras.layers.TextVectorization(max_tokens=max_vocabulary_size)\n",
    "text_vectorization.adapt(sample_reviews)\n",
    "text_vectorization(X_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
       "array([[2., 2., 0., 1.],\n",
       "       [3., 0., 2., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_example = tf.constant([[1,3,1,0,0], [2,2,0,0,0]])\n",
    "tf.reduce_sum(tf.one_hot(simple_example, 4), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BagOfWords(keras.layers.Layer):\n",
    "    def __init__(self, n_tokens, dtype=tf.int32, **kwargs):\n",
    "        super().__init__(dtype=dtype, **kwargs)\n",
    "        self.n_tokens = n_tokens\n",
    "    def call(self, inputs):\n",
    "        one_hot = tf.one_hot(inputs, self.n_tokens)\n",
    "        return tf.reduce_sum(one_hot, axis=1)[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[2., 0., 1.],\n",
       "       [0., 2., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words = BagOfWords(n_tokens=4)\n",
    "bag_of_words(simple_example)\n",
    "# index 0 제외 1,2,3 순서 (1 2개, 3 1개)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tokens = max_vocabulary_size + n_oov_buckets + 1\n",
    "bag_of_words = BagOfWords(n_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 35s 42ms/step - loss: 0.4111 - accuracy: 0.8250 - val_loss: 0.3433 - val_accuracy: 0.8567\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 34s 42ms/step - loss: 0.3399 - accuracy: 0.8579 - val_loss: 0.4340 - val_accuracy: 0.8239\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 34s 42ms/step - loss: 0.3076 - accuracy: 0.8713 - val_loss: 0.3627 - val_accuracy: 0.8439\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 34s 42ms/step - loss: 0.2673 - accuracy: 0.8896 - val_loss: 0.3688 - val_accuracy: 0.8391\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 33s 42ms/step - loss: 0.2223 - accuracy: 0.9101 - val_loss: 0.3553 - val_accuracy: 0.8522\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28c58596dc8>"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    text_vectorization,\n",
    "    bag_of_words,\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(train_set, epochs=5, validation_data=valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 8s 25ms/step - loss: 0.3469 - accuracy: 0.8589\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.34688103199005127, 0.8589000105857849]"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_embedding(inputs):\n",
    "    not_pad = tf.math.count_nonzero(inputs, axis=-1)\n",
    "    n_words = tf.math.count_nonzero(not_pad, axis=-1, keepdims=True)\n",
    "    sqrt_n_words = tf.math.sqrt(tf.cast(n_words, tf.float32))\n",
    "    return tf.reduce_sum(inputs, axis=1) / sqrt_n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[4.9497476 , 4.9497476 , 0.70710677],\n",
       "       [2.828427  , 1.4142135 , 0.        ]], dtype=float32)>"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "another_example = tf.constant([[[3., 2., 1.], [4., 5., 0.], [0., 0., 0.]], [[1., 2., 0.], [3., 0., 0.], [0., 0., 0.]]])\n",
    "compute_mean_embedding(another_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 10s 11ms/step - loss: 0.4815 - accuracy: 0.7633 - val_loss: 0.4071 - val_accuracy: 0.8068\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 0.3453 - accuracy: 0.8505 - val_loss: 0.3447 - val_accuracy: 0.8495\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 0.3239 - accuracy: 0.8620 - val_loss: 0.3232 - val_accuracy: 0.8585\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 0.3140 - accuracy: 0.8657 - val_loss: 0.3225 - val_accuracy: 0.8589\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 0.3089 - accuracy: 0.8668 - val_loss: 0.3218 - val_accuracy: 0.8601\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28c5885ca48>"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_size = 20\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    text_vectorization,\n",
    "    keras.layers.Embedding(input_dim=n_tokens, output_dim=embedding_size, mask_zero=True),\n",
    "    keras.layers.Lambda(compute_mean_embedding),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(train_set, epochs=5, validation_data=valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 6ms/step - loss: 0.3072 - accuracy: 0.8663\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.30724701285362244, 0.8662999868392944]"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "datasets = tfds.load(name=\"imdb_reviews\")\n",
    "train_set, test_set = datasets[\"train\"], datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\", shape=(), dtype=string)\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for example in train_set.take(1):\n",
    "    print(example[\"text\"])\n",
    "    print(example[\"label\"])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c4e1c02ba9a4315c9c9b9f3ccdc568bf0028a114bbf7c4447cf8df78c88a2f71"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('handson-ml2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
