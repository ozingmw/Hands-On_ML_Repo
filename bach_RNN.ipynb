{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOAD_ROOT = \"https://github.com/ageron/handson-ml2/raw/master/datasets/jsb_chorales/\"\n",
    "FILENAME = \"jsb_chorales.tgz\"\n",
    "filepath = keras.utils.get_file(FILENAME,\n",
    "                                DOWNLOAD_ROOT + FILENAME,\n",
    "                                cache_subdir=\"datasets/jsb_chorales\",\n",
    "                                extract=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsb_chorales_dir = Path(filepath).parent\n",
    "train_files = sorted(jsb_chorales_dir.glob(\"train/chorale_*.csv\"))\n",
    "valid_files = sorted(jsb_chorales_dir.glob(\"valid/chorale_*.csv\"))\n",
    "test_files = sorted(jsb_chorales_dir.glob(\"test/chorale_*.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_chorales(filepaths):\n",
    "    return [pd.read_csv(filepath).values.tolist() for filepath in filepaths]\n",
    "\n",
    "train_chorales = load_chorales(train_files)\n",
    "valid_chorales = load_chorales(valid_files)\n",
    "test_chorales = load_chorales(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = set()\n",
    "for chorales in (train_chorales, valid_chorales, test_chorales):\n",
    "    for chorale in chorales:\n",
    "        for chord in chorale:\n",
    "            notes |= set(chord)\n",
    "\n",
    "n_notes = len(notes)\n",
    "min_note = min(notes - {0})\n",
    "max_note = max(notes)\n",
    "\n",
    "assert min_note == 36\n",
    "assert max_note == 81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import Audio\n",
    "\n",
    "\n",
    "def notes_to_frequencies(notes):\n",
    "    # 한 옥타브 올라갈 때 주파수는 두배가 됩니다; 옥타브마다 12개의 반음이 있습니다;\n",
    "    # 옥타브 4의 A는 440Hz이고 음표 번호는 69입니다.\n",
    "    return 2 ** ((np.array(notes) - 69) / 12) * 440\n",
    "\n",
    "def frequencies_to_samples(frequencies, tempo, sample_rate):\n",
    "    note_duration = 60 / tempo # tempo는 분당 박자 수로 측정합니다\n",
    "    # 매 박자마다 딸깍거리는 소리를 줄이기 위해 주파수를 반올림하여 각 음의 끝에서 샘플을 0에 가깝게 만듭니다.\n",
    "    frequencies = np.round(note_duration * frequencies) / note_duration\n",
    "    n_samples = int(note_duration * sample_rate)\n",
    "    time = np.linspace(0, note_duration, n_samples)\n",
    "    sine_waves = np.sin(2 * np.pi * frequencies.reshape(-1, 1) * time)\n",
    "    # (음표 0 = 무음을 포함해) 9Hz 이하인 주파수는 모두 삭제합니다\n",
    "    sine_waves *= (frequencies > 9.).reshape(-1, 1)\n",
    "    return sine_waves.reshape(-1)\n",
    "\n",
    "def chords_to_samples(chords, tempo, sample_rate):\n",
    "    freqs = notes_to_frequencies(chords)\n",
    "    freqs = np.r_[freqs, freqs[-1:]] # 마지막 음표를 조금 더 길게합니다\n",
    "    merged = np.mean([frequencies_to_samples(melody, tempo, sample_rate)\n",
    "                     for melody in freqs.T], axis=0)\n",
    "    n_fade_out_samples = sample_rate * 60 // tempo # 마지막 음을 희미하게 합니다\n",
    "    fade_out = np.linspace(1., 0., n_fade_out_samples)**2\n",
    "    merged[-n_fade_out_samples:] *= fade_out\n",
    "    return merged\n",
    "\n",
    "def play_chords(chords, tempo=160, amplitude=0.1, sample_rate=44100, filepath=None):\n",
    "    samples = amplitude * chords_to_samples(chords, tempo, sample_rate)\n",
    "    if filepath:\n",
    "        from scipy.io import wavfile\n",
    "        samples = (2**15 * samples).astype(np.int16)\n",
    "        wavfile.write(filepath, sample_rate, samples)\n",
    "        return Audio(filepath)\n",
    "    else:\n",
    "        return Audio(samples, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(3):\n",
    "    play_chords(train_chorales[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target(batch):\n",
    "    X = batch[:, :-1]\n",
    "    Y = batch[:, 1:] # 각 스텝에서 아르페지오에 있는 다음 음표를 예측합니다\n",
    "    return X, Y\n",
    "\n",
    "def preprocess(window):\n",
    "    window = tf.where(window == 0, window, window - min_note + 1) # 값 이동\n",
    "    return tf.reshape(window, [-1]) # 아르페지오로 변환\n",
    "\n",
    "def bach_dataset(chorales, batch_size=32, shuffle_buffer_size=None,\n",
    "                 window_size=32, window_shift=16, cache=True):\n",
    "    def batch_window(window):\n",
    "        return window.batch(window_size + 1)\n",
    "\n",
    "    def to_windows(chorale):\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(chorale)\n",
    "        dataset = dataset.window(window_size + 1, window_shift, drop_remainder=True)\n",
    "        return dataset.flat_map(batch_window)\n",
    "\n",
    "    chorales = tf.ragged.constant(chorales, ragged_rank=1)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(chorales)\n",
    "    dataset = dataset.flat_map(to_windows).map(preprocess)\n",
    "    if cache:\n",
    "        dataset = dataset.cache()\n",
    "    if shuffle_buffer_size:\n",
    "        dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(create_target)\n",
    "    return dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = bach_dataset(train_chorales, shuffle_buffer_size=1000)\n",
    "valid_set = bach_dataset(valid_chorales)\n",
    "test_set = bach_dataset(test_chorales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embedding_dims = 5\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(input_dim=n_notes, output_dim=n_embedding_dims,\n",
    "                           input_shape=[None]),\n",
    "    keras.layers.Conv1D(32, kernel_size=2, padding=\"causal\", activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv1D(48, kernel_size=2, padding=\"causal\", activation=\"relu\", dilation_rate=2),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv1D(64, kernel_size=2, padding=\"causal\", activation=\"relu\", dilation_rate=4),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv1D(96, kernel_size=2, padding=\"causal\", activation=\"relu\", dilation_rate=8),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.LSTM(256, return_sequences=True),\n",
    "    keras.layers.Dense(n_notes, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "98/98 [==============================] - 19s 45ms/step - loss: 1.7889 - accuracy: 0.5475 - val_loss: 3.8022 - val_accuracy: 0.1122\n",
      "Epoch 2/20\n",
      "98/98 [==============================] - 3s 34ms/step - loss: 0.8614 - accuracy: 0.7704 - val_loss: 4.2623 - val_accuracy: 0.1353\n",
      "Epoch 3/20\n",
      "98/98 [==============================] - 3s 35ms/step - loss: 0.7273 - accuracy: 0.7970 - val_loss: 3.7935 - val_accuracy: 0.1185\n",
      "Epoch 4/20\n",
      "98/98 [==============================] - 3s 34ms/step - loss: 0.6574 - accuracy: 0.8114 - val_loss: 2.9618 - val_accuracy: 0.2262\n",
      "Epoch 5/20\n",
      "98/98 [==============================] - 3s 34ms/step - loss: 0.6082 - accuracy: 0.8222 - val_loss: 2.1722 - val_accuracy: 0.3525\n",
      "Epoch 6/20\n",
      "98/98 [==============================] - 3s 35ms/step - loss: 0.5665 - accuracy: 0.8316 - val_loss: 1.0832 - val_accuracy: 0.6702\n",
      "Epoch 7/20\n",
      "98/98 [==============================] - 3s 34ms/step - loss: 0.5306 - accuracy: 0.8401 - val_loss: 0.8031 - val_accuracy: 0.7609\n",
      "Epoch 8/20\n",
      "98/98 [==============================] - 3s 34ms/step - loss: 0.5005 - accuracy: 0.8478 - val_loss: 0.6602 - val_accuracy: 0.8085\n",
      "Epoch 9/20\n",
      "98/98 [==============================] - 3s 34ms/step - loss: 0.4723 - accuracy: 0.8553 - val_loss: 0.6040 - val_accuracy: 0.8242\n",
      "Epoch 10/20\n",
      "98/98 [==============================] - 3s 35ms/step - loss: 0.4465 - accuracy: 0.8621 - val_loss: 0.6770 - val_accuracy: 0.8011\n",
      "Epoch 11/20\n",
      "98/98 [==============================] - 3s 34ms/step - loss: 0.4253 - accuracy: 0.8679 - val_loss: 0.6229 - val_accuracy: 0.8171\n",
      "Epoch 12/20\n",
      "98/98 [==============================] - 3s 34ms/step - loss: 0.4019 - accuracy: 0.8748 - val_loss: 0.6603 - val_accuracy: 0.8094\n",
      "Epoch 13/20\n",
      "98/98 [==============================] - 3s 35ms/step - loss: 0.3815 - accuracy: 0.8810 - val_loss: 0.6341 - val_accuracy: 0.8167\n",
      "Epoch 14/20\n",
      "98/98 [==============================] - 3s 34ms/step - loss: 0.3602 - accuracy: 0.8874 - val_loss: 0.6206 - val_accuracy: 0.8205\n",
      "Epoch 15/20\n",
      "98/98 [==============================] - 3s 35ms/step - loss: 0.3425 - accuracy: 0.8928 - val_loss: 0.6484 - val_accuracy: 0.8147\n",
      "Epoch 16/20\n",
      "98/98 [==============================] - 3s 35ms/step - loss: 0.3254 - accuracy: 0.8982 - val_loss: 0.6690 - val_accuracy: 0.8055\n",
      "Epoch 17/20\n",
      "98/98 [==============================] - 3s 34ms/step - loss: 0.4398 - accuracy: 0.8622 - val_loss: 0.6946 - val_accuracy: 0.7996\n",
      "Epoch 18/20\n",
      "98/98 [==============================] - 3s 34ms/step - loss: 0.3555 - accuracy: 0.8873 - val_loss: 0.6186 - val_accuracy: 0.8214\n",
      "Epoch 19/20\n",
      "98/98 [==============================] - 3s 34ms/step - loss: 0.3117 - accuracy: 0.9020 - val_loss: 0.6159 - val_accuracy: 0.8224\n",
      "Epoch 20/20\n",
      "98/98 [==============================] - 3s 35ms/step - loss: 0.2938 - accuracy: 0.9072 - val_loss: 0.6419 - val_accuracy: 0.8154\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1abab1cb8c8>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Nadam(learning_rate=1e-3)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit(train_set, epochs=20, validation_data=valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c4e1c02ba9a4315c9c9b9f3ccdc568bf0028a114bbf7c4447cf8df78c88a2f71"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 ('handson-ml2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
