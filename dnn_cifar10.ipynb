{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "X_train = X_train_full[5000:]\n",
    "y_train = y_train_full[5000:]\n",
    "X_valid = X_train_full[:5000]\n",
    "y_valid = y_train_full[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for layer in range(20):\n",
    "    model.add(keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=keras.optimizers.Nadam(learning_rate=5e-5), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "cb_earlystopping = keras.callbacks.EarlyStopping(patience=10)\n",
    "cb_model_checkpoint = keras.callbacks.ModelCheckpoint(\"my_cifar10_model.h5\", save_best_only=True)\n",
    "run_index = 3\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_{:03d}\".format(run_index))\n",
    "cb_tensorboard = keras.callbacks.TensorBoard(run_logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 14s 8ms/step - loss: 6.3170 - accuracy: 0.1616 - val_loss: 2.2560 - val_accuracy: 0.2028\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 2.1047 - accuracy: 0.2357 - val_loss: 2.0101 - val_accuracy: 0.2710\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.9826 - accuracy: 0.2716 - val_loss: 1.9654 - val_accuracy: 0.2724\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.9046 - accuracy: 0.3014 - val_loss: 1.9286 - val_accuracy: 0.2970\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.8365 - accuracy: 0.3290 - val_loss: 1.8214 - val_accuracy: 0.3302\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.7821 - accuracy: 0.3520 - val_loss: 1.8209 - val_accuracy: 0.3486\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.7381 - accuracy: 0.3695 - val_loss: 1.7496 - val_accuracy: 0.3694\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.6964 - accuracy: 0.3884 - val_loss: 1.7181 - val_accuracy: 0.3782\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.6639 - accuracy: 0.4015 - val_loss: 1.6936 - val_accuracy: 0.3900\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.6344 - accuracy: 0.4130 - val_loss: 1.6560 - val_accuracy: 0.4086\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.6113 - accuracy: 0.4198 - val_loss: 1.6171 - val_accuracy: 0.4260\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.5893 - accuracy: 0.4289 - val_loss: 1.6292 - val_accuracy: 0.4114\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.5716 - accuracy: 0.4356 - val_loss: 1.7222 - val_accuracy: 0.3862\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.5520 - accuracy: 0.4417 - val_loss: 1.6214 - val_accuracy: 0.4234\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.5353 - accuracy: 0.4467 - val_loss: 1.5919 - val_accuracy: 0.4384\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.5212 - accuracy: 0.4528 - val_loss: 1.5877 - val_accuracy: 0.4368\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.5034 - accuracy: 0.4584 - val_loss: 1.5969 - val_accuracy: 0.4276\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.4912 - accuracy: 0.4642 - val_loss: 1.5927 - val_accuracy: 0.4336\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.4778 - accuracy: 0.4690 - val_loss: 1.5830 - val_accuracy: 0.4314\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.4650 - accuracy: 0.4748 - val_loss: 1.5545 - val_accuracy: 0.4472\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.4527 - accuracy: 0.4770 - val_loss: 1.5558 - val_accuracy: 0.4422\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.4390 - accuracy: 0.4831 - val_loss: 1.5325 - val_accuracy: 0.4550\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.4241 - accuracy: 0.4894 - val_loss: 1.5682 - val_accuracy: 0.4444\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.4144 - accuracy: 0.4896 - val_loss: 1.5427 - val_accuracy: 0.4558\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.4022 - accuracy: 0.4955 - val_loss: 1.5913 - val_accuracy: 0.4374\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.3925 - accuracy: 0.4992 - val_loss: 1.6116 - val_accuracy: 0.4340\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.3816 - accuracy: 0.5032 - val_loss: 1.5459 - val_accuracy: 0.4538\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.3729 - accuracy: 0.5055 - val_loss: 1.5559 - val_accuracy: 0.4584\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.3602 - accuracy: 0.5128 - val_loss: 1.5289 - val_accuracy: 0.4616\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.3546 - accuracy: 0.5144 - val_loss: 1.5184 - val_accuracy: 0.4730\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.3440 - accuracy: 0.5179 - val_loss: 1.5396 - val_accuracy: 0.4568\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.3325 - accuracy: 0.5219 - val_loss: 1.5772 - val_accuracy: 0.4472\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.3218 - accuracy: 0.5256 - val_loss: 1.5240 - val_accuracy: 0.4660\n",
      "Epoch 34/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.3132 - accuracy: 0.5279 - val_loss: 1.5357 - val_accuracy: 0.4588\n",
      "Epoch 35/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.3075 - accuracy: 0.5282 - val_loss: 1.5434 - val_accuracy: 0.4564\n",
      "Epoch 36/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.2972 - accuracy: 0.5354 - val_loss: 1.5453 - val_accuracy: 0.4574\n",
      "Epoch 37/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.2890 - accuracy: 0.5349 - val_loss: 1.5653 - val_accuracy: 0.4528\n",
      "Epoch 38/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.2806 - accuracy: 0.5402 - val_loss: 1.5374 - val_accuracy: 0.4656\n",
      "Epoch 39/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.2732 - accuracy: 0.5426 - val_loss: 1.5277 - val_accuracy: 0.4646\n",
      "Epoch 40/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.2613 - accuracy: 0.5459 - val_loss: 1.5480 - val_accuracy: 0.4616\n",
      "Epoch 41/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.2561 - accuracy: 0.5488 - val_loss: 1.5720 - val_accuracy: 0.4572\n",
      "Epoch 42/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.2460 - accuracy: 0.5518 - val_loss: 1.5281 - val_accuracy: 0.4694\n",
      "Epoch 43/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.2407 - accuracy: 0.5544 - val_loss: 1.5241 - val_accuracy: 0.4690\n",
      "Epoch 44/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.2307 - accuracy: 0.5586 - val_loss: 1.5164 - val_accuracy: 0.4608\n",
      "Epoch 45/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.2235 - accuracy: 0.5598 - val_loss: 1.5567 - val_accuracy: 0.4630\n",
      "Epoch 46/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.2177 - accuracy: 0.5631 - val_loss: 1.5342 - val_accuracy: 0.4758\n",
      "Epoch 47/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.2059 - accuracy: 0.5675 - val_loss: 1.5660 - val_accuracy: 0.4626\n",
      "Epoch 48/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.2001 - accuracy: 0.5693 - val_loss: 1.5428 - val_accuracy: 0.4716\n",
      "Epoch 49/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.1928 - accuracy: 0.5713 - val_loss: 1.5647 - val_accuracy: 0.4686\n",
      "Epoch 50/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.1876 - accuracy: 0.5721 - val_loss: 1.6048 - val_accuracy: 0.4630\n",
      "Epoch 51/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.1797 - accuracy: 0.5773 - val_loss: 1.5425 - val_accuracy: 0.4644\n",
      "Epoch 52/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.1714 - accuracy: 0.5773 - val_loss: 1.5385 - val_accuracy: 0.4748\n",
      "Epoch 53/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.1651 - accuracy: 0.5818 - val_loss: 1.5593 - val_accuracy: 0.4694\n",
      "Epoch 54/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.1543 - accuracy: 0.5853 - val_loss: 1.5483 - val_accuracy: 0.4714\n",
      "Epoch 55/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.1491 - accuracy: 0.5881 - val_loss: 1.5897 - val_accuracy: 0.4620\n",
      "Epoch 56/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.1468 - accuracy: 0.5878 - val_loss: 1.5914 - val_accuracy: 0.4688\n",
      "Epoch 57/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.1372 - accuracy: 0.5931 - val_loss: 1.5872 - val_accuracy: 0.4736\n",
      "Epoch 58/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.1298 - accuracy: 0.5921 - val_loss: 1.6042 - val_accuracy: 0.4616\n",
      "Epoch 59/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.1229 - accuracy: 0.5983 - val_loss: 1.6057 - val_accuracy: 0.4650\n",
      "Epoch 60/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.1187 - accuracy: 0.5980 - val_loss: 1.5853 - val_accuracy: 0.4654\n",
      "Epoch 61/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.1117 - accuracy: 0.6007 - val_loss: 1.5984 - val_accuracy: 0.4790\n",
      "Epoch 62/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.1046 - accuracy: 0.6012 - val_loss: 1.6226 - val_accuracy: 0.4610\n",
      "Epoch 63/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.0959 - accuracy: 0.6061 - val_loss: 1.5966 - val_accuracy: 0.4716\n",
      "Epoch 64/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.0960 - accuracy: 0.6058 - val_loss: 1.6138 - val_accuracy: 0.4680\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2d2c158ed48>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=[cb_earlystopping, cb_model_checkpoint, cb_tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 2ms/step - loss: 1.5164 - accuracy: 0.4608\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.5163871049880981, 0.4607999920845032]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.load_model(\"my_cifar10_model.h5\")\n",
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 22s 12ms/step - loss: 1.8319 - accuracy: 0.3436 - val_loss: 1.7187 - val_accuracy: 0.3908\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.6649 - accuracy: 0.4076 - val_loss: 1.5634 - val_accuracy: 0.4378\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.5969 - accuracy: 0.4340 - val_loss: 1.5399 - val_accuracy: 0.4496\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.5484 - accuracy: 0.4494 - val_loss: 1.5074 - val_accuracy: 0.4532\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.5021 - accuracy: 0.4672 - val_loss: 1.5173 - val_accuracy: 0.4568\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.4694 - accuracy: 0.4811 - val_loss: 1.4353 - val_accuracy: 0.4926\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.4355 - accuracy: 0.4919 - val_loss: 1.3879 - val_accuracy: 0.5050\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.3990 - accuracy: 0.5026 - val_loss: 1.4059 - val_accuracy: 0.4930\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.3795 - accuracy: 0.5097 - val_loss: 1.3741 - val_accuracy: 0.5132\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.3586 - accuracy: 0.5194 - val_loss: 1.3586 - val_accuracy: 0.5192\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.3309 - accuracy: 0.5294 - val_loss: 1.3444 - val_accuracy: 0.5246\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.3174 - accuracy: 0.5335 - val_loss: 1.3739 - val_accuracy: 0.5176\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.2960 - accuracy: 0.5414 - val_loss: 1.3590 - val_accuracy: 0.5232\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.2812 - accuracy: 0.5490 - val_loss: 1.3163 - val_accuracy: 0.5312\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.2652 - accuracy: 0.5524 - val_loss: 1.3181 - val_accuracy: 0.5332\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.2445 - accuracy: 0.5612 - val_loss: 1.3420 - val_accuracy: 0.5376\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.2296 - accuracy: 0.5655 - val_loss: 1.3260 - val_accuracy: 0.5330\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.2180 - accuracy: 0.5686 - val_loss: 1.3133 - val_accuracy: 0.5402\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.2048 - accuracy: 0.5760 - val_loss: 1.3297 - val_accuracy: 0.5394\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.1879 - accuracy: 0.5802 - val_loss: 1.3381 - val_accuracy: 0.5344\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.1727 - accuracy: 0.5874 - val_loss: 1.3252 - val_accuracy: 0.5356\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.1623 - accuracy: 0.5907 - val_loss: 1.3194 - val_accuracy: 0.5404\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.1479 - accuracy: 0.5947 - val_loss: 1.3252 - val_accuracy: 0.5364\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.1378 - accuracy: 0.5978 - val_loss: 1.3243 - val_accuracy: 0.5432\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.1190 - accuracy: 0.6042 - val_loss: 1.3155 - val_accuracy: 0.5410\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.1141 - accuracy: 0.6098 - val_loss: 1.3410 - val_accuracy: 0.5350\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.1001 - accuracy: 0.6100 - val_loss: 1.3208 - val_accuracy: 0.5424\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.0838 - accuracy: 0.6171 - val_loss: 1.3076 - val_accuracy: 0.5506\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.0747 - accuracy: 0.6224 - val_loss: 1.3253 - val_accuracy: 0.5470\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.0680 - accuracy: 0.6226 - val_loss: 1.3297 - val_accuracy: 0.5342\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.0569 - accuracy: 0.6268 - val_loss: 1.3452 - val_accuracy: 0.5396\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.0438 - accuracy: 0.6315 - val_loss: 1.3442 - val_accuracy: 0.5404\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.0444 - accuracy: 0.6335 - val_loss: 1.3273 - val_accuracy: 0.5474\n",
      "Epoch 34/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.0226 - accuracy: 0.6401 - val_loss: 1.3372 - val_accuracy: 0.5398\n",
      "Epoch 35/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.0156 - accuracy: 0.6420 - val_loss: 1.3459 - val_accuracy: 0.5418\n",
      "Epoch 36/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 0.9985 - accuracy: 0.6491 - val_loss: 1.3700 - val_accuracy: 0.5376\n",
      "Epoch 37/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 0.9971 - accuracy: 0.6483 - val_loss: 1.3512 - val_accuracy: 0.5490\n",
      "Epoch 38/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 0.9900 - accuracy: 0.6513 - val_loss: 1.3659 - val_accuracy: 0.5482\n",
      "157/157 [==============================] - 1s 3ms/step - loss: 1.3076 - accuracy: 0.5506\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3075640201568604, 0.550599992275238]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_add_batch = keras.models.Sequential()\n",
    "model_add_batch.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "model_add_batch.add(keras.layers.BatchNormalization())\n",
    "for layer in range(20):\n",
    "    model_add_batch.add(keras.layers.Dense(100, kernel_initializer=\"he_normal\", use_bias=False))\n",
    "    model_add_batch.add(keras.layers.BatchNormalization())\n",
    "    model_add_batch.add(keras.layers.Activation(\"elu\"))\n",
    "model_add_batch.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model_add_batch.compile(loss=\"sparse_categorical_crossentropy\", optimizer=keras.optimizers.Nadam(learning_rate=5e-4), metrics=[\"accuracy\"])\n",
    "\n",
    "cb_earlystopping = keras.callbacks.EarlyStopping(patience=10)\n",
    "cb_model_checkpoint = keras.callbacks.ModelCheckpoint(\"my_cifar10_batch_model.h5\", save_best_only=True)\n",
    "run_index = 1\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_batch_logs\", \"run_batch_{:03d}\".format(run_index))\n",
    "cb_tensorboard = keras.callbacks.TensorBoard(run_logdir)\n",
    "\n",
    "model_add_batch.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=[cb_earlystopping, cb_model_checkpoint, cb_tensorboard])\n",
    "\n",
    "model_add_batch = keras.models.load_model(\"my_cifar10_batch_model.h5\")\n",
    "model_add_batch.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 14s 8ms/step - loss: 1.9225 - accuracy: 0.3124 - val_loss: 1.7925 - val_accuracy: 0.3774\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.7055 - accuracy: 0.3960 - val_loss: 1.7446 - val_accuracy: 0.3924\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.6104 - accuracy: 0.4283 - val_loss: 1.5793 - val_accuracy: 0.4386\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.5353 - accuracy: 0.4598 - val_loss: 1.5461 - val_accuracy: 0.4596\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.4849 - accuracy: 0.4796 - val_loss: 1.5635 - val_accuracy: 0.4300\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.4371 - accuracy: 0.4963 - val_loss: 1.5492 - val_accuracy: 0.4684\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.3944 - accuracy: 0.5156 - val_loss: 1.5159 - val_accuracy: 0.4740\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.3618 - accuracy: 0.5280 - val_loss: 1.5401 - val_accuracy: 0.4818\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.3221 - accuracy: 0.5428 - val_loss: 1.4555 - val_accuracy: 0.5024\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.2896 - accuracy: 0.5528 - val_loss: 1.4642 - val_accuracy: 0.4982\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.2596 - accuracy: 0.5652 - val_loss: 1.4773 - val_accuracy: 0.4960\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.2289 - accuracy: 0.5743 - val_loss: 1.5027 - val_accuracy: 0.4994\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.2054 - accuracy: 0.5862 - val_loss: 1.5095 - val_accuracy: 0.5020\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.1865 - accuracy: 0.5915 - val_loss: 1.4897 - val_accuracy: 0.5130\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.1523 - accuracy: 0.6046 - val_loss: 1.5042 - val_accuracy: 0.5042\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.1318 - accuracy: 0.6114 - val_loss: 1.4725 - val_accuracy: 0.5194\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.1063 - accuracy: 0.6198 - val_loss: 1.5327 - val_accuracy: 0.5056\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.0944 - accuracy: 0.6261 - val_loss: 1.5393 - val_accuracy: 0.5182\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.0652 - accuracy: 0.6385 - val_loss: 1.5634 - val_accuracy: 0.5094\n",
      "157/157 [==============================] - 1s 3ms/step - loss: 2.8235 - accuracy: 0.0972\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.8234598636627197, 0.09719999879598618]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_selu = keras.models.Sequential()\n",
    "model_selu.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for layer in range(20):\n",
    "    model_selu.add(keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"))\n",
    "model_selu.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model_selu.compile(loss=\"sparse_categorical_crossentropy\", optimizer=keras.optimizers.Nadam(learning_rate=7e-4), metrics=[\"accuracy\"])\n",
    "\n",
    "cb_earlystopping = keras.callbacks.EarlyStopping(patience=10)\n",
    "cb_model_checkpoint = keras.callbacks.ModelCheckpoint(\"my_cifar10_selu_model.h5\", save_best_only=True)\n",
    "run_index = 1\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_selu_logs\", \"run_selu_{:03d}\".format(run_index))\n",
    "cb_tensorboard = keras.callbacks.TensorBoard(run_logdir)\n",
    "\n",
    "X_means = X_train.mean(axis=0)\n",
    "X_stds = X_train.std(axis=0)\n",
    "X_train_scaled = (X_train - X_means) / X_stds\n",
    "X_valid_scaled = (X_valid - X_means) / X_stds\n",
    "X_test_scaled = (X_test - X_means) / X_stds\n",
    "\n",
    "model_selu.fit(X_train_scaled, y_train, epochs=100, validation_data=(X_valid_scaled, y_valid), callbacks=[cb_earlystopping, cb_model_checkpoint, cb_tensorboard])\n",
    "\n",
    "model_selu = keras.models.load_model(\"my_cifar10_selu_model.h5\")\n",
    "model_selu.evaluate(X_valid_scaled, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5024"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MCAlphaDropout(keras.layers.AlphaDropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)\n",
    "\n",
    "mc_model = keras.models.Sequential([MCAlphaDropout(layer.rate) if isinstance(layer, keras.layers.AlphaDropout) else layer for layer in model_selu.layers])\n",
    "\n",
    "def mc_dropout_predict_probas(mc_model, X, n_samples=10):\n",
    "    Y_probas = [mc_model.predict(X) for sample in range(n_samples)]\n",
    "    return np.mean(Y_probas, axis=0)\n",
    "\n",
    "def mc_dropout_predict_classes(mc_model, X, n_samples=10):\n",
    "    Y_probas = mc_dropout_predict_probas(mc_model, X, n_samples)\n",
    "    return np.argmax(Y_probas, axis=1)\n",
    "\n",
    "y_pred = mc_dropout_predict_classes(mc_model, X_valid_scaled)\n",
    "np.mean(y_pred == y_valid[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "K = keras.backend\n",
    "\n",
    "class ExponentialLearningRate(keras.callbacks.Callback):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        self.rates = []\n",
    "        self.losses = []\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        self.rates.append(K.get_value(self.model.optimizer.learning_rate))\n",
    "        self.losses.append(logs[\"loss\"])\n",
    "        K.set_value(self.model.optimizer.learning_rate, self.model.optimizer.learning_rate * self.factor)\n",
    "\n",
    "def find_learning_rate(model, X, y, epochs=1, batch_size=32, min_rate=10**-5, max_rate=10):\n",
    "    init_weights = model.get_weights()\n",
    "    iterations = math.ceil(len(X) / batch_size) * epochs\n",
    "    factor = np.exp(np.log(max_rate / min_rate) / iterations)\n",
    "    init_lr = K.get_value(model.optimizer.learning_rate)\n",
    "    K.set_value(model.optimizer.learning_rate, min_rate)\n",
    "    exp_lr = ExponentialLearningRate(factor)\n",
    "    history = model.fit(X, y, epochs=epochs, batch_size=batch_size,\n",
    "                        callbacks=[exp_lr])\n",
    "    K.set_value(model.optimizer.learning_rate, init_lr)\n",
    "    model.set_weights(init_weights)\n",
    "    return exp_lr.rates, exp_lr.losses\n",
    "\n",
    "def plot_lr_vs_loss(rates, losses):\n",
    "    plt.plot(rates, losses)\n",
    "    plt.gca().set_xscale('log')\n",
    "    plt.hlines(min(losses), min(rates), max(rates))\n",
    "    plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 2])\n",
    "    plt.xlabel(\"Learning rate\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "\n",
    "class OneCycleScheduler(keras.callbacks.Callback):\n",
    "    def __init__(self, iterations, max_rate, start_rate=None,\n",
    "                 last_iterations=None, last_rate=None):\n",
    "        self.iterations = iterations\n",
    "        self.max_rate = max_rate\n",
    "        self.start_rate = start_rate or max_rate / 10\n",
    "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
    "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
    "        self.last_rate = last_rate or self.start_rate / 1000\n",
    "        self.iteration = 0\n",
    "    def _interpolate(self, iter1, iter2, rate1, rate2):\n",
    "        return ((rate2 - rate1) * (self.iteration - iter1)\n",
    "                / (iter2 - iter1) + rate1)\n",
    "    def on_batch_begin(self, batch, logs):\n",
    "        if self.iteration < self.half_iteration:\n",
    "            rate = self._interpolate(0, self.half_iteration, self.start_rate, self.max_rate)\n",
    "        elif self.iteration < 2 * self.half_iteration:\n",
    "            rate = self._interpolate(self.half_iteration, 2 * self.half_iteration,\n",
    "                                     self.max_rate, self.start_rate)\n",
    "        else:\n",
    "            rate = self._interpolate(2 * self.half_iteration, self.iterations,\n",
    "                                     self.start_rate, self.last_rate)\n",
    "        self.iteration += 1\n",
    "        K.set_value(self.model.optimizer.learning_rate, rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352/352 [==============================] - 3s 8ms/step - loss: nan - accuracy: 0.1449\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9.999999747378752e-06,\n",
       " 9.615227699279785,\n",
       " 2.612095355987549,\n",
       " 4.055954388209752)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, kernel_initializer=\"lecun_normal\", activation=\"selu\"))\n",
    "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "batch_size = 128\n",
    "rates, losses = find_learning_rate(model, X_train_scaled, y_train, epochs=1, batch_size=batch_size)\n",
    "plot_lr_vs_loss(rates, losses)\n",
    "plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 1.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "352/352 [==============================] - 4s 9ms/step - loss: 2.0751 - accuracy: 0.2809 - val_loss: 1.7801 - val_accuracy: 0.3692\n",
      "Epoch 2/15\n",
      "352/352 [==============================] - 3s 8ms/step - loss: 1.7671 - accuracy: 0.3760 - val_loss: 1.7079 - val_accuracy: 0.3918\n",
      "Epoch 3/15\n",
      "352/352 [==============================] - 3s 8ms/step - loss: 1.6280 - accuracy: 0.4217 - val_loss: 1.6681 - val_accuracy: 0.4122\n",
      "Epoch 4/15\n",
      "352/352 [==============================] - 3s 8ms/step - loss: 1.5544 - accuracy: 0.4494 - val_loss: 1.7590 - val_accuracy: 0.4008\n",
      "Epoch 5/15\n",
      "352/352 [==============================] - 3s 9ms/step - loss: 1.4934 - accuracy: 0.4664 - val_loss: 1.5920 - val_accuracy: 0.4502\n",
      "Epoch 6/15\n",
      "352/352 [==============================] - 3s 8ms/step - loss: 1.4522 - accuracy: 0.4852 - val_loss: 1.6635 - val_accuracy: 0.4208\n",
      "Epoch 7/15\n",
      "352/352 [==============================] - 3s 8ms/step - loss: 1.4220 - accuracy: 0.4932 - val_loss: 1.5988 - val_accuracy: 0.4538\n",
      "Epoch 8/15\n",
      "352/352 [==============================] - 3s 8ms/step - loss: 1.3501 - accuracy: 0.5199 - val_loss: 1.5044 - val_accuracy: 0.4806\n",
      "Epoch 9/15\n",
      "352/352 [==============================] - 3s 8ms/step - loss: 1.2698 - accuracy: 0.5475 - val_loss: 1.4898 - val_accuracy: 0.4928\n",
      "Epoch 10/15\n",
      "352/352 [==============================] - 3s 9ms/step - loss: 1.1999 - accuracy: 0.5719 - val_loss: 1.5067 - val_accuracy: 0.5076\n",
      "Epoch 11/15\n",
      "352/352 [==============================] - 3s 8ms/step - loss: 1.1308 - accuracy: 0.5956 - val_loss: 1.5336 - val_accuracy: 0.4994\n",
      "Epoch 12/15\n",
      "352/352 [==============================] - 3s 9ms/step - loss: 1.0612 - accuracy: 0.6220 - val_loss: 1.5075 - val_accuracy: 0.5150\n",
      "Epoch 13/15\n",
      "352/352 [==============================] - 3s 9ms/step - loss: 0.9881 - accuracy: 0.6453 - val_loss: 1.5617 - val_accuracy: 0.5142\n",
      "Epoch 14/15\n",
      "352/352 [==============================] - 3s 8ms/step - loss: 0.9242 - accuracy: 0.6667 - val_loss: 1.5879 - val_accuracy: 0.5244\n",
      "Epoch 15/15\n",
      "352/352 [==============================] - 3s 8ms/step - loss: 0.8837 - accuracy: 0.6834 - val_loss: 1.6018 - val_accuracy: 0.5206\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, kernel_initializer=\"lecun_normal\", activation=\"selu\"))\n",
    "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-2)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "n_epochs = 15\n",
    "onecycle = OneCycleScheduler(len(X_train_scaled) // batch_size * n_epochs, max_rate=0.05)\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs, batch_size=batch_size, validation_data=(X_valid_scaled, y_valid), callbacks=[onecycle])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c4e1c02ba9a4315c9c9b9f3ccdc568bf0028a114bbf7c4447cf8df78c88a2f71"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('handson-ml2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
